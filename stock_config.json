import io
import json
import os
import hashlib
from dataclasses import dataclass
from datetime import datetime
import time
from decimal import Decimal, InvalidOperation, getcontext
import re
from pathlib import Path
from typing import Dict, Any, Tuple, List

import pandas as pd
import streamlit as st
from openpyxl import load_workbook

# ============================
# Persistent config (best effort)
# ============================
APP_DIR = Path(__file__).parent

# Prefer a writable data dir (avoid writing into repo root directly)
DATA_DIR = Path(os.environ.get("STOCKAPP_DATA_DIR", str(APP_DIR / ".data")))
DATA_DIR.mkdir(parents=True, exist_ok=True)

CONFIG_FILE = DATA_DIR / "stock_config.json"
CONFIG_BAK = DATA_DIR / "stock_config.bak.json"

EXCEL_CONFIG_SHEET = "_STOCKAPP_CONFIG"
EXCEL_CONFIG_CELL = "A1"

DEFAULT_PRODUCTS = [
    {"name": "ì—”ë‹¤ì´ë¸Œ", "keyword": "ì—”ë‹¤ì´ë¸Œ"},
    {"name": "ìƒ¬ë¡¯", "keyword": "ìƒ¬ë¡¯"},
    {"name": "ì•„ìŠ¤íŒŒë¼", "keyword": "ìƒ ì•„ìŠ¤íŒŒë¼"},
    {"name": "í™”ì´íŠ¸ì•„ìŠ¤íŒŒë¼", "keyword": "í™”ì´íŠ¸ ì•„ìŠ¤íŒŒë¼"},
    {"name": "ë¯¸ë‹ˆì–‘ë°°ì¶”", "keyword": "ë¯¸ë‹ˆì–‘ë°°ì¶”"},
    {"name": "ì–‘ì†¡ì´", "keyword": "ì–‘ì†¡ì´"},
    {"name": "ìƒˆì†¡ì´", "keyword": "ìƒˆì†¡ì´"},
    {"name": "ëŠíƒ€ë¦¬", "keyword": "ëŠíƒ€ë¦¬"},
    {"name": "íŒ½ì´", "keyword": "íŒ½ì´"},
    {"name": "í™ë‹¹ê·¼", "keyword": "í™ë‹¹ê·¼"},
    {"name": "ë¸Œë¡œì½œë¦¬", "keyword": "ë¸Œë¡œì½œë¦¬"},
    {"name": "ì»¬ë¦¬í”Œë¼ì›Œ", "keyword": "ì»¬ë¦¬í”Œë¼ì›Œ"},
    {"name": "ì¤„ê¸°ìƒëŸ¬ë¦¬", "keyword": "ì¤„ê¸°ìƒëŸ¬ë¦¬"},
    {"name": "ì˜¤ë Œì§€", "keyword": "ì˜¤ë Œì§€"},
    {"name": "ìëª½", "keyword": "ìëª½"},
    {"name": "ë ˆëª¬", "keyword": "ë ˆëª¬"},
    {"name": "ë¼ì„", "keyword": "ë¼ì„"},
    {"name": "ì–‘ìƒì¶”", "keyword": "ì–‘ìƒì¶”"},
    {"name": "ì•Œë°°ê¸°", "keyword": "ì•Œë°°ê¸°"},
    {"name": "ë°©ìš¸í† ë§ˆí† ", "keyword": "ë°©ìš¸í† ë§ˆí† "},
    {"name": "ì™„ìˆ™í† ë§ˆí† ", "keyword": "ì™„ìˆ™í† ë§ˆí† "},
    {"name": "ì•„ë³´ì¹´ë„", "keyword": "ì•„ë³´ì¹´ë„"},
    {"name": "ì‹ìš©ê½ƒ", "keyword": "ì‹ìš©ê½ƒ"},
    {"name": "ì²­í”¼ë§", "keyword": "ì²­í”¼ë§"},
    {"name": "ë¯¸ë‹ˆíŒŒí”„ë¦¬ì¹´", "keyword": "ë¯¸ë‹ˆ íŒŒí”„ë¦¬ì¹´"},
    {"name": "ì‚¼ìƒ‰íŒŒí”„ë¦¬ì¹´", "keyword": "ì‚¼ìƒ‰ íŒŒí”„ë¦¬ì¹´"},
    {"name": "ë¹„íŠ¸", "keyword": "ë¹„íŠ¸"},
    {"name": "ì½œë¼ë¹„", "keyword": "ì½œë¼ë¹„"},
    {"name": "íŒŒì„¸ë¦¬", "keyword": "íŒŒì„¸ë¦¬"},
    {"name": "ê¹ë§ˆëŠ˜", "keyword": "ê¹ë§ˆëŠ˜"},
    {"name": "ë‹¨í˜¸ë°•", "keyword": "ë‹¨í˜¸ë°•"},
    {"name": "ì¥¬í‚¤ë‹ˆ", "keyword": "ì¥¬í‚¤ë‹ˆ"},
    {"name": "ê°€ì§€", "keyword": "ê°€ì§€"},
    {"name": "ë°±ì˜¤ì´", "keyword": "ë°±ì˜¤ì´"},
]


def _default_config() -> Dict[str, Any]:
    return {
        "version": 9,
        "inventory_column": "ì¬ê³ ìˆ˜ëŸ‰",
        "name_column": "ìƒí’ˆëª…",
        "products": [],
        "rules": {},  # {base_product: [ {keyword, mode, value, table}, ... ]}
        "ref_qty": {},  # {product_name: "ì°¸ê³ ìˆ˜ëŸ‰"}
        "recognition_logic": DEFAULT_RECOGNITION_LOGIC.copy(),
    }


def _atomic_write(path: Path, text: str, encoding: str = "utf-8") -> None:
    tmp = path.with_suffix(path.suffix + ".tmp")
    tmp.write_text(text, encoding=encoding)
    tmp.replace(path)


def load_config() -> Dict[str, Any]:
    for p in [CONFIG_FILE, CONFIG_BAK]:
        if p.exists():
            try:
                cfg = json.loads(p.read_text(encoding="utf-8"))
                if not isinstance(cfg, dict):
                    continue
                cfg.setdefault("version", 9)
                cfg.setdefault("inventory_column", "ì¬ê³ ìˆ˜ëŸ‰")
                cfg.setdefault("name_column", "ìƒí’ˆëª…")
                cfg.setdefault("products", [])
                cfg.setdefault("rules", {})
                cfg.setdefault("ref_qty", {})
                cfg.setdefault("recognition_logic", DEFAULT_RECOGNITION_LOGIC.copy())
                return cfg
            except Exception:
                continue
    return _default_config()


def save_config(cfg: Dict[str, Any]) -> None:
    cfg["version"] = 9
    txt = json.dumps(cfg, ensure_ascii=False, indent=2)
    if CONFIG_FILE.exists():
        try:
            _atomic_write(CONFIG_BAK, CONFIG_FILE.read_text(encoding="utf-8"))
        except Exception:
            pass
    _atomic_write(CONFIG_FILE, txt)


# ----------------------------
# Excel helpers
# ----------------------------
def find_header_row_and_columns(ws, name_col: str, inv_col: str, max_scan_rows: int = 15) -> Tuple[int, int, int]:
    for r in range(1, max_scan_rows + 1):
        row_vals = [ws.cell(r, c).value for c in range(1, ws.max_column + 1)]
        row_str = [str(v).strip() if v is not None else "" for v in row_vals]
        if name_col in row_str and inv_col in row_str:
            name_idx = row_str.index(name_col) + 1
            inv_idx = row_str.index(inv_col) + 1
            return r, name_idx, inv_idx
    raise ValueError(f"í—¤ë”ì—ì„œ '{name_col}' ë˜ëŠ” '{inv_col}' ì»¬ëŸ¼ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")


def to_number(x) -> float:
    if x is None:
        return 0.0
    if isinstance(x, (int, float)):
        return float(x)
    s = str(x).strip()
    if s == "":
        return 0.0
    s = s.replace(",", "")
    try:
        return float(s)
    except Exception:
        return 0.0


def parse_input_number(x) -> float:
    """Parse user input from data_editor (string or number)."""
    if x is None:
        return 0.0
    if isinstance(x, (int, float)):
        return float(x)
    s = str(x).strip()
    if s == "":
        return 0.0
    s = s.replace(",", "")
    try:
        return float(s)
    except Exception:
        return 0.0


def qty_key(q: float) -> str:
    """Normalize input quantity to mapping key string."""
    try:
        qf = float(q)
    except Exception:
        return str(q)
    if abs(qf - int(qf)) < 1e-9:
        return str(int(qf))
    return str(qf)


def parse_map_string(s: str) -> Dict[str, int]:
    """
    Accept formats:
      - "2=5, 3=7"
      - "2:5\n3:7"
      - "2 -> 5"
    Return {"2":5, "3":7}
    Values are stored as INT to avoid 5.0 display.
    """
    if not s:
        return {}
    txt = str(s).strip()
    if not txt:
        return {}
    txt = txt.replace("\n", ",")
    txt = txt.replace("â†’", "=").replace("->", "=").replace(":", "=")
    out: Dict[str, int] = {}
    for chunk in [x.strip() for x in txt.split(",") if x.strip()]:
        if "=" not in chunk:
            continue
        k, v = chunk.split("=", 1)
        k = k.strip()
        v = v.strip()
        if not k or not v:
            continue

        # allow wildcard key "*" (ì…ë ¥ê°’ ë¬´ê´€ ê³ ì • ì ìš©)
        if k == "*":
            try:
                out["*"] = int(round(float(v)))
            except Exception:
                pass
            continue
        try:
            kf = float(k)
            kk = str(int(kf)) if abs(kf - int(kf)) < 1e-9 else str(kf)
            out[kk] = int(round(float(v)))
        except Exception:
            continue
    return out


def fmt_int(v) -> str:
    try:
        return str(int(round(float(v))))
    except Exception:
        return str(v)


def fmt_qty_for_memo(v: float) -> str:
    """Pretty formatting for memo txt (avoid trailing .0)."""
    try:
        vf = float(v)
    except Exception:
        return str(v)
    if abs(vf - int(vf)) < 1e-9:
        return str(int(vf))
    s = f"{vf:.12f}".rstrip("0").rstrip(".")
    return s if s else "0"


# ----------------------------
# Inventory total (display-only)
# ----------------------------
getcontext().prec = 28

_RE_KG = re.compile(r"(\d+(?:\.\d+)?)\s*kg", re.IGNORECASE)
_RE_G = re.compile(r"(\d+(?:\.\d+)?)\s*g", re.IGNORECASE)
_RE_PACK = re.compile(r"(\d+(?:\.\d+)?)\s*íŒ©")

_RE_PACK_EN = re.compile(r"(\d+(?:\.\d+)?)\s*pack", re.IGNORECASE)
_RE_BONG = re.compile(r"(\d+(?:\.\d+)?)\s*ë´‰")
_RE_TONG = re.compile(r"(\d+(?:\.\d+)?)\s*í†µ")
_RE_EA = re.compile(r"(\d+(?:\.\d+)?)\s*ê°œ")
_RE_BOX = re.compile(r"(\d+(?:\.\d+)?)\s*(ë°•ìŠ¤|box)", re.IGNORECASE)


# ----------------------------
# Recognition logic (unit parsing)
# ----------------------------
# ê° í•­ëª©: priority(ë‚®ì„ìˆ˜ë¡ ë¨¼ì €), output_unit(í‘œì‹œ ë‹¨ìœ„), multiplier(ìˆ«ìì— ê³±), aliases(ì¸ì‹í•  ë¬¸ìì—´ë“¤)
DEFAULT_RECOGNITION_LOGIC: List[Dict[str, Any]] = [
    {"priority": 10, "output_unit": "ë‹¨",   "multiplier": "1",     "aliases": ["ë‹¨"]},
    {"priority": 20, "output_unit": "íŒ©",   "multiplier": "1",     "aliases": ["íŒ©", "pack"]},
    {"priority": 30, "output_unit": "ë´‰",   "multiplier": "1",     "aliases": ["ë´‰"]},
    {"priority": 40, "output_unit": "í†µ",   "multiplier": "1",     "aliases": ["í†µ"]},
    {"priority": 50, "output_unit": "ê°œ",   "multiplier": "1",     "aliases": ["ê°œ", "ea"]},
    {"priority": 60, "output_unit": "ë°•ìŠ¤", "multiplier": "1",     "aliases": ["ë°•ìŠ¤", "box"]},
    # gëŠ” kgë¡œ í™˜ì‚°(ì˜ˆ: 500g -> 0.5kg)
    {"priority": 90, "output_unit": "kg",   "multiplier": "0.001", "aliases": ["g", "ê·¸ë¨"]},
    {"priority": 100,"output_unit": "kg",   "multiplier": "1",     "aliases": ["kg", "í‚¬ë¡œ", "í‚¤ë¡œ"]},
]

def _normalize_recognition_logic(logic: Any) -> List[Dict[str, Any]]:
    """configì— ì €ì¥ëœ recognition_logicë¥¼ ì•ˆì „í•˜ê²Œ ì •ê·œí™”í•©ë‹ˆë‹¤."""
    if not isinstance(logic, list):
        logic = []
    cleaned: List[Dict[str, Any]] = []
    for it in logic:
        if not isinstance(it, dict):
            continue
        unit = str(it.get("output_unit") or it.get("unit") or "").strip()
        if not unit:
            continue

        # priority
        try:
            pr = int(it.get("priority", 999))
        except Exception:
            pr = 999

        # multiplier
        mult_raw = it.get("multiplier", "1")
        try:
            mult = str(mult_raw).strip()
            Decimal(mult)  # validate
        except Exception:
            mult = "1"

        aliases = it.get("aliases") or it.get("patterns") or []
        if isinstance(aliases, str):
            aliases = [a.strip() for a in aliases.split(",") if a.strip()]
        if not isinstance(aliases, list):
            aliases = []
        aliases = [str(a).strip() for a in aliases if str(a).strip()]
        if not aliases:
            continue

        cleaned.append({"priority": pr, "output_unit": unit, "multiplier": mult, "aliases": aliases})

    cleaned.sort(key=lambda x: (x.get("priority", 999), len(str(x.get("output_unit","")))))
    return cleaned

def _get_recognition_logic(cfg: Dict[str, Any]) -> List[Dict[str, Any]]:
    return _normalize_recognition_logic((cfg or {}).get("recognition_logic")) or DEFAULT_RECOGNITION_LOGIC.copy()



def _cell_to_decimal(v) -> Decimal:
    if v is None:
        return Decimal("0")
    if isinstance(v, (int, float)):
        # Use str() to preserve "0.1" rather than binary float
        return Decimal(str(v))
    s = str(v).strip().replace(",", "")
    if not s:
        return Decimal("0")
    try:
        return Decimal(s)
    except Exception:
        return Decimal("0")


def _parse_factor_and_unit(name_str: str, recognition_logic: List[Dict[str, Any]] = None) -> Tuple[Decimal, str]:
    """
    ìƒí’ˆëª…(ë˜ëŠ” ê·œì¹™ í‚¤ì›Œë“œ) ë¬¸ìì—´ì—ì„œ "ìˆ«ì+ë‹¨ìœ„"ë¥¼ ì°¾ì•„ (factor, unit)ì„ ë°˜í™˜í•©ë‹ˆë‹¤.

    - recognition_logic ê¸°ë°˜ìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
      ê° í•­ëª©: priority, output_unit, multiplier, aliases
        ì˜ˆ) {"output_unit":"ë‹¨","multiplier":"1","aliases":["ë‹¨"]}

    - ìˆ«ì+ë‹¨ìœ„ê°€ ìˆìœ¼ë©´ factor = ìˆ«ì * multiplier
    - ë‹¨ìœ„ë§Œ ìˆìœ¼ë©´ factor = 1 * multiplier

    ë°˜í™˜ unitì€ output_unit ì…ë‹ˆë‹¤. (ì˜ˆ: gëŠ” kgë¡œ í™˜ì‚°ë˜ì–´ unit='kg')
    """
    s = name_str or ""
    logic = _normalize_recognition_logic(recognition_logic) if recognition_logic is not None else DEFAULT_RECOGNITION_LOGIC.copy()

    for rule in logic:
        unit = str(rule.get("output_unit", "")).strip()
        if not unit:
            continue
        try:
            mult = Decimal(str(rule.get("multiplier", "1")).strip() or "1")
        except Exception:
            mult = Decimal("1")

        aliases = rule.get("aliases") or []
        if not isinstance(aliases, list):
            aliases = [str(aliases)]

        for alias in aliases:
            alias = str(alias).strip()
            if not alias:
                continue

            # 1) ìˆ«ì+ë‹¨ìœ„
            try:
                if alias.isascii():
                    m = re.search(rf"(\d+(?:\.\d+)?)\s*{re.escape(alias)}\b", s, flags=re.IGNORECASE)
                else:
                    m = re.search(rf"(\d+(?:\.\d+)?)\s*{re.escape(alias)}", s)
            except Exception:
                m = None

            if m:
                try:
                    n = Decimal(m.group(1))
                except Exception:
                    n = Decimal("1")
                return (n * mult), unit

            # 2) ë‹¨ìœ„ë§Œ ìˆëŠ” ê²½ìš°
            try:
                if alias.isascii():
                    if re.search(rf"\b{re.escape(alias)}\b", s, flags=re.IGNORECASE):
                        return (Decimal("1") * mult), unit
                else:
                    if alias in s:
                        return (Decimal("1") * mult), unit
            except Exception:
                pass

    return Decimal("1"), ""


def _fmt_decimal(d: Decimal, max_decimals: int = 3) -> str:
    try:
        q = Decimal("1") if max_decimals <= 0 else (Decimal("1") / (Decimal(10) ** max_decimals))
        d2 = d.quantize(q)  # rounding
    except Exception:
        d2 = d

    s = format(d2, "f")
    s = s.rstrip("0").rstrip(".")
    return s if s else "0"


def compute_stock_display_map(xlsx_bytes: bytes, cfg: Dict[str, Any]) -> Dict[str, str]:
    """
    Build display stock totals per base product using **ê·œì¹™ê´€ë¦¬ í‚¤ì›Œë“œ**ë¥¼ ì°¸ê³ í•˜ì—¬ ê³„ì‚°í•©ë‹ˆë‹¤.

    - ê° ê¸°ì¤€ìƒí’ˆ(base)ì— ëŒ€í•´ ê·œì¹™(í‚¤ì›Œë“œ)ì„ ì´ìš©í•´ ì˜µì…˜ ë‹¨ìœ„ë¥¼ íŒŒì•…í•©ë‹ˆë‹¤.
      ì˜ˆ) '1kg', '500g', '100g' -> kg(í•©ì‚°),  '5íŒ©' -> íŒ©,  '6í†µ' -> í†µ
    - ì—‘ì…€ì˜ ê° í–‰(ìƒí’ˆëª…)ì„ ê·œì¹™ í‚¤ì›Œë“œì™€ ë§¤ì¹­í•˜ì—¬, ì¬ê³ ìˆ˜ëŸ‰ * (ì˜µì…˜ ë‹¨ìœ„ ìˆ˜ëŸ‰) ë¥¼ í•©ì‚°í•©ë‹ˆë‹¤.
      ì˜ˆ) ì–‘ìƒì¶”6í†µ:3ê°œ + ì–‘ìƒì¶”1í†µ:3ê°œ => 3*6 + 3*1 = 21í†µ

    Stock ì—†ëŠ” ê²½ìš°ëŠ” '0'ìœ¼ë¡œ í‘œì‹œí•©ë‹ˆë‹¤.
    """
    inv_col = cfg.get("inventory_column", "ì¬ê³ ìˆ˜ëŸ‰")
    name_col = cfg.get("name_column", "ìƒí’ˆëª…")
    recog_logic = _get_recognition_logic(cfg)

    wb = load_workbook(io.BytesIO(xlsx_bytes), data_only=True)
    ws = wb.active
    header_row, name_idx, inv_idx = find_header_row_and_columns(ws, name_col=name_col, inv_col=inv_col)

    products = cfg.get("products", []) or []
    base_kw: Dict[str, str] = {}
    for p in products:
        bn = str(p.get("name", "")).strip()
        kw = str(p.get("keyword") or p.get("name") or "").strip()
        if bn:
            base_kw[bn] = kw or bn

    bases_sorted = sorted([(bn, base_kw.get(bn, bn)) for bn in base_kw.keys()], key=lambda x: len(str(x[1] or "")), reverse=True)

    rules_map = cfg.get("rules", {}) or {}

    def _scoped_tokens(base_keyword: str, rule_keyword: str) -> List[str]:
        base_keyword = str(base_keyword or "").strip()
        rule_keyword = str(rule_keyword or "").strip()
        if not rule_keyword:
            return []
        if not base_keyword:
            return [rule_keyword]
        # If user already typed full keyword incl base keyword, treat as absolute substring match
        if base_keyword in rule_keyword:
            return [rule_keyword]
        # Otherwise require BOTH base keyword and rule keyword
        return [base_keyword, rule_keyword]

    # Build matchers from rules (per base)
    matchers_by_base: Dict[str, List[Dict[str, Any]]] = {}
    unit_pref: Dict[str, str] = {}
    unit_seen: Dict[str, str] = {}

    for bn, bkw in bases_sorted:
        rs = rules_map.get(bn, []) or []
        ms: List[Dict[str, Any]] = []

        unit_counts: Dict[str, int] = {}
        for i, r in enumerate(rs):
            rr = Rule.from_dict(r)
            if not rr.keyword:
                continue

            tokens = _scoped_tokens(bkw, rr.keyword)
            if not tokens:
                continue

            factor, unit = _parse_factor_and_unit(rr.keyword, recog_logic)

            ms.append(
                {
                    "tokens": tokens,
                    "keyword": rr.keyword,
                    "factor": factor,
                    "unit": unit,
                    "order": i,
                }
            )

            if unit:
                unit_counts[unit] = unit_counts.get(unit, 0) + 1

        # Sort by specificity (longer/ more tokens first)
        ms.sort(
            key=lambda m: (sum(len(t) for t in m["tokens"]), len(m["tokens"]), len(m["keyword"])),
            reverse=True,
        )
        matchers_by_base[bn] = ms

        # Preferred unit: kg wins if any kg/g exists; else most frequent unit in rules
        if unit_counts.get("kg", 0) > 0:
            unit_pref[bn] = "kg"
        elif unit_counts:
            unit_pref[bn] = sorted(unit_counts.items(), key=lambda kv: (kv[1], len(kv[0])), reverse=True)[0][0]
        else:
            unit_pref[bn] = ""

    totals: Dict[str, Decimal] = {}

    def _match_tokens(tokens: List[str], name_str: str) -> bool:
        return bool(tokens) and all((t in name_str) for t in tokens)

    for r in range(header_row + 1, ws.max_row + 1):
        name_val = ws.cell(r, name_idx).value
        if name_val is None or str(name_val).strip() == "":
            continue

        name_str = str(name_val)

        # determine base product by base keyword
        base_name = None
        for bn, kw in bases_sorted:
            if kw and kw in name_str:
                base_name = bn
                break
        if base_name is None:
            continue

        inv_qty = _cell_to_decimal(ws.cell(r, inv_idx).value)
        if inv_qty == 0:
            continue

        # 1) Try rules-based matching first
        chosen = None
        for m in matchers_by_base.get(base_name, []):
            if _match_tokens(m["tokens"], name_str):
                chosen = m
                break

        has_rules = bool(matchers_by_base.get(base_name))
        if chosen and chosen.get("unit"):
            factor = chosen["factor"]
            unit = chosen["unit"]
        else:
            # âœ… ê·œì¹™ì´ ìˆëŠ” ê¸°ì¤€ìƒí’ˆì€ "ê·œì¹™ì— ë§¤ì¹­ë˜ëŠ” ì˜µì…˜"ë§Œ í•©ì‚° (ë‹¨ìœ„ í˜¼í•© ë°©ì§€)
            # ê·œì¹™ì´ ìˆëŠ”ë° ì–´ë–¤ í‚¤ì›Œë“œì—ë„ ì•ˆ ê±¸ë¦¬ë©´, í•´ë‹¹ í–‰ì€ ì¬ê³  í•©ì‚°ì—ì„œ ì œì™¸í•©ë‹ˆë‹¤.
            if has_rules and chosen is None:
                continue
            # 2) Fallback: parse from actual name (kg/g/íŒ©/ë´‰/í†µ/ê°œ/ë°•ìŠ¤)
            factor, unit = _parse_factor_and_unit(name_str, recog_logic)

        # Track unit seen (if rules did not define a preferred unit)
        pref = unit_pref.get(base_name, "") or ""
        if pref:
            unit_seen[base_name] = pref
        elif unit:
            unit_seen[base_name] = unit_seen.get(base_name) or unit

        # Sum: inv_qty * factor (factor already normalized: g -> kg)
        totals[base_name] = totals.get(base_name, Decimal("0")) + (inv_qty * factor)

    out: Dict[str, str] = {}
    for bn, _kw in bases_sorted:
        total = totals.get(bn, Decimal("0"))
        if total == 0:
            out[bn] = "0"
            continue

        unit = unit_pref.get(bn, "") or unit_seen.get(bn, "") or ""
        if unit == "kg":
            out[bn] = f"{_fmt_decimal(total, max_decimals=3)}kg"
        else:
            s = _fmt_decimal(total, max_decimals=3)
            out[bn] = f"{s}{unit}" if unit else s

    return out


# ----------------------------
# Rule model
# ----------------------------
@dataclass
class Rule:
    keyword: str
    mode: str   # "mul" or "map"
    value: float
    table: Dict[str, int]

    @staticmethod
    def from_dict(d: Dict[str, Any]) -> "Rule":
        keyword = str(d.get("keyword", "")).strip()
        mode_raw = str(d.get("mode", "mul")).strip()

        # legacy fixed -> map wildcard (ì…ë ¥ê°’ ë¬´ê´€í•˜ê²Œ ì ìš©)
        value_raw = float(d.get("value", 1.0) or 0.0)

        table = d.get("table") or {}
        if isinstance(table, str):
            table = parse_map_string(table)
        if not isinstance(table, dict):
            table = {}

        t2: Dict[str, int] = {}
        for k, v in table.items():
            try:
                t2[str(k)] = int(round(float(v)))
            except Exception:
                continue

        # migrate fixed -> map wildcard
        if mode_raw == "fixed":
            mode_raw = "map"
            t2.setdefault("*", int(round(value_raw)))

        # ignore legacy round key silently
        if mode_raw not in ("mul", "map"):
            mode_raw = "mul"

        return Rule(keyword=keyword, mode=mode_raw, value=value_raw, table=t2)


def build_actions(cfg: Dict[str, Any], inputs: Dict[str, float]) -> Tuple[List[Dict[str, Any]], pd.DataFrame]:
    """
    Build merged actions like:
      [{"match_tokens":["ì—”ë‹¤ì´ë¸Œ","1kg"],"display":"ì—”ë‹¤ì´ë¸Œ & 1kg","delta":5.0,"bases":[...]}...]

    âœ… ê·œì¹™ í‚¤ì›Œë“œ ìë™ 'ê¸°ì¤€ìƒí’ˆ ìŠ¤ì½”í”„' ì²˜ë¦¬
    - ê·œì¹™ í‚¤ì›Œë“œì— **ê¸°ì¤€ìƒí’ˆ í‚¤ì›Œë“œ(ì˜ˆ: ì—”ë‹¤ì´ë¸Œ)**ê°€ ì´ë¯¸ í¬í•¨ë˜ì–´ ìˆìœ¼ë©´: ê·¸ëŒ€ë¡œ(ë‹¨ì¼ í¬í•¨ë¬¸ìì—´) ë§¤ì¹­
      ì˜ˆ) "ì—”ë‹¤ì´ë¸Œ1kg" -> 'ì—”ë‹¤ì´ë¸Œ1kg'ê°€ í¬í•¨ëœ ìƒí’ˆëª…ë§Œ ë§¤ì¹­
    - ê·œì¹™ í‚¤ì›Œë“œì— ê¸°ì¤€ìƒí’ˆ í‚¤ì›Œë“œê°€ **ì—†ìœ¼ë©´**: (ê¸°ì¤€ìƒí’ˆ í‚¤ì›Œë“œ AND ê·œì¹™ í‚¤ì›Œë“œ) ë‘˜ ë‹¤ í¬í•¨ëœ ìƒí’ˆëª…ë§Œ ë§¤ì¹­
      ì˜ˆ) ê¸°ì¤€=ì—”ë‹¤ì´ë¸Œ, ê·œì¹™í‚¤ì›Œë“œ="1kg" -> 'ì—”ë‹¤ì´ë¸Œ'ì™€ '1kg'ê°€ ëª¨ë‘ ë“¤ì–´ê°„ ìƒí’ˆëª…ë§Œ ë§¤ì¹­
         (ê·¸ë˜ì„œ ë‹¤ë¥¸ ìƒí’ˆì˜ "1kg"ëŠ” ê±´ë“œë¦¬ì§€ ì•ŠìŠµë‹ˆë‹¤)

    For mode=map: delta = table[input_qty_key]. If not found and "*" exists, use wildcard.
    If not found and no wildcard -> record missing.
    """
    prod_kw = {p.get("name"): (p.get("keyword") or p.get("name")) for p in cfg.get("products", [])}
    rules_map = cfg.get("rules", {}) or {}

    def _scoped_tokens(base_keyword: str, rule_keyword: str) -> List[str]:
        base_keyword = str(base_keyword or "").strip()
        rule_keyword = str(rule_keyword or "").strip()
        if not rule_keyword:
            return []
        if not base_keyword:
            return [rule_keyword]
        # If user already typed full keyword incl base keyword, treat as absolute substring match
        if base_keyword in rule_keyword:
            return [rule_keyword]
        # Otherwise require BOTH base keyword and rule keyword to exist in product name
        return [base_keyword, rule_keyword]

    actions_raw: List[Dict[str, Any]] = []
    missing: List[Dict[str, Any]] = []

    for base, qty in inputs.items():
        qty = float(qty or 0.0)
        if qty == 0:
            continue

        base_kw = prod_kw.get(base, base)

        rule_list = rules_map.get(base, [])
        if rule_list:
            for r in rule_list:
                rr = Rule.from_dict(r)
                if not rr.keyword:
                    continue

                if rr.mode == "map":
                    k = qty_key(qty)
                    if k in rr.table:
                        delta = rr.table[k]
                    elif "*" in rr.table:
                        delta = rr.table["*"]
                    else:
                        missing.append({"ê¸°ì¤€ìƒí’ˆ": base, "í‚¤ì›Œë“œ": rr.keyword, "ì…ë ¥ê°’": qty, "ì‚¬ìœ ": f"ë§¤í•‘ì— '{k}' ì—†ìŒ"})
                        continue
                else:  # mul
                    delta = qty * rr.value

                tokens = _scoped_tokens(base_kw, rr.keyword)
                if not tokens:
                    continue

                display = " & ".join(tokens) if len(tokens) > 1 else tokens[0]

                actions_raw.append({
                    "base": base,
                    "match_tokens": tokens,
                    "display": display,
                    "delta": float(delta),
                })
        else:
            # No rule: base keyword alone is used (legacy behavior)
            tokens = [str(base_kw)]
            actions_raw.append({"base": base, "match_tokens": tokens, "display": tokens[0], "delta": float(qty)})

    # merge by match_tokens key (so different base products with same suffix like "1kg" won't collide)
    merged: Dict[str, float] = {}
    bases: Dict[str, set] = {}
    meta: Dict[str, Dict[str, Any]] = {}

    for a in actions_raw:
        key = "\u0001".join(a["match_tokens"])
        merged[key] = merged.get(key, 0.0) + float(a["delta"])
        bases.setdefault(key, set()).add(a["base"])
        if key not in meta:
            meta[key] = {"match_tokens": a["match_tokens"], "display": a.get("display")}

    out = []
    for k, v in merged.items():
        out.append({
            "match_tokens": meta[k]["match_tokens"],
            "display": meta[k].get("display") or " & ".join(meta[k]["match_tokens"]),
            "delta": v,
            "bases": sorted(list(bases.get(k, []))),
        })

    out.sort(key=lambda x: len(str(x.get("display", ""))), reverse=True)
    return out, pd.DataFrame(missing)


def embed_config_into_workbook(wb, cfg: Dict[str, Any]) -> None:
    """Save cfg JSON into a hidden sheet so settings can travel with the Excel file."""
    if EXCEL_CONFIG_SHEET in wb.sheetnames:
        ws_cfg = wb[EXCEL_CONFIG_SHEET]
    else:
        ws_cfg = wb.create_sheet(EXCEL_CONFIG_SHEET)
    ws_cfg[EXCEL_CONFIG_CELL].value = json.dumps(cfg, ensure_ascii=False)
    try:
        ws_cfg.sheet_state = "hidden"
    except Exception:
        pass


def extract_config_from_workbook_bytes(xlsx_bytes: bytes) -> Dict[str, Any]:
    wb = load_workbook(io.BytesIO(xlsx_bytes))
    if EXCEL_CONFIG_SHEET not in wb.sheetnames:
        raise ValueError("ì—‘ì…€ì— ì €ì¥ëœ ì„¤ì • ì‹œíŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
    ws_cfg = wb[EXCEL_CONFIG_SHEET]
    raw = ws_cfg[EXCEL_CONFIG_CELL].value
    if not raw:
        raise ValueError("ì—‘ì…€ ì„¤ì • ì…€(A1)ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
    cfg = json.loads(str(raw))
    if not isinstance(cfg, dict):
        raise ValueError("ì—‘ì…€ ì„¤ì • í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤.")
    cfg.setdefault("version", 9)
    cfg.setdefault("inventory_column", "ì¬ê³ ìˆ˜ëŸ‰")
    cfg.setdefault("name_column", "ìƒí’ˆëª…")
    cfg.setdefault("products", [])
    cfg.setdefault("rules", {})
    return cfg




def update_workbook_bytes(
    xlsx_bytes: bytes,
    cfg: Dict[str, Any],
    inputs: Dict[str, float],
) -> Tuple[bytes, pd.DataFrame, pd.DataFrame, bytes]:
    """
    Returns:
      - updated workbook bytes
      - df_changes (summary)
      - df_missing (map missing table)
      - changed_rows_only workbook bytes (header + changed rows)
    """
    inv_col = cfg.get("inventory_column", "ì¬ê³ ìˆ˜ëŸ‰")
    name_col = cfg.get("name_column", "ìƒí’ˆëª…")

    wb = load_workbook(io.BytesIO(xlsx_bytes))
    ws = wb.active

    header_row, name_idx, inv_idx = find_header_row_and_columns(ws, name_col=name_col, inv_col=inv_col)

    actions, df_missing = build_actions(cfg, inputs)

    def match_action(a: Dict[str, Any], name_str: str) -> bool:
        toks = a.get("match_tokens") or []
        return bool(toks) and all((t in name_str) for t in toks)

    changes = []
    changed_row_indices: List[int] = []

    for r in range(header_row + 1, ws.max_row + 1):
        name_val = ws.cell(r, name_idx).value
        if name_val is None or str(name_val).strip() == "":
            continue

        # skip guideline rows ("í•„ìˆ˜")
        first_cell = ws.cell(r, 1).value
        if isinstance(first_cell, str) and first_cell.strip() in ("í•„ìˆ˜",):
            continue
        if isinstance(name_val, str) and name_val.strip() in ("í•„ìˆ˜",):
            continue

        name_str = str(name_val)
        matched = [a for a in actions if match_action(a, name_str)]
        if not matched:
            continue

        delta = sum(float(m["delta"]) for m in matched)
        if abs(delta) < 1e-12:
            continue

        old = to_number(ws.cell(r, inv_idx).value)
        new = old + delta
        ws.cell(r, inv_idx).value = new

        # record changed row index (to keep same format, we will delete other rows later)
        changed_row_indices.append(r)

        changes.append({
            "í–‰ë²ˆí˜¸": r,
            "ìƒí’ˆëª…": name_str,
            "ê¸°ì¡´ì¬ê³ ": old,
            "ì¦ê°": delta,
            "ìµœì¢…ì¬ê³ ": new,
            "ë§¤ì¹­í‚¤ì›Œë“œ": ", ".join([str(m.get("display") or " & ".join(m.get("match_tokens", []))) for m in matched]),
            "ì›ì²œìƒí’ˆ": ", ".join(sorted({b for m in matched for b in m.get("bases", [])})),
        })

    # Embed current config into output Excel (portable persistence)
    embed_config_into_workbook(wb, cfg)

    out = io.BytesIO()
    wb.save(out)
    out.seek(0)
    updated_bytes = out.getvalue()
    # Build "changed rows only" workbook while keeping the same format/spec as the uploaded Excel:
    # 1) reload the UPDATED workbook bytes (so styles/column widths/etc. are preserved)
    # 2) delete all non-changed data rows (below header_row) from bottom to top
    wb2 = load_workbook(io.BytesIO(updated_bytes))
    ws2 = wb2.active

    keep = set(changed_row_indices)
    # delete from bottom to avoid shifting
    for rr in range(ws2.max_row, header_row, -1):
        if rr not in keep:
            ws2.delete_rows(rr, 1)

    out2 = io.BytesIO()
    wb2.save(out2)
    out2.seek(0)
    changed_rows_bytes = out2.getvalue()

    return updated_bytes, pd.DataFrame(changes), df_missing, changed_rows_bytes


# ============================
# UI
# ============================
st.set_page_config(page_title="ìƒí’ˆìˆ˜ëŸ‰ ì¼ê´„ë³€ê²½", layout="wide")
cfg = load_config()

# ìë™ë³µì›: ìƒí’ˆëª©ë¡ì´ ë¹„ì–´ìˆìœ¼ë©´ ê¸°ë³¸ 34ê°œë¥¼ ì±„ì›Œë„£ìŠµë‹ˆë‹¤.
if not cfg.get("products"):
    cfg["products"] = DEFAULT_PRODUCTS.copy()
    save_config(cfg)

st.title("ìƒí’ˆìˆ˜ëŸ‰ ì¼ê´„ë³€ê²½")

# ============================
# Sidebar navigation
# ============================
st.sidebar.title("ë©”ë‰´")
page = st.sidebar.radio(
    "ì´ë™",
    options=["â‘  ì¬ê³  ì…ë ¥", "â‘¡ ìƒí’ˆëª©ë¡ ê´€ë¦¬", "â‘¢ ê·œì¹™ ê´€ë¦¬", "â‘£ ë°±ì—…/ë³µì›"],
    index=0,  # âœ… ì‹œì‘ í˜ì´ì§€: ì¬ê³  ì…ë ¥
)

st.sidebar.caption("ì¬ê³  ì…ë ¥ì´ ê¸°ë³¸ í™”ë©´ì´ë©°, ë‚˜ë¨¸ì§€ ê´€ë¦¬ í˜ì´ì§€ëŠ” ì‚¬ì´ë“œë°”ì—ì„œ ì´ë™í•©ë‹ˆë‹¤.")

# ============================
# Pages
# ============================
if page.startswith("â‘ "):
    st.subheader("ì—‘ì…€ íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°")
    uploaded = st.file_uploader("ìŠ¤ë§ˆíŠ¸ìŠ¤í† ì–´ ìˆ˜ì •ì–‘ì‹ ì—‘ì…€(.xlsx)ì„ ì—…ë¡œë“œí•˜ì„¸ìš”", type=["xlsx"], key="xlsx_uploader")

    # ì—…ë¡œë“œ ë°”ì´íŠ¸(ì¤‘ë³µ getvalue() ë°©ì§€)
    uploaded_bytes = uploaded.getvalue() if uploaded is not None else None

    # âœ… 'ë©”ëª¨ì¥ìœ¼ë¡œ ì €ì¥' ë“±ìœ¼ë¡œ rerun ë˜ì–´ë„, ë§ˆì§€ë§‰ ì ìš© ê²°ê³¼(ë‹¤ìš´ë¡œë“œ/ë³€ê²½í‘œ)ê°€ ì‚¬ë¼ì§€ì§€ ì•Šê²Œ ìœ ì§€
    if "last_apply_result" not in st.session_state:
        st.session_state.last_apply_result = None

    def _fingerprint_upload(name: str, b: bytes) -> str:
        md5 = hashlib.md5(b).hexdigest()
        return f"{name}:{len(b)}:{md5}"

    current_fp = (
        _fingerprint_upload(getattr(uploaded, "name", "") or "", uploaded_bytes)
        if uploaded_bytes is not None
        else None
    )

    # ë‹¤ë¥¸ ì—‘ì…€ì„ ìƒˆë¡œ ì—…ë¡œë“œí•˜ë©´, ì´ì „ ê²°ê³¼ëŠ” ìë™ìœ¼ë¡œ ìˆ¨ê¹€ ì²˜ë¦¬
    _last = st.session_state.get("last_apply_result")
    if _last is not None:
        if (current_fp is None) or (_last.get("fingerprint") != current_fp):
            st.session_state.last_apply_result = None


    st.subheader("ì…ë ¥í•  ìˆ˜ëŸ‰")
    prod_list = cfg.get("products", [])
    if not prod_list:
        st.info("ìƒí’ˆëª©ë¡ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. ì‚¬ì´ë“œë°”ì˜ 'ìƒí’ˆëª©ë¡ ê´€ë¦¬'ì—ì„œ ìƒí’ˆì„ ë¨¼ì € ì¶”ê°€í•˜ì„¸ìš”.")
    else:
        # ì—…ë¡œë“œëœ ì—‘ì…€ë¡œë¶€í„° 'ì¬ê³ ìˆ˜ëŸ‰(í‘œì‹œìš©)'ì„ ê³„ì‚° (ì…ë ¥/ì €ì¥ì—ëŠ” ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ)
        stock_map: Dict[str, str] = {}
        if uploaded is not None:
            try:
                stock_map = compute_stock_display_map(uploaded_bytes, cfg)
            except Exception:
                stock_map = {}

        current_names = [str(p.get("name", "")).strip() for p in prod_list if str(p.get("name", "")).strip()]
        if not current_names:
            st.info("ìƒí’ˆëª©ë¡ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. ì‚¬ì´ë“œë°”ì˜ 'ìƒí’ˆëª©ë¡ ê´€ë¦¬'ì—ì„œ ìƒí’ˆì„ ë¨¼ì € ì¶”ê°€í•˜ì„¸ìš”.")
        else:
                        # ì…ë ¥ê°’ì€ 'âœ… ì—‘ì…€ì— ì ìš©í•˜ê¸°'ë¥¼ ëˆŒë €ì„ ë•Œë§Œ í™•ì •(ì €ì¥)ë©ë‹ˆë‹¤.
            def _align_qty_df(_df: pd.DataFrame, _names: List[str]) -> pd.DataFrame:
                try:
                    if _df is not None and not _df.empty and "ìƒí’ˆ" in _df.columns and "ì…ë ¥ìˆ˜ëŸ‰" in _df.columns:
                        _m = _df.set_index("ìƒí’ˆ")["ì…ë ¥ìˆ˜ëŸ‰"].to_dict()
                    else:
                        _m = {}
                except Exception:
                    _m = {}

                return pd.DataFrame(
                    {
                        "ìƒí’ˆ": _names,
                        "ì…ë ¥ìˆ˜ëŸ‰": ["" if (_m.get(n) is None) else _m.get(n, "") for n in _names],
                    }
                )

            # ë§ˆì§€ë§‰ìœ¼ë¡œ 'ì ìš©(í™•ì •)'ëœ ê°’(=ì—‘ì…€ ì ìš©ì— ì‚¬ìš©ë˜ëŠ” ê°’)
            if "qty_committed_df" not in st.session_state:
                st.session_state.qty_committed_df = pd.DataFrame(
                    {"ìƒí’ˆ": current_names, "ì…ë ¥ìˆ˜ëŸ‰": [""] * len(current_names)}
                )
            st.session_state.qty_committed_df = _align_qty_df(st.session_state.qty_committed_df, current_names)
            # --- ì°¸ê³ ìˆ˜ëŸ‰: ì €ì¥ê°’(cfg) + ì„¸ì…˜ í¸ì§‘ì¤‘(draft) í•©ì³ì„œ í‘œì‹œ ---
            ref_saved = (cfg.get("ref_qty") or {})
            if "ref_qty_draft" not in st.session_state:
                st.session_state.ref_qty_draft = {}
            ref_draft: Dict[str, Any] = st.session_state.ref_qty_draft or {}

            def _ref_value(_name: str) -> str:
                # draftì— í‚¤ê°€ ìˆìœ¼ë©´(ë¹ˆê°’ í¬í•¨) draftë¥¼ ìš°ì„ í•©ë‹ˆë‹¤.
                if _name in ref_draft:
                    v = ref_draft.get(_name)
                    return "" if v is None else str(v)
                v2 = ref_saved.get(_name)
                return "" if v2 is None else str(v2)

            # í‘œ í‘œì‹œìš©(ì¬ê³ ìˆ˜ëŸ‰/ì°¸ê³ ìˆ˜ëŸ‰ì€ í‘œì‹œ/ì…ë ¥ë§Œ)
            df_view = st.session_state.qty_committed_df.copy()
            df_view["ì¬ê³ ìˆ˜ëŸ‰"] = [stock_map.get(n, "") for n in df_view["ìƒí’ˆ"]]
            df_view["ì°¸ê³ ìˆ˜ëŸ‰"] = [_ref_value(n) for n in df_view["ìƒí’ˆ"]]
            df_view = df_view[["ìƒí’ˆ", "ì…ë ¥ìˆ˜ëŸ‰", "ì¬ê³ ìˆ˜ëŸ‰", "ì°¸ê³ ìˆ˜ëŸ‰"]]

            st.caption("â€» í‘œì— ê°’ì„ ì…ë ¥í•´ë„ ì¦‰ì‹œ ì €ì¥/ì ìš©ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤. **âœ… ì—‘ì…€ì— ì ìš©í•˜ê¸°**ë¥¼ ëˆŒëŸ¬ì•¼ ì—‘ì…€ì— ë°˜ì˜ë©ë‹ˆë‹¤.")
            st.caption("â€» 'ì…ë ¥ìˆ˜ëŸ‰'ì„ ë¹„ì›Œë‘ë©´ 0ìœ¼ë¡œ ì²˜ë¦¬ë©ë‹ˆë‹¤. (ì˜ˆ: ì—”ë‹¤ì´ë¸Œ - 0)")
            st.caption("â€» 'ì°¸ê³ ìˆ˜ëŸ‰'ì€ **ğŸ’¾ ì°¸ê³ ìˆ˜ëŸ‰ ì €ì¥**ì„ ëˆŒëŸ¬ì•¼ ì €ì¥ë˜ë©°, ì €ì¥ í›„ì—ëŠ” ìˆ˜ì • ì „ê¹Œì§€ ìœ ì§€ë©ë‹ˆë‹¤.")

            # ìƒí’ˆëª©ë¡ì´ ë³€ê²½ë˜ë©´ í¸ì§‘ ìƒíƒœë¥¼ ì´ˆê¸°í™”í•˜ê¸° ìœ„í•´ keyë¥¼ ë³€ê²½í•©ë‹ˆë‹¤.
            _sig = hashlib.md5(("|".join(current_names)).encode("utf-8")).hexdigest()[:10]
            _editor_key = f"qty_editor_{_sig}"


            # data_editorê°€ rerun ë•Œ ê°’ì´ ì‚¬ë¼ì§€ëŠ” ê²ƒì„ ë°©ì§€:
            # - ê°™ì€ keyì˜ widget stateê°€ ìˆìœ¼ë©´ ê·¸ ê°’ì„ ìš°ì„  ì‚¬ìš©
            # - ì¬ê³ ìˆ˜ëŸ‰(í‘œì‹œìš©)ë§Œ ë§¤ë²ˆ ìƒˆë¡œ ê°±ì‹ 
            _df_for_editor = df_view
            if _editor_key in st.session_state and isinstance(st.session_state.get(_editor_key), pd.DataFrame):
                _prev = st.session_state.get(_editor_key).copy()
                try:
                    if "ìƒí’ˆ" in _prev.columns:
                        _prev["ìƒí’ˆ"] = _prev["ìƒí’ˆ"].astype(str)
                        _prev = _prev.set_index("ìƒí’ˆ").reindex(current_names).reset_index()
                except Exception:
                    _prev = df_view.copy()

                for _c in ["ì…ë ¥ìˆ˜ëŸ‰", "ì¬ê³ ìˆ˜ëŸ‰", "ì°¸ê³ ìˆ˜ëŸ‰"]:
                    if _c not in _prev.columns:
                        _prev[_c] = ""

                _prev["ì¬ê³ ìˆ˜ëŸ‰"] = [stock_map.get(n, "") for n in _prev["ìƒí’ˆ"]]
                _df_for_editor = _prev[["ìƒí’ˆ", "ì…ë ¥ìˆ˜ëŸ‰", "ì¬ê³ ìˆ˜ëŸ‰", "ì°¸ê³ ìˆ˜ëŸ‰"]]

            df_edit = st.data_editor(
                _df_for_editor,
                key=_editor_key,
                use_container_width=True,
                num_rows="fixed",
                disabled=["ìƒí’ˆ", "ì¬ê³ ìˆ˜ëŸ‰"],
                column_config={
                    "ì…ë ¥ìˆ˜ëŸ‰": st.column_config.TextColumn("ì…ë ¥ìˆ˜ëŸ‰", help="ìˆ«ì ì…ë ¥(ìŒìˆ˜/ì†Œìˆ˜ ê°€ëŠ¥). ì˜ˆ: 3, -2, 1.5"),
                    "ì¬ê³ ìˆ˜ëŸ‰": st.column_config.TextColumn("ì¬ê³ ìˆ˜ëŸ‰", help="ì—…ë¡œë“œí•œ ì—‘ì…€ ê¸°ì¤€, ëª¨ë“  ì˜µì…˜ ì¬ê³  í•©(í‘œì‹œìš©)"),
                    "ì°¸ê³ ìˆ˜ëŸ‰": st.column_config.TextColumn("ì°¸ê³ ìˆ˜ëŸ‰", help="ë©”ëª¨/ì°¸ê³ ìš© ìˆ˜ëŸ‰(ì €ì¥ ì‹œ ìœ ì§€). ì˜ˆ: 10"),
                },
            )


            # âœ… ì°¸ê³ ìˆ˜ëŸ‰ì€ ì €ì¥ ë²„íŠ¼ì„ ëˆ„ë¥´ê¸° ì „ì—ë„, ì…ë ¥í•œ ê°’ì´ ì„¸ì…˜ì—ì„œ ìœ ì§€ë˜ë„ë¡ draftì— ë°˜ì˜
            _draft_new: Dict[str, Any] = {}
            for _, _rr in df_edit.iterrows():
                _pname = str(_rr.get("ìƒí’ˆ", "")).strip()
                if not _pname:
                    continue

                _raw = _rr.get("ì°¸ê³ ìˆ˜ëŸ‰", "")
                if _raw is None:
                    _sval = ""
                elif isinstance(_raw, (int, float)):
                    _sval = fmt_qty_for_memo(float(_raw))
                else:
                    _sval = str(_raw).strip()
                    if _sval != "":
                        _clean = _sval.replace(",", "")
                        if re.fullmatch(r"[+-]?\d+(?:\.\d+)?", _clean):
                            _sval = fmt_qty_for_memo(parse_input_number(_sval))

                # ë¹ˆ ë¬¸ìì—´ë„ overrideë¡œ ì €ì¥(ì €ì¥ê°’ì´ ìˆì–´ë„ ì‚¬ìš©ìê°€ ì§€ìš´ ìƒíƒœ ìœ ì§€)
                _draft_new[_pname] = _sval

            st.session_state.ref_qty_draft = _draft_new

            # âœ… í˜„ì¬ í‘œ(í¸ì§‘ì¤‘ ê°’) ê¸°ì¤€ìœ¼ë¡œ ì…ë ¥ê°’/ë©”ëª¨ ìƒì„± (ë¹ˆì¹¸ì€ 0)
            df_inputs = df_edit.drop(columns=["ì¬ê³ ìˆ˜ëŸ‰", "ì°¸ê³ ìˆ˜ëŸ‰"], errors="ignore").copy()

            inputs: Dict[str, float] = {}
            memo_lines: List[str] = []
            for _, r in df_inputs.iterrows():
                name = str(r.get("ìƒí’ˆ", "")).strip()
                if not name:
                    continue
                qty = parse_input_number(r.get("ì…ë ¥ìˆ˜ëŸ‰", ""))
                inputs[name] = qty
                memo_lines.append(f"{name} - {fmt_qty_for_memo(qty)}")
            memo_text = "\n".join(memo_lines)

            col_a, col_b, col_c = st.columns(3, gap="small")
            with col_a:
                apply_clicked = st.button(
                    "âœ… ì—‘ì…€ì— ì ìš©í•˜ê¸°",
                    disabled=(uploaded is None),
                    use_container_width=True,
                )
            with col_b:
                memo_filename = "ì£¼ë¬¸ì–‘ì‹.txt"
                st.download_button(
                    "ğŸ“ ë©”ëª¨ì¥ìœ¼ë¡œ ì €ì¥",
                    data=memo_text.encode("utf-8"),
                    file_name=memo_filename,
                    mime="text/plain",
                    disabled=(len(memo_lines) == 0),
                    help="í˜„ì¬ í‘œ(í¸ì§‘ì¤‘ ê°’) ê¸°ì¤€ìœ¼ë¡œ í…ìŠ¤íŠ¸(.txt)ë¥¼ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤. (ë¹ˆì¹¸=0)",
                    use_container_width=True,
                )
            with col_c:
                ref_save_clicked = st.button(
                    "ğŸ’¾ ì°¸ê³ ìˆ˜ëŸ‰ ì €ì¥",
                    use_container_width=True,
                    key=f"save_ref_qty_{_editor_key}",
                )

            # ğŸ’¾ ì°¸ê³ ìˆ˜ëŸ‰ ì €ì¥ ì²˜ë¦¬(ì €ì¥ í›„ ìˆ˜ì • ì „ê¹Œì§€ ìœ ì§€)
            if ref_save_clicked:
                ref_map_new: Dict[str, Any] = {}
                for _, rr in df_edit.iterrows():
                    pname = str(rr.get("ìƒí’ˆ", "")).strip()
                    if not pname:
                        continue

                    raw = rr.get("ì°¸ê³ ìˆ˜ëŸ‰", "")
                    if raw is None:
                        sval = ""
                    elif isinstance(raw, (int, float)):
                        sval = fmt_qty_for_memo(float(raw))
                    else:
                        sval = str(raw).strip()
                        if sval != "":
                            s_clean = sval.replace(",", "")
                            if re.fullmatch(r"[+-]?\d+(?:\.\d+)?", s_clean):
                                sval = fmt_qty_for_memo(parse_input_number(sval))

                    if sval == "":
                        continue
                    ref_map_new[pname] = sval

                cfg["ref_qty"] = ref_map_new
                save_config(cfg)
                st.session_state.ref_qty_draft = {}
                st.success("ì°¸ê³ ìˆ˜ëŸ‰ ì €ì¥ ì™„ë£Œ!")
                st.rerun()

            if apply_clicked:
                # âœ… ì ìš© ë²„íŠ¼ì„ ëˆŒë €ì„ ë•Œë§Œ 'í™•ì •(ì €ì¥)' + ì—‘ì…€ ë°˜ì˜
                st.session_state.qty_committed_df = df_inputs.copy()

                if uploaded_bytes is None:
                    st.warning("ì—‘ì…€ íŒŒì¼ì„ ì—…ë¡œë“œí•´ ì£¼ì„¸ìš”.")
                    st.session_state.last_apply_result = None
                else:
                    try:
                        updated_bytes, df_changes, df_missing, changed_rows_bytes = update_workbook_bytes(
                            uploaded_bytes, cfg, inputs
                        )
                    except Exception as e:
                        st.error(f"ì ìš© ì¤‘ ì˜¤ë¥˜: {e}")
                        st.session_state.last_apply_result = None
                    else:
                        out_name_changed = "ì¬ê³ ìˆ˜ëŸ‰ì¼ê´„ë³€ê²½.xlsx"

                        # âœ… ê²°ê³¼ë¥¼ session_stateì— ì €ì¥ (ë©”ëª¨ì¥ ë‹¤ìš´ë¡œë“œ ë“± rerunì—ë„ ìœ ì§€)
                        st.session_state.last_apply_result = {
                            "fingerprint": current_fp,
                            "out_name": out_name_changed,
                            "changed_rows_bytes": changed_rows_bytes,
                            "df_changes": df_changes,
                            "df_missing": df_missing,
                            "applied_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                        }

            # âœ… ë§ˆì§€ë§‰ ì ìš© ê²°ê³¼ í‘œì‹œ(ë²„íŠ¼/í‘œê°€ rerunìœ¼ë¡œ ì‚¬ë¼ì§€ì§€ ì•ŠìŒ)
            _last = st.session_state.get("last_apply_result")
            if _last is not None:
                st.success(f"ì™„ë£Œ! ì•„ë˜ì—ì„œ ë‹¤ìš´ë¡œë“œí•˜ì„¸ìš”. (ë§ˆì§€ë§‰ ì ìš©: {_last.get('applied_at', '')})")

                st.download_button(
                    "â¬‡ï¸ ë‹¤ìš´ë¡œë“œ",
                    data=_last["changed_rows_bytes"],
                    file_name=_last["out_name"],
                    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                    help="ì¬ê³ ìˆ˜ëŸ‰ì´ ë³€ê²½ëœ í–‰(í—¤ë” í¬í•¨)ë§Œ ë‚¨ê¸´ íŒŒì¼ì…ë‹ˆë‹¤.",
                    use_container_width=True,
                    key=f"excel_dl_{_last.get('fingerprint', '')}",
                )

                df_changes = _last.get("df_changes")
                df_missing = _last.get("df_missing")

                if isinstance(df_changes, pd.DataFrame):
                    if df_changes.empty:
                        st.info("ë³€ê²½ëœ í–‰ì´ ì—†ìŠµë‹ˆë‹¤. (í‚¤ì›Œë“œ ë§¤ì¹­ì´ ì•ˆ ëê±°ë‚˜ ì…ë ¥ì´ 0ì´ê±°ë‚˜, map ëª¨ë“œì—ì„œ ì…ë ¥ê°’ì´ ë§¤í•‘ì— ì—†ì„ ìˆ˜ ìˆì–´ìš”)")
                    else:
                        st.dataframe(df_changes.drop(columns=["í–‰ë²ˆí˜¸"], errors="ignore"), use_container_width=True)

                if isinstance(df_missing, pd.DataFrame) and (not df_missing.empty):
                    st.warning("âš ï¸ map(ë§¤í•‘) ê·œì¹™ì—ì„œ 'ì…ë ¥ê°’ â†’ ì ìš©ê°’'ì´ ì •ì˜ë˜ì§€ ì•Šì•„ ì ìš©ë˜ì§€ ì•Šì€ í•­ëª©ì´ ìˆìŠµë‹ˆë‹¤.")
                    st.dataframe(df_missing, use_container_width=True)

elif page.startswith("â‘¡"):
    st.subheader("ìƒí’ˆëª©ë¡ ì¶”ê°€/ì‚­ì œ/ìˆ˜ì •")

    prod_list = cfg.get("products", [])
    if prod_list:
        df_prod_raw = pd.DataFrame(prod_list)
    else:
        df_prod_raw = pd.DataFrame(columns=["keyword", "name"])

    # ensure required columns exist
    for _c in ["keyword", "name"]:
        if _c not in df_prod_raw.columns:
            df_prod_raw[_c] = ""

    # âœ… ì»¬ëŸ¼ ìˆœì„œ: í‚¤ì›Œë“œ(íŒ¨í„´) -> í‘œì‹œë  ìƒí’ˆëª…
    df_prod = df_prod_raw[["keyword", "name"]].rename(
        columns={
            "keyword": "ì‹¤ì œ ìƒí’ˆëª…(íŒ¨í„´)",
            "name": "í‘œì‹œë  ìƒí’ˆëª…",
        }
    )

    st.write("â€¢ **ì‹¤ì œ ìƒí’ˆëª…(íŒ¨í„´)**ì€ ì—‘ì…€ 'ìƒí’ˆëª…'ì—ì„œ ë§¤ì¹­í•  ë¬¸ìì—´ì…ë‹ˆë‹¤. (ë¹„ìš°ë©´ 'í‘œì‹œë  ìƒí’ˆëª…'ê³¼ ë™ì¼í•˜ê²Œ ì²˜ë¦¬)")
    st.caption("â€» í‘œì—ì„œ ì…ë ¥/ìˆ˜ì • í›„ **ì €ì¥ ë²„íŠ¼ì„ ëˆŒëŸ¬ì•¼** ì„¤ì •ì´ ì €ì¥ë©ë‹ˆë‹¤.")

    with st.form("prod_form", clear_on_submit=False):
        df_prod_edit = st.data_editor(
            df_prod,
            key="prod_editor",
            use_container_width=True,
            num_rows="dynamic",
            column_config={
                "ì‹¤ì œ ìƒí’ˆëª…(íŒ¨í„´)": st.column_config.TextColumn("ì‹¤ì œ ìƒí’ˆëª…(íŒ¨í„´)"),
                "í‘œì‹œë  ìƒí’ˆëª…": st.column_config.TextColumn("í‘œì‹œë  ìƒí’ˆëª…"),
            },
        )
        save_prod = st.form_submit_button("ğŸ’¾ ìƒí’ˆëª©ë¡ ì €ì¥", type="primary", use_container_width=True)

    if save_prod:
        cleaned = []
        for _, r in df_prod_edit.iterrows():
            name = str(r.get("í‘œì‹œë  ìƒí’ˆëª…", "")).strip()
            if not name:
                continue
            kw = str(r.get("ì‹¤ì œ ìƒí’ˆëª…(íŒ¨í„´)", "")).strip() or name
            cleaned.append({"name": name, "keyword": kw})

        cfg["products"] = cleaned
        # drop rules of deleted products
        rules = cfg.get("rules", {}) or {}
        rules = {k: v for k, v in rules.items() if k in {p["name"] for p in cleaned}}
        cfg["rules"] = rules

        save_config(cfg)
        st.success("ìƒí’ˆëª©ë¡ ì €ì¥ ì™„ë£Œ!")
        st.rerun()

elif page.startswith("â‘¢"):
        st.subheader("ê·œì¹™ ì¶”ê°€/ì‚­ì œ/ìˆ˜ì •")



        # ğŸ” ì¸ì‹ë¡œì§ ê´€ë¦¬ëŠ” ì•„ë˜ìª½(ê·œì¹™ ì €ì¥ ë²„íŠ¼ ì•„ë˜)ì— ìˆìŠµë‹ˆë‹¤.

        prod_names = [p.get("name") for p in cfg.get("products", []) if p.get("name")]
        if not prod_names:
            st.info("ë¨¼ì € 'â‘¡ ìƒí’ˆëª©ë¡ ê´€ë¦¬'ì—ì„œ ìƒí’ˆì„ ì¶”ê°€í•˜ì„¸ìš”.")
        else:
            base = st.selectbox("ê·œì¹™ì„ í¸ì§‘í•  ê¸°ì¤€ ìƒí’ˆ", options=prod_names)

            st.markdown(
                """
    - **mul(ë°°ìˆ˜)**: `ì…ë ¥ìˆ˜ëŸ‰ Ã— value` ë§Œí¼ ë”í•¨  
    - **map(ë§¤í•‘)**: ì…ë ¥ê°’ë³„ë¡œ ë”± ì •í•œ ê°’ë§Œ ë”í•¨ (ì˜ˆ: `1=2, 2=3`)  
      - mapì€ **ë§¤í•‘(table)** ì¹¸ì— `ì…ë ¥ê°’=ì ìš©ê°’`ì„ `,`ë¡œ êµ¬ë¶„í•´ì„œ ì ì–´ìš”.
    - **í‚¤ì›Œë“œ ìë™ ìŠ¤ì½”í”„(ì¤‘ìš”)**: ê¸°ì¤€ìƒí’ˆì„ ì˜ˆ) **ì—”ë‹¤ì´ë¸Œ**ë¡œ ì„ íƒí•œ ìƒíƒœì—ì„œ í‚¤ì›Œë“œë¥¼ `1kg`ì²˜ëŸ¼ **ì˜µì…˜ë§Œ** ì“°ë©´,  
      ì—‘ì…€ ìƒí’ˆëª…ì— `ì—”ë‹¤ì´ë¸Œ`ì™€ `1kg`ê°€ **ë‘˜ ë‹¤ í¬í•¨ëœ í–‰ë§Œ** ì ìš©ë©ë‹ˆë‹¤. (ë‹¤ë¥¸ ìƒí’ˆì˜ `1kg`ëŠ” ì˜í–¥ ì—†ìŒ)  
      ì´ë¯¸ `ì—”ë‹¤ì´ë¸Œ1kg`ì²˜ëŸ¼ **ì „ì²´ ë¬¸ìì—´**ì„ ì“°ë©´ ê·¸ëŒ€ë¡œ ê·¸ ë¬¸ìì—´ë¡œ ë§¤ì¹­í•©ë‹ˆë‹¤.
                """
            )

            rule_list = (cfg.get("rules", {}) or {}).get(base, [])
            ui_rows = []
            for rr in rule_list:
                rr2 = Rule.from_dict(rr)
                table = rr2.table or {}

                def _sort_key(x):
                    try:
                        return float(x) if x != "*" else 1e19
                    except Exception:
                        return 1e19

                map_str = ", ".join([f"{k}={fmt_int(table[k])}" for k in sorted(table.keys(), key=_sort_key)]) if table else ""

                ui_rows.append({
                    "í‚¤ì›Œë“œ(ì—‘ì…€ ìƒí’ˆëª… í¬í•¨ ë¬¸ìì—´)": rr2.keyword,
                    "ëª¨ë“œ": rr2.mode,
                    "value": fmt_int(rr2.value),
                    "ë§¤í•‘(table) - map ëª¨ë“œì—ì„œë§Œ": map_str,
                })

            df_rule = pd.DataFrame(ui_rows) if ui_rows else pd.DataFrame(columns=[
                "í‚¤ì›Œë“œ(ì—‘ì…€ ìƒí’ˆëª… í¬í•¨ ë¬¸ìì—´)", "ëª¨ë“œ", "value", "ë§¤í•‘(table) - map ëª¨ë“œì—ì„œë§Œ"
            ])

            df_rule_edit = st.data_editor(
                df_rule,
                key="rule_editor",
                use_container_width=True,
                num_rows="dynamic",
                column_config={
                    "í‚¤ì›Œë“œ(ì—‘ì…€ ìƒí’ˆëª… í¬í•¨ ë¬¸ìì—´)": st.column_config.TextColumn("í‚¤ì›Œë“œ"),
                    "ëª¨ë“œ": st.column_config.SelectboxColumn("ëª¨ë“œ", options=["mul", "map"]),
                    "value": st.column_config.TextColumn("value", help="mul ëª¨ë“œì—ì„œë§Œ ì‚¬ìš© (ì •ìˆ˜)"),
                    "ë§¤í•‘(table) - map ëª¨ë“œì—ì„œë§Œ": st.column_config.TextColumn(
                        "ë§¤í•‘(table)",
                        help="ì˜ˆ: 1=2, 2=3\n(ì¤„ë°”ê¿ˆë„ ê°€ëŠ¥)",
                    ),
                },
            )

            if st.button("ğŸ’¾ ê·œì¹™ ì €ì¥"):
                cleaned = []
                for _, r in df_rule_edit.iterrows():
                    kw = str(r.get("í‚¤ì›Œë“œ(ì—‘ì…€ ìƒí’ˆëª… í¬í•¨ ë¬¸ìì—´)", "")).strip()
                    if not kw:
                        continue
                    mode = str(r.get("ëª¨ë“œ", "mul")).strip()
                    val_raw = r.get("value", 1)
                    if str(val_raw).strip() == "":
                        val = 1
                    else:
                        try:
                            val = int(round(parse_input_number(val_raw)))
                        except Exception:
                            val = 1

                    table_str = str(r.get("ë§¤í•‘(table) - map ëª¨ë“œì—ì„œë§Œ", "") or "")
                    table = parse_map_string(table_str) if mode == "map" else {}

                    cleaned.append({
                        "keyword": kw,
                        "mode": mode,
                        "value": val,
                        "table": table,
                    })

                rules = cfg.get("rules", {}) or {}
                rules[base] = cleaned
                cfg["rules"] = rules
                save_config(cfg)
                st.success("ê·œì¹™ ì €ì¥ ì™„ë£Œ!")
                st.rerun()


        st.write("")

        # ----------------------------
        # Recognition logic editor (moved from sidebar)
        # ----------------------------
        with st.expander("ğŸ” ì¸ì‹ë¡œì§ ê´€ë¦¬(ë‹¨ìœ„ ì¸ì‹)", expanded=False):
            st.caption("ìƒí’ˆëª…/ê·œì¹™ í‚¤ì›Œë“œì—ì„œ ë‹¨ìœ„ë¥¼ ì¸ì‹í•˜ëŠ” ë¡œì§ì…ë‹ˆë‹¤. (priorityëŠ” ë‚´ë¶€ì ìœ¼ë¡œ í–‰ ìˆœì„œë¡œ ìë™ ë¶€ì—¬ë©ë‹ˆë‹¤)")

            logic_rows = _get_recognition_logic(cfg)
            df_logic = pd.DataFrame(
                [
                    {
                        "priority": int(r.get("priority", 999)),
                        "output_unit": str(r.get("output_unit", "")),
                        "multiplier": str(r.get("multiplier", "1")),
                        "aliases": ", ".join([str(a) for a in (r.get("aliases") or [])]),
                    }
                    for r in logic_rows
                ]
            )
            if df_logic.empty:
                df_logic = pd.DataFrame(columns=["priority", "output_unit", "multiplier", "aliases"])

            # í™”ë©´ì—ì„œëŠ” priorityë¥¼ ìˆ¨ê¸°ê³ , ì €ì¥ ì‹œ í–‰ ìˆœì„œëŒ€ë¡œ ìë™ ë¶€ì—¬í•©ë‹ˆë‹¤.
            df_ui = df_logic.sort_values("priority").reset_index(drop=True).drop(columns=["priority"], errors="ignore")

            edited = st.data_editor(
                df_ui,
                use_container_width=True,
                num_rows="dynamic",
                hide_index=True,
                column_config={
                    "output_unit": st.column_config.TextColumn("output_unit", help="ìµœì¢… í‘œì‹œ ë‹¨ìœ„ (ì˜ˆ: ë‹¨, kg, íŒ©)"),
                    "multiplier": st.column_config.TextColumn("multiplier", help="ìˆ«ìì— ê³±í•´ì§€ëŠ” ê°’ (ì˜ˆ: gâ†’kg í™˜ì‚°ì€ 0.001)"),
                    "aliases": st.column_config.TextColumn("aliases (ì‰¼í‘œë¡œ êµ¬ë¶„)", help="ì¸ì‹í•  ë¬¸ìì—´ë“¤. ì˜ˆ: kg,í‚¬ë¡œ,í‚¤ë¡œ"),
                },
                key="recognition_logic_editor",
            )

            c1, c2 = st.columns(2)
            if c1.button("ğŸ’¾ ì¸ì‹ë¡œì§ ì €ì¥", use_container_width=True, key="save_recognition_logic"):
                raw_rows: List[Dict[str, Any]] = []
                try:
                    for idx, row in edited.iterrows():
                        unit = str(row.get("output_unit", "")).strip()
                        aliases_raw = str(row.get("aliases", "")).strip()
                        if not unit or not aliases_raw:
                            continue

                        mult = str(row.get("multiplier", "1")).strip() or "1"
                        aliases = [a.strip() for a in aliases_raw.split(",") if a.strip()]

                        # âœ… priorityëŠ” í™”ë©´ì— ìˆ¨ê¸°ê³ , í˜„ì¬ í–‰ ìˆœì„œëŒ€ë¡œ ìë™ ë¶€ì—¬
                        pr = int((idx + 1) * 10)

                        raw_rows.append({"priority": pr, "output_unit": unit, "multiplier": mult, "aliases": aliases})

                    cfg["recognition_logic"] = _normalize_recognition_logic(raw_rows)
                    save_config(cfg)
                    st.success("ì¸ì‹ë¡œì§ ì €ì¥ ì™„ë£Œ!")
                    st.rerun()
                except Exception as e:
                    st.error(f"ì €ì¥ ì‹¤íŒ¨: {e}")

            # â¬‡ï¸ ë‚´ë³´ë‚´ê¸°: í˜„ì¬ ì¸ì‹ë¡œì§ì„ JSONìœ¼ë¡œ ë‹¤ìš´ë¡œë“œ
            logic_now = _normalize_recognition_logic(cfg.get("recognition_logic")) or DEFAULT_RECOGNITION_LOGIC.copy()
            export_bytes = json.dumps(logic_now, ensure_ascii=False, indent=2).encode("utf-8")
            export_name = f"recognition_logic_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            c2.download_button("â¬‡ï¸ ë‚´ë³´ë‚´ê¸°(JSON)", data=export_bytes, file_name=export_name, mime="application/json", use_container_width=True)

            # ê°€ë“œ: 'ë‹¨' í•­ëª©ì´ ì—†ìœ¼ë©´ '1ë‹¨' í‚¤ì›Œë“œë„ kgë¡œ ë–¨ì–´ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
            units_now = [str(r.get("output_unit", "")).strip() for r in logic_now]
            if "ë‹¨" not in units_now:
                st.warning("âš ï¸ í˜„ì¬ ì¸ì‹ë¡œì§ì— 'ë‹¨' í•­ëª©ì´ ì—†ìŠµë‹ˆë‹¤. '1ë‹¨' í‚¤ì›Œë“œë¥¼ ì¨ë„ kgë¡œ ì¸ì‹ë  ìˆ˜ ìˆì–´ìš”.")

else:
    st.subheader("ì„¤ì • ë°±ì—…/ë³µì› (ìƒí’ˆëª©ë¡ + ê·œì¹™)")
    cfg_json = json.dumps(cfg, ensure_ascii=False, indent=2).encode("utf-8")
    st.download_button("â¬‡ï¸ ì„¤ì •(JSON) ë‹¤ìš´ë¡œë“œ", data=cfg_json, file_name="stock_config.json", mime="application/json")

    up_cfg = st.file_uploader("ì„¤ì •(JSON) ì—…ë¡œë“œí•˜ì—¬ ë³µì›", type=["json"], key="cfg_uploader")

    # âœ… ë³µì› ì™„ë£Œ ì•ŒëŒ: 3ì´ˆë§Œ í‘œì‹œ í›„ ìë™ìœ¼ë¡œ ì‚¬ë¼ì§
    if "restore_notice" not in st.session_state:
        st.session_state.restore_notice = None  # ("success"|"error", message)

    restore_clicked = st.button("â™»ï¸ ì„¤ì • ë³µì›", disabled=(up_cfg is None))
    notice_ph = st.empty()

    if restore_clicked:
        try:
            new_cfg = json.loads(up_cfg.getvalue().decode("utf-8"))
            if "products" not in new_cfg or "rules" not in new_cfg:
                raise ValueError("í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            cfg = new_cfg
            save_config(cfg)
            st.session_state.restore_notice = ("success", "ì„¤ì • ë³µì› ì™„ë£Œ! ë°”ë¡œ ë°˜ì˜ë©ë‹ˆë‹¤.")
        except Exception as e:
            st.session_state.restore_notice = ("error", f"ë³µì› ì‹¤íŒ¨: {e}")

    # ë²„íŠ¼ ë°”ë¡œ ì•„ë˜ì— ë³µì› ê²°ê³¼ í‘œì‹œ (3ì´ˆ í›„ ìë™ ì‚­ì œ)
    if st.session_state.restore_notice:
        kind, msg = st.session_state.restore_notice
        if kind == "success":
            notice_ph.success(msg)
        else:
            notice_ph.error(msg)

        time.sleep(3)
        notice_ph.empty()
        st.session_state.restore_notice = None
