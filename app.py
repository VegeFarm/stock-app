import io
import os
import re
import json
import math
import shutil
from decimal import Decimal
from datetime import datetime, timedelta, timezone
from pathlib import Path
from zoneinfo import ZoneInfo
from collections import defaultdict, OrderedDict
from typing import Optional, Tuple, List, Dict

import pandas as pd
import streamlit as st
import openpyxl

# -------------------- Optional deps --------------------
# Excel decrypt (SmartStore password 0000)
try:
    import msoffcrypto
except ModuleNotFoundError:
    msoffcrypto = None

# PDF screenshot render (PyMuPDF)
try:
    import fitz  # PyMuPDF (pymupdf)
except Exception:
    fitz = None

# Pillow (merge PNG pages -> one PNG)
try:
    from PIL import Image
except Exception:
    Image = None

# PDF text extract libs
try:
    import pdfplumber
except Exception:
    pdfplumber = None

try:
    from pypdf import PdfReader
except Exception:
    try:
        from PyPDF2 import PdfReader  # fallback
    except Exception:
        PdfReader = None

# ReportLab (PDFs)
from reportlab.platypus import (
    SimpleDocTemplate,
    Table,
    LongTable,
    TableStyle,
    Paragraph,
    Spacer,
    KeepTogether,
    HRFlowable,
)
from reportlab.pdfgen import canvas
from reportlab.lib.pagesizes import A4, landscape
from reportlab.lib import colors
from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
from reportlab.lib.units import mm
from reportlab.pdfbase import pdfmetrics
from reportlab.pdfbase.ttfonts import TTFont
from reportlab.pdfbase.cidfonts import UnicodeCIDFont


# =====================================================
# TIMEZONE
# =====================================================
KST_TZ = ZoneInfo("Asia/Seoul")
KST = timezone(timedelta(hours=9))  # for legacy functions


def now_prefix_kst() -> str:
    return datetime.now(KST).strftime("%Y%m%d_%H%M%S")


# =====================================================
# PATHS / STORAGE
# =====================================================
# ë°°í¬í™˜ê²½(Render ë“±)ì—ì„œ ì¬ì‹œì‘/ì¬ë°°í¬ í›„ì—ë„ ì„¤ì •/ë£°/ë‚´ë³´ë‚´ê¸° ë°ì´í„°ê°€ ìœ ì§€ë˜ë„ë¡,
# ì˜êµ¬ ì €ì¥ ë£¨íŠ¸ í´ë”ë¥¼ í™˜ê²½ë³€ìˆ˜/ë””ìŠ¤í¬ ë§ˆìš´íŠ¸ ê²½ë¡œë¡œ ìë™ ì„ íƒí•©ë‹ˆë‹¤.
#
# ê¶Œì¥(Render):
#  - Persistent Disk mount path: /var/data
#  - Environment Variable: APP_DATA_DIR=/var/data
#
# ì„ íƒ ìš°ì„ ìˆœìœ„:
#  1) APP_DATA_DIR í™˜ê²½ë³€ìˆ˜
#  2) /var/data ê°€ ì¡´ì¬í•˜ê³  ì“°ê¸° ê°€ëŠ¥í•˜ë©´ ì‚¬ìš©
#  3) í˜„ì¬ ì‘ì—… í´ë”(ë¡œì»¬ ì‹¤í–‰ìš©)
_env_dir = (os.environ.get("APP_DATA_DIR") or "").strip()

def _pick_writable_dir(cands: list[Path]) -> Path:
    for p in cands:
        try:
            p.mkdir(parents=True, exist_ok=True)
            test = p / ".write_test"
            with open(test, "w", encoding="utf-8") as f:
                f.write("ok")
            try:
                test.unlink()
            except Exception:
                pass
            return p
        except Exception:
            continue
    return Path(".")

_candidates: list[Path] = []
if _env_dir:
    _candidates.append(Path(_env_dir))
_candidates.extend([Path("/var/data"), Path(".")])

APP_DATA_DIR = _pick_writable_dir(_candidates)
print(f"[BOOT] APP_DATA_DIR_ENV='{_env_dir}' -> USING='{APP_DATA_DIR}'")

# (1) ì¬ê³ ê´€ë¦¬ ì €ì¥
INVENTORY_FILE = str(APP_DATA_DIR / "inventory.csv")

# (2) PACK/BOX/EA ê·œì¹™(ì œí’ˆë³„ í•©ê³„ ê³„ì‚°ìš©)
RULES_FILE = str(APP_DATA_DIR / "rules.txt")
COUNT_UNITS = ["ê°œ", "í†µ", "íŒ©", "ë´‰"]

# (3) 2ë²ˆ ì½”ë“œ(ì—‘ì…€ ì—…ë¡œë“œ/ë§¤ì¹­ ê·œì¹™) ë°ì´í„° ì €ì¥
DATA_DIR = APP_DATA_DIR / "data"
DATA_DIR.mkdir(parents=True, exist_ok=True)

MAPPING_PATH = DATA_DIR / "name_mappings.json"
EXPR_RULES_PATH = DATA_DIR / "expression_rules.json"
BACKUP_DIR = DATA_DIR / "rules_backup"
BACKUP_DIR.mkdir(parents=True, exist_ok=True)

# âœ… TC ì„¤ì • ì €ì¥ íŒŒì¼ (í”„ë¡œê·¸ë¨ ê»ë‹¤ ì¼œë„ ìœ ì§€)
TC_SETTINGS_PATH = DATA_DIR / "tc_settings.json"

# âœ… ìŠ¤í‹°ì»¤ ì œì™¸ ì„¤ì • ì €ì¥ íŒŒì¼ (í”„ë¡œê·¸ë¨ ê»ë‹¤ ì¼œë„ ìœ ì§€)
STICKER_SETTINGS_PATH = DATA_DIR / "sticker_settings.json"

# âœ… ë ˆí¬(ì•± í´ë”)ì— "TCì£¼ë¬¸_ë“±ë¡ì–‘ì‹.xlsx" íŒŒì¼ì„ ê°™ì´ ì˜¬ë ¤ë‘ë©´ ì—…ë¡œë“œ ì—†ì´ ìë™ ì‚¬ìš©
TC_TEMPLATE_DEFAULT_PATH = Path("TCì£¼ë¬¸_ë“±ë¡ì–‘ì‹.xlsx")

# âœ… SmartStore ì—‘ì…€ ë¹„ë²ˆ
EXCEL_PASSWORD = "0000"

# -------------------- Export helpers (inventory snapshots) --------------------
EXPORT_ROOT = str(APP_DATA_DIR / "exports")

# -------------------- Atomic write helpers --------------------
def _atomic_write_text(path: str | Path, text: str, encoding: str = "utf-8") -> None:
    p = Path(path)
    tmp = p.with_suffix(p.suffix + ".tmp")
    tmp.write_text(text, encoding=encoding)
    tmp.replace(p)


def _atomic_write_bytes(path: str | Path, data: bytes) -> None:
    p = Path(path)
    tmp = p.with_suffix(p.suffix + ".tmp")
    tmp.write_bytes(data)
    tmp.replace(p)



def kst_date_folder() -> str:
    return datetime.now(KST).strftime("%Y.%m.%d")


def ensure_export_root() -> str:
    try:
        os.makedirs(EXPORT_ROOT, exist_ok=True)
    except Exception:
        pass
    return EXPORT_ROOT


def export_inventory_snapshot(df: pd.DataFrame) -> tuple[str, str]:
    """
    ì¬ê³ í‘œ(df)ë¥¼ exports/YYYY.MM.DD/ì¬ê³ í‘œ_YYYY.MM.DD.xlsx ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
    ê°™ì€ ë‚ ì§œì— ì—¬ëŸ¬ ë²ˆ ë‚´ë³´ë‚´ê¸°ë¥¼ ëˆ„ë¥´ë©´ íŒŒì¼ì€ ë®ì–´ì”ë‹ˆë‹¤.
    """
    ensure_export_root()
    date_str = kst_date_folder()
    folder = os.path.join(EXPORT_ROOT, date_str)
    os.makedirs(folder, exist_ok=True)

    file_path = os.path.join(folder, f"ì¬ê³ í‘œ_{date_str}.xlsx")
    data = inventory_df_to_xlsx_bytes(df)
    with open(file_path, "wb") as f:
        f.write(data)
    return date_str, file_path


def list_export_dates() -> list[str]:
    ensure_export_root()
    try:
        names = os.listdir(EXPORT_ROOT)
    except Exception:
        return []

    out: list[str] = []
    for name in names:
        p = os.path.join(EXPORT_ROOT, name)
        if os.path.isdir(p) and re.fullmatch(r"\d{4}\.\d{2}\.\d{2}", name):
            out.append(name)

    out.sort(reverse=True)
    return out


def read_export_xlsx_bytes(date_str: str) -> bytes | None:
    p = os.path.join(EXPORT_ROOT, date_str, f"ì¬ê³ í‘œ_{date_str}.xlsx")
    if not os.path.exists(p):
        return None
    try:
        with open(p, "rb") as f:
            return f.read()
    except Exception:
        return None


def delete_export_date(date_str: str) -> bool:
    """exports/YYYY.MM.DD í´ë”(í•´ë‹¹ ë‚ ì§œ ë‚´ë³´ë‚´ê¸°)ë¥¼ í†µì§¸ë¡œ ì‚­ì œ"""
    ensure_export_root()
    if not re.fullmatch(r"\d{4}\.\d{2}\.\d{2}", (date_str or "")):
        return False

    folder = os.path.join(EXPORT_ROOT, date_str)

    # ì•ˆì „ì¥ì¹˜: exports í´ë” ë°–ì„ ì‚­ì œí•˜ì§€ ì•Šë„ë¡ ê²½ë¡œ ê²€ì¦
    root_abs = os.path.abspath(EXPORT_ROOT)
    folder_abs = os.path.abspath(folder)
    if not folder_abs.startswith(root_abs):
        return False

    if os.path.isdir(folder_abs):
        shutil.rmtree(folder_abs)
        return True
    return False


# =====================================================
# âœ… ì œí’ˆë³„ í•©ê³„ ê³ ì • ìˆœì„œ(í‘œì— í•­ìƒ ë¨¼ì €, ìœ„â†’ì•„ë˜ ê¸°ì¤€)
# =====================================================
FIXED_PRODUCT_ORDER = [
    "ê³ ìˆ˜",
    "ê³µì‹¬ì±„",
    "ê·¸ë¦°ë¹ˆ",
    "ë‹¹ê·€ì",
    "ë”œ",
    "ë˜ë””ì‰¬",
    "ë¡œì¦ˆë§ˆë¦¬",
    "ë¡œì¼€íŠ¸",
    "ë°”ì§ˆ",
    "ë¡œì¦ˆì",
    "ë¹„íƒ€ë¯¼",
    "ìŒˆìƒëŸ¬ë¦¬",
    "ìŒˆì¶”",
    "ì• í”Œë¯¼íŠ¸",
    "ì™€ì¼ë“œ",
    "ìë¡œë©”ì¸",
    "ì ê²¨ì",
    "ì ê·¼ëŒ€",
    "ì ì¹˜ì»¤ë¦¬",
    "ì²­ê²½ì±„",
    "ì²­ì¹˜ì»¤ë¦¬",
    "ì¼€ì¼",
    "íƒ€ì„",
    "í†µë¡œë©”ì¸",
    "í–¥ë‚˜ë¬¼",
    "ë‰´ê·¸ë¦°",
    "ì²˜ë¹Œ",
]


# =====================================================
# (A) PACK/BOX/EA ê·œì¹™ (1ë²ˆì½”ë“œ)
# =====================================================
def norm_type(t: str) -> str:
    t = (t or "").strip()
    if t in ["íŒ©", "PACK", "pack", "Pack"]:
        return "PACK"
    if t in ["ë°•ìŠ¤", "BOX", "box", "Box"]:
        return "BOX"
    if t in ["ê°œ", "EA", "ea", "Each", "EACH"]:
        return "EA"
    return t.upper().strip()


def display_type(typ: str) -> str:
    typ = norm_type(typ)
    return {"PACK": "íŒ©", "BOX": "ë°•ìŠ¤", "EA": "ê°œ"}.get(typ, typ)


def parse_pack_size_g(val: str) -> float:
    """(PACK/EA) ê°’: 500 / 500g / 0.5kg í—ˆìš© -> gë¡œ ë°˜í™˜"""
    v = (val or "").strip().lower().replace(" ", "")
    if v.endswith("kg"):
        return float(v[:-2]) * 1000.0
    if v.endswith("g"):
        return float(v[:-1])
    return float(v)


def parse_box_size_kg(val: str) -> float:
    """(BOX) ê°’: 2 / 2kg / 2000g í—ˆìš© -> kgë¡œ ë°˜í™˜"""
    v = (val or "").strip().lower().replace(" ", "")
    if v.endswith("g"):
        return float(v[:-1]) / 1000.0
    if v.endswith("kg"):
        return float(v[:-2])
    return float(v)


def load_rules_text() -> str:
    if os.path.exists(RULES_FILE):
        try:
            with open(RULES_FILE, "r", encoding="utf-8") as f:
                return f.read()
        except Exception:
            pass

    return """# TYPE,ìƒí’ˆëª…,ê°’
# íŒ©(PACK),ìƒí’ˆëª…,íŒ©_ê¸°ì¤€_g(=1íŒ©ì´ ëª‡ gì¸ì§€)  ex) 500 / 500g / 0.5kg
# ë°•ìŠ¤(BOX),ìƒí’ˆëª…,ë°•ìŠ¤_ê¸°ì¤€_kg(=1ë°•ìŠ¤ê°€ ëª‡ kgì¸ì§€) ex) 2 / 2kg / 2000g
# ê°œ(EA),ìƒí’ˆëª…,1ê°œ_ê¸°ì¤€_g(=1ê°œê°€ ëª‡ gì¸ì§€) ex) 1kg / 500g
#
# âœ… ì¶œë ¥ ê·œì¹™
# - í™”ë©´/ê²°ê³¼ëŠ” ëª¨ë‘ ìˆ«ìë§Œ ì¶œë ¥(ë‹¨ìœ„ ê¸€ì ì—†ìŒ)
# - BOX ë“±ë¡ ìƒí’ˆì€ 1 ë¯¸ë§Œì´ì–´ë„ ë‚˜ëˆ ì„œ í‘œì‹œ (ì˜ˆ: 600g / 2000g = 0.3)

# --- íŒ© (ìì£¼ ì“°ëŠ” ê²ƒë“¤) ---
íŒ©,ê±´ëŒ€ì¶”,500
íŒ©,ë°©ìš¸í† ë§ˆí† ,500
íŒ©,ì–‘ì†¡ì´,500
íŒ©,ì™„ìˆ™í† ë§ˆí† ,1kg

# --- ê°œ (ìì£¼ ì“°ëŠ” ê²ƒë“¤) ---
ê°œ,ê¹ë§ˆëŠ˜,1kg
ê°œ,ì²­í”¼ë§,500

# --- ë°•ìŠ¤ (ì´ì¤‘ëŸ‰ Ã· 2kg => ë°•ìŠ¤) ---
ë°•ìŠ¤,ë˜ë””ì‰¬,2
ë°•ìŠ¤,ì ê·¼ëŒ€,2
ë°•ìŠ¤,ë¹„íƒ€ë¯¼,2
ë°•ìŠ¤,ìŒˆìƒëŸ¬ë¦¬,2
ë°•ìŠ¤,ìë¡œë©”ì¸,2
ë°•ìŠ¤,ì ê²¨ì,2
ë°•ìŠ¤,ì ê·¼ëŒ€,2
ë°•ìŠ¤,ì ì¹˜ì»¤ë¦¬,2
ë°•ìŠ¤,ì²­ì¹˜ì»¤ë¦¬,2
ë°•ìŠ¤,ì¼€ì¼,2
ë°•ìŠ¤,í†µë¡œë©”ì¸,2
ë°•ìŠ¤,í–¥ë‚˜ë¬¼,2
ë°•ìŠ¤,ë‰´ê·¸ë¦°,2
ë°•ìŠ¤,ì²­ê²½ì±„,4
"""


def save_rules_text(text: str) -> None:
    _atomic_write_text(RULES_FILE, text or "", encoding="utf-8")


def parse_rules(text: str):
    pack_rules = {}  # {ìƒí’ˆëª…: {"size_g": float}}
    box_rules = {}   # {ìƒí’ˆëª…: {"size_kg": float}}
    ea_rules = {}    # {ìƒí’ˆëª…: {"size_g": float}}

    for raw in (text or "").splitlines():
        line = raw.strip()
        if not line or line.startswith("#"):
            continue

        parts = [p.strip() for p in line.split(",")]
        if len(parts) < 3:
            continue

        typ = norm_type(parts[0])
        name = parts[1].strip()
        val_raw = parts[2].strip()

        try:
            if typ == "PACK":
                size_g = parse_pack_size_g(val_raw)
                if size_g > 0:
                    pack_rules[name] = {"size_g": size_g}

            elif typ == "BOX":
                size_kg = parse_box_size_kg(val_raw)
                if size_kg > 0:
                    box_rules[name] = {"size_kg": size_kg}

            elif typ == "EA":
                size_g = parse_pack_size_g(val_raw)
                if size_g > 0:
                    ea_rules[name] = {"size_g": size_g}
        except Exception:
            continue

    return pack_rules, box_rules, ea_rules


def upsert_rule(text: str, typ: str, name: str, val: str) -> str:
    typ_norm = norm_type(typ)
    typ_disp = display_type(typ_norm)

    name = (name or "").strip()
    val = (val or "").strip()
    if not typ_norm or not name or not val:
        return text

    lines = (text or "").splitlines()
    out = []
    replaced = False

    for ln in lines:
        if ln.strip().startswith("#") or not ln.strip():
            out.append(ln)
            continue

        parts = [p.strip() for p in ln.split(",")]
        if len(parts) >= 2 and norm_type(parts[0]) == typ_norm and parts[1] == name:
            out.append(f"{typ_disp},{name},{val}")
            replaced = True
        else:
            out.append(ln)

    if not replaced:
        if out and out[-1].strip() != "":
            out.append("")
        out.append(f"{typ_disp},{name},{val}")

    return "\n".join(out)


# =====================================================
# (B) 2ë²ˆ ì½”ë“œ: ë§¤ì¹­/í‘œí˜„ ê·œì¹™ + ì—‘ì…€ ì²˜ë¦¬ + PDF/TC ì¶œë ¥
# =====================================================
UNIT_PATTERNS = [
    r"\d+(?:\.\d+)?kg\s*~\s*\d+(?:\.\d+)?kg",
    r"\d+(?:\.\d+)?kg",
    r"(?:ì•½\s*)?\d+(?:\.\d+)?g",
    r"\d+ê°œ",
    r"\d+í†µ",
    r"\d+ë‹¨",
    r"\d+ë´‰",
    r"\d+íŒ©",
]
UNIT_RE = re.compile(r"(" + "|".join(UNIT_PATTERNS) + r")")


def extract_variant(name: str) -> str:
    s = (name or "").strip()
    m = UNIT_RE.search(s)
    if not m:
        return ""
    u = m.group(0)
    u = re.sub(r"\s+", "", u)
    u = u.replace("ì•½", "")
    if "~" in u:
        u = u.split("~", 1)[1]
    return u


def normalize_text(s: str) -> str:
    s = (s or "").strip()
    s = re.sub(r"\s+", " ", s)
    return s


def _safe_int(v) -> Optional[int]:
    if v is None or (isinstance(v, float) and pd.isna(v)):
        return None
    try:
        return int(v)
    except Exception:
        return None


def _xml_escape(s: str) -> str:
    s = str(s or "")
    return s.replace("&", "&amp;").replace("<", "&lt;").replace(">", "&gt;")


def _text_width_pt(text: str, font: str, size: float) -> float:
    try:
        w = pdfmetrics.stringWidth(text, font, size)
        if not w or w <= 0:
            return len(text) * size * 0.55
        return w
    except Exception:
        return len(text) * size * 0.55


def fmt_qty(x):
    try:
        x = float(x)
        return int(x) if x.is_integer() else x
    except Exception:
        return x


def _as_int_qty(v) -> int:
    try:
        f = float(v)
        if abs(f - round(f)) < 1e-9:
            return int(round(f))
        return int(round(f))
    except Exception:
        return 0


# -------------------- TC defaults --------------------
TC_PRODUCT_NAME_FIXED = "ì±„ì†ŒíŒœìƒí’ˆ"
TC_ACCESS_FALLBACK = "ê²½ë¹„ì‹¤ í˜¸ì¶œ"
TC_TYPE_DAWN_DEFAULT = "ìë™"
TC_TYPE_NEXT_DEFAULT = "íƒë°°ëŒ€í–‰"

# ìˆ˜ì·¨ì¸ë³„ PDF ìŠ¤íƒ€ì¼
RECIPIENT_FONT_SIZE = 12
RECIPIENT_LEADING = 15
RECIPIENT_BLOCK_GAP_MM = 4.0
RECIPIENT_LINE_AFTER_MM = 4.0

# ìŠ¤í‹°ì»¤ ìš©ì§€ ì„¤ì • (21Ã—29.5cm / 65ì¹¸ / 3.82Ã—2.11cm)
STICKER_COLS = 5
STICKER_ROWS = 13
STICKER_PER_PAGE = STICKER_COLS * STICKER_ROWS  # 65

# ìš©ì§€ í¬ê¸° (mm)  -> 21Ã—29.5cm
STICKER_PAGE_W_MM = 210.0
STICKER_PAGE_H_MM = 295.0

# ì‚¬ìš©ì ì§€ì • ì—¬ë°± (cm -> mm)
STICKER_MARGIN_LEFT_MM = 4.0
STICKER_MARGIN_RIGHT_MM = 4.0
STICKER_MARGIN_TOP_MM = 11.0
STICKER_MARGIN_BOTTOM_MM = 10.0

# ìŠ¤í‹°ì»¤(ë¼ë²¨) í¬ê¸° (mm) -> 3.82Ã—2.11cm
STICKER_CELL_W_MM = 38.2
STICKER_CELL_H_MM = 21.1

# ìŠ¤í‹°ì»¤ ê°„ê²©: ìƒ/í•˜ 0cm, ì¢Œ/ìš° 0.3cm
# âš ï¸ ë‹¤ë§Œ "ìš©ì§€(21cm) - ì—¬ë°±(0.4cm*2)" í­ ì•ˆì— 5ì¹¸ì„ ë§ì¶”ê¸° ìœ„í•´,
#     ê°€ë¡œ ê°„ê²©ì€ í•„ìš” ì‹œ 0.3cmë³´ë‹¤ ì•„ì£¼ ì¡°ê¸ˆ(â‰ˆ0.025cm) ì¤„ì–´ë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
STICKER_GAP_X_MM = 3.0
STICKER_GAP_Y_MM = 0.0

# ê¸€ì
STICKER_FONT_SIZE = 13
STICKER_LEADING = 16

# í”„ë¦°í„° ì¶œë ¥ ë³´ì •(í•„ìš” ì‹œ ìˆ˜ë™ ì¡°ì •, ê¸°ë³¸ 0)
STICKER_OFFSET_X_MM = 0.0
STICKER_OFFSET_Y_MM = 0.0

# âœ… í–‰ë³„ í…ìŠ¤íŠ¸ ìœ„ì¹˜ ë¯¸ì„¸ë³´ì •
# - 1~5í–‰(ìƒë‹¨ 5ì¤„): ìƒí’ˆëª… ìœ„ì¹˜ë¥¼ "ì¡°ê¸ˆ ë” ìœ„ë¡œ"
# - 12~13í–‰(í•˜ë‹¨ 2ì¤„): ìƒí’ˆëª… ìœ„ì¹˜ë¥¼ "ì¡°ê¸ˆ ë” ì•„ë˜ë¡œ"
# (ë‹¨ìœ„: mm, í•„ìš”í•˜ë©´ ìˆ«ìë§Œ ì¡°ì ˆí•˜ë©´ ë©ë‹ˆë‹¤)
STICKER_TEXT_SHIFT_TOP_ROWS_MM = 3.5
STICKER_TEXT_SHIFT_BOTTOM_ROWS_MM = 3.5


def _clean_access_message(msg: str) -> str:
    s = str(msg or "").strip()
    return s if s else TC_ACCESS_FALLBACK


# -------------------- âœ… TC Settings (persist) --------------------
def load_tc_settings() -> Dict[str, str]:
    default = {"dawn": TC_TYPE_DAWN_DEFAULT, "next": TC_TYPE_NEXT_DEFAULT}
    if not TC_SETTINGS_PATH.exists():
        return default
    try:
        data = json.loads(TC_SETTINGS_PATH.read_text(encoding="utf-8"))
        if not isinstance(data, dict):
            return default
        dawn = normalize_text(data.get("dawn", "")) or TC_TYPE_DAWN_DEFAULT
        nxt = normalize_text(data.get("next", "")) or TC_TYPE_NEXT_DEFAULT
        return {"dawn": dawn, "next": nxt}
    except Exception:
        return default


def save_tc_settings(dawn: str, nxt: str) -> None:
    dawn = normalize_text(dawn) or TC_TYPE_DAWN_DEFAULT
    nxt = normalize_text(nxt) or TC_TYPE_NEXT_DEFAULT
    _atomic_write_text(
        TC_SETTINGS_PATH,
        json.dumps({"dawn": dawn, "next": nxt}, ensure_ascii=False, indent=2),
        encoding="utf-8",
    )


# -------------------- í‘œí˜„ê·œì¹™ (í†µ/ê°œ/íŒ©/ë´‰ ê°™ì€ ë‹¨ìœ„ ê´€ë¦¬) --------------------
def default_expression_rules() -> Dict:
    return {
        "default_unit": "ê°œ",
        "units": [
            {"enabled": True, "unit": "ê°œ"},
            {"enabled": True, "unit": "ë´‰"},
            {"enabled": True, "unit": "í†µ"},
            {"enabled": True, "unit": "íŒ©"},
            {"enabled": True, "unit": "ë‹¨"},
        ],
        "note": "í•©ì‚°ê·œì¹™(N)ì´ ì ìš©ë  ë‹¨ìœ„ë¥¼ ê´€ë¦¬í•©ë‹ˆë‹¤.",
    }


def save_expression_rules(data: Dict) -> None:
    _atomic_write_text(EXPR_RULES_PATH, json.dumps(data, ensure_ascii=False, indent=2), encoding="utf-8")


def load_expression_rules() -> Dict:
    if not EXPR_RULES_PATH.exists():
        data = default_expression_rules()
        save_expression_rules(data)
        return data
    try:
        data = json.loads(EXPR_RULES_PATH.read_text(encoding="utf-8"))
        if not isinstance(data, dict):
            raise ValueError("invalid")

        if "units" not in data or not isinstance(data["units"], list):
            data["units"] = default_expression_rules()["units"]

        if "default_unit" not in data or not isinstance(data["default_unit"], str):
            data["default_unit"] = default_expression_rules()["default_unit"]

        cleaned_units = []
        for r in data["units"]:
            u = normalize_text(r.get("unit", ""))
            if not u:
                continue
            cleaned_units.append({"enabled": bool(r.get("enabled", True)), "unit": u})
        data["units"] = cleaned_units

        # âœ… ê¸°ë³¸ í‘œí˜„ ë‹¨ìœ„ì— "ë‹¨"ì´ ì—†ìœ¼ë©´ ìë™ìœ¼ë¡œ ì¶”ê°€ (ê¸°ì¡´ ì €ì¥ê°’ í˜¸í™˜)
        migrated = False
        try:
            units_list = data.get("units", [])
            unit_names = [normalize_text(r.get("unit", "")) for r in units_list if isinstance(r, dict)]
            if "ë‹¨" not in unit_names:
                # ë˜ë„ë¡ "íŒ©" ë‹¤ìŒì— ë„£ê¸°
                if "íŒ©" in unit_names:
                    pos = unit_names.index("íŒ©") + 1
                    units_list.insert(pos, {"enabled": True, "unit": "ë‹¨"})
                else:
                    units_list.append({"enabled": True, "unit": "ë‹¨"})
                data["units"] = units_list
                migrated = True
        except Exception:
            migrated = False

        # ë§ˆì´ê·¸ë ˆì´ì…˜ì´ ë°œìƒí–ˆìœ¼ë©´ íŒŒì¼ì—ë„ ë°˜ì˜
        if migrated:
            try:
                save_expression_rules(data)
            except Exception:
                pass
        data["default_unit"] = normalize_text(data.get("default_unit", "ê°œ")) or "ê°œ"
        return data
    except Exception:
        data = default_expression_rules()
        save_expression_rules(data)
        return data


def get_bundle_units(expr: Dict) -> List[str]:
    units = []
    for r in expr.get("units", []):
        if r.get("enabled", True):
            u = normalize_text(r.get("unit", ""))
            if u:
                units.append(u)

    seen = set()
    out = []
    for u in units:
        if u not in seen:
            out.append(u)
            seen.add(u)
    return out


def build_bundle_re(bundle_units: List[str]) -> re.Pattern:
    if not bundle_units:
        bundle_units = ["ê°œ"]
    unit_alt = "|".join(map(re.escape, bundle_units))
    return re.compile(rf"^\s*(\d+)\s*({unit_alt})\s*$")


# -------------------- ìƒí’ˆëª… ë§¤ì¹­ ê·œì¹™ (í•©ì‚°ê·œì¹™ N í¬í•¨) --------------------
def default_mapping_rules() -> List[Dict]:
    return [
        {
            "enabled": True,
            "match_type": "contains",
            "pattern": "ì™€ì¼ë“œë£¨ê¼´ë¼",
            "display_name": "ì™€ì¼ë“œ",
            "sum_rule": None,
            "note": 'ì˜ˆ) "ì±„ì†ŒíŒœ ì™€ì¼ë“œë£¨ê¼´ë¼ 1kg ..." -> ì™€ì¼ë“œ',
        },
        {
            "enabled": True,
            "match_type": "contains",
            "pattern": "ë¼ë””ì¹˜ì˜¤",
            "display_name": "ë¼ë””ì¹˜ì˜¤",
            "sum_rule": None,
            "note": 'ì˜ˆ) "ì±„ì†ŒíŒœ ë¼ë””ì¹˜ì˜¤ 1í†µ ..." -> ë¼ë””ì¹˜ì˜¤',
        },
        {
            "enabled": False,
            "match_type": "contains",
            "pattern": "ì˜¤ë Œì§€",
            "display_name": "ì˜¤ë Œì§€",
            "sum_rule": 5,
            "note": "ì˜ˆ) ì˜¤ë Œì§€ í•©ì‚°ê·œì¹™=5",
        },
    ]


def save_mapping_rules(rules: List[Dict]) -> None:
    _atomic_write_text(MAPPING_PATH, json.dumps(rules, ensure_ascii=False, indent=2), encoding="utf-8")


def load_mapping_rules() -> List[Dict]:
    if not MAPPING_PATH.exists():
        rules = default_mapping_rules()
        save_mapping_rules(rules)
        return rules

    try:
        raw = json.loads(MAPPING_PATH.read_text(encoding="utf-8"))
        if isinstance(raw, list):
            cleaned = []
            for r in raw:
                sr = _safe_int(r.get("sum_rule"))
                if sr is not None and sr < 2:
                    sr = None
                cleaned.append(
                    dict(
                        enabled=bool(r.get("enabled", True)),
                        match_type=normalize_text(r.get("match_type", "contains")) or "contains",
                        pattern=normalize_text(r.get("pattern", "")),
                        display_name=normalize_text(r.get("display_name", "")),
                        sum_rule=sr,
                        note=normalize_text(r.get("note", "")),
                    )
                )
            return cleaned
    except Exception:
        pass

    rules = default_mapping_rules()
    save_mapping_rules(rules)
    return rules


def apply_mapping(actual_name: str, rules: List[Dict]) -> Tuple[str, bool, Optional[int]]:
    actual = normalize_text(actual_name)
    if not actual:
        return "", False, None

    for r in rules:
        if not r.get("enabled", True):
            continue

        mt = normalize_text(r.get("match_type", "contains")) or "contains"
        pattern = normalize_text(r.get("pattern", ""))
        display = normalize_text(r.get("display_name", ""))
        sr = _safe_int(r.get("sum_rule"))

        if not pattern or not display:
            continue

        matched = False
        if mt == "exact":
            matched = (actual == pattern)
        elif mt == "contains":
            matched = (pattern in actual)
        elif mt == "regex":
            try:
                matched = bool(re.search(pattern, actual))
            except re.error:
                matched = False

        if matched:
            if sr is not None and sr < 2:
                sr = None
            return display, True, sr

    # fallback
    s = re.sub(r"^\s*ì±„ì†ŒíŒœ\s*", "", actual)
    s = re.sub(r"\([^)]*\)", "", s).strip()
    s = re.sub(r"\s+", " ", s).strip()
    m = UNIT_RE.search(s)
    if m:
        s = s[: m.start()].strip()

    toks = s.split()
    if not toks:
        return actual, False, None

    PREFIX = {"ìƒ", "ìœ ê¸°ë†", "êµ­ì‚°", "ìˆ˜ì…", "ëƒ‰ë™", "ë² ì´ë¹„", "í”„ë¦¬ë¯¸ì—„"}
    if len(toks) >= 2 and toks[0] in PREFIX:
        fallback = toks[0] + toks[1]
    else:
        fallback = toks[0]

    return fallback, False, None


def mapping_df_from_list(rules: List[Dict]) -> pd.DataFrame:
    df = pd.DataFrame(rules)
    keep = ["enabled", "match_type", "pattern", "display_name", "sum_rule", "note"]
    for c in keep:
        if c not in df.columns:
            df[c] = None
    return df[keep]


def mapping_list_from_df(edited: pd.DataFrame) -> List[Dict]:
    cleaned = []
    for _, row in edited.iterrows():
        pattern = normalize_text(row.get("pattern"))
        display = normalize_text(row.get("display_name"))
        if not pattern or not display:
            continue

        mt = normalize_text(row.get("match_type")) or "contains"
        if mt not in {"contains", "exact", "regex"}:
            mt = "contains"

        sr = _safe_int(row.get("sum_rule"))
        if sr is not None and sr < 2:
            sr = None

        cleaned.append(
            dict(
                enabled=bool(row.get("enabled", True)),
                match_type=mt,
                pattern=pattern,
                display_name=display,
                sum_rule=sr,
                note=normalize_text(row.get("note")),
            )
        )
    return cleaned


# -------------------- Backups (Excel) --------------------
def backup_rules_to_excel(mapping_rules: List[Dict], expr_rules: Dict) -> Path:
    out_path = BACKUP_DIR / "ìƒí’ˆë³„ë§¤ì¹­ê·œì¹™_ë°±ì—….xlsx"

    df_map = mapping_df_from_list(mapping_rules).rename(
        columns={
            "enabled": "ì‚¬ìš©",
            "match_type": "ë§¤ì¹­ë°©ì‹",
            "pattern": "ì‹¤ì œìƒí’ˆëª…(íŒ¨í„´)",
            "display_name": "í‘œì‹œë ìƒí’ˆëª…",
            "sum_rule": "í•©ì‚°ê·œì¹™(N)",
            "note": "ë©”ëª¨",
        }
    )

    units = expr_rules.get("units", [])
    df_expr = pd.DataFrame(units)
    if df_expr.empty:
        df_expr = pd.DataFrame([{"enabled": True, "unit": expr_rules.get("default_unit", "ê°œ")}])
    if "enabled" not in df_expr.columns:
        df_expr["enabled"] = True
    if "unit" not in df_expr.columns:
        df_expr["unit"] = ""
    df_expr = df_expr[["enabled", "unit"]].rename(columns={"enabled": "ì‚¬ìš©", "unit": "ë‹¨ìœ„"})

    df_meta = pd.DataFrame(
        [
            {"í‚¤": "default_unit", "ê°’": expr_rules.get("default_unit", "ê°œ")},
            {"í‚¤": "note", "ê°’": expr_rules.get("note", "")},
        ]
    )

    with pd.ExcelWriter(out_path, engine="openpyxl") as writer:
        df_map.to_excel(writer, sheet_name="ìƒí’ˆëª…ë§¤ì¹­", index=False)
        df_expr.to_excel(writer, sheet_name="í‘œí˜„ê·œì¹™_ë‹¨ìœ„", index=False)
        df_meta.to_excel(writer, sheet_name="í‘œí˜„ê·œì¹™_ì„¤ì •", index=False)

    return out_path


# -------------------- Sidebar panels (ë§¤ì¹­ ê·œì¹™ í˜ì´ì§€ì—ì„œë§Œ) --------------------
def sidebar_backup_folder():
    with st.sidebar.expander("ğŸ“ ê·œì¹™ ë°±ì—…í´ë”", expanded=False):
        try:
            backups = sorted(BACKUP_DIR.glob("*.xlsx"), key=lambda p: p.stat().st_mtime, reverse=True)
        except Exception:
            backups = []

        if not backups:
            st.caption("ì•„ì§ ë°±ì—… íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            return

        for i, fp in enumerate(backups[:60]):
            cols = st.columns([6, 2, 2])
            cols[0].write(fp.name)

            try:
                b = fp.read_bytes()
                cols[1].download_button(
                    "ë‹¤ìš´",
                    data=b,
                    file_name=fp.name,
                    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                    key=f"dl_bk_{i}_{fp.name}",
                    use_container_width=True,
                )
            except Exception:
                cols[1].write("")

            if cols[2].button("ì‚­ì œ", key=f"rm_bk_{i}_{fp.name}", use_container_width=True):
                try:
                    fp.unlink()
                    st.success(f"ì‚­ì œ ì™„ë£Œ: {fp.name}")
                    st.rerun()
                except Exception:
                    st.error("ì‚­ì œ ì‹¤íŒ¨")


def sidebar_expression_rules():
    expr = load_expression_rules()
    units = expr.get("units", [])
    default_unit = normalize_text(expr.get("default_unit", "ê°œ")) or "ê°œ"

    with st.sidebar.expander("âš™ï¸ í‘œí˜„ê·œì¹™", expanded=False):
        st.caption("í•©ì‚°ê·œì¹™(N)ì„ ì ìš©í•  ë‹¨ìœ„ë¥¼ ê´€ë¦¬í•©ë‹ˆë‹¤. (í†µ/ê°œ/íŒ©/ë´‰ ë“±)")

        df = pd.DataFrame(units)
        if df.empty:
            df = pd.DataFrame([{"enabled": True, "unit": default_unit}])
        if "enabled" not in df.columns:
            df["enabled"] = True
        if "unit" not in df.columns:
            df["unit"] = ""
        df = df[["enabled", "unit"]]

        edited = st.data_editor(
            df,
            hide_index=True,
            num_rows="dynamic",
            use_container_width=True,
            column_config={
                "enabled": st.column_config.CheckboxColumn("ì‚¬ìš©", default=True),
                "unit": st.column_config.TextColumn("ë‹¨ìœ„"),
            },
            key="expr_units_editor",
        )

        enabled_units = []
        for _, r in edited.iterrows():
            u = normalize_text(r.get("unit", ""))
            if u:
                enabled_units.append((bool(r.get("enabled", True)), u))

        enabled_only = [u for en, u in enabled_units if en]
        if not enabled_only:
            enabled_only = ["ê°œ"]

        if default_unit not in enabled_only:
            default_unit = enabled_only[0]

        new_default = st.selectbox(
            "ê¸°ë³¸ë‹¨ìœ„ (êµ¬ë¶„ì´ ë¹„ì–´ìˆì„ ë•Œ)",
            options=enabled_only,
            index=enabled_only.index(default_unit) if default_unit in enabled_only else 0,
            key="expr_default_unit",
        )

        if st.button("ğŸ’¾ í‘œí˜„ê·œì¹™ ì €ì¥", use_container_width=True, key="save_expr_rules_btn"):
            cleaned_units = []
            seen = set()
            for en, u in enabled_units:
                if u in seen:
                    continue
                cleaned_units.append({"enabled": bool(en), "unit": u})
                seen.add(u)

            data = {
                "default_unit": new_default,
                "units": cleaned_units,
                "note": expr.get("note", ""),
            }
            save_expression_rules(data)
            st.success("í‘œí˜„ê·œì¹™ ì €ì¥ ì™„ë£Œ")
            st.rerun()


# -------------------- Excel decrypt / read --------------------
def decrypt_excel(uploaded_bytes: bytes, password: str = EXCEL_PASSWORD) -> io.BytesIO:
    if msoffcrypto is None:
        raise ModuleNotFoundError("msoffcrypto not installed")
    decrypted = io.BytesIO()
    office = msoffcrypto.OfficeFile(io.BytesIO(uploaded_bytes))
    office.load_key(password=password)
    office.decrypt(decrypted)
    decrypted.seek(0)
    return decrypted


def _norm_col(x) -> str:
    s = str(x if x is not None else "")
    s = s.replace("\xa0", " ").replace("\n", " ").replace("\r", " ")
    return normalize_text(s)


def find_col(df: pd.DataFrame, keywords: List[str]) -> Optional[str]:
    """ì»¬ëŸ¼ëª… íƒìƒ‰: ê³µë°±/ê°œí–‰/NBSP ë“±ì„ ì •ê·œí™”í•´ì„œ ë§¤ì¹­í•©ë‹ˆë‹¤."""
    cols = list(df.columns)
    if not cols:
        return None

    kw_norm = [_norm_col(k) for k in (keywords or []) if _norm_col(k)]
    if not kw_norm:
        return None

    col_norm = [_norm_col(c) for c in cols]

    # 1) ì •ê·œí™” í›„ ì™„ì „ì¼ì¹˜
    for k in kw_norm:
        for c, cn in zip(cols, col_norm):
            if k == cn:
                return c

    # 2) ì •ê·œí™” í›„ ë¶€ë¶„ì¼ì¹˜
    for c, cn in zip(cols, col_norm):
        for k in kw_norm:
            if k in cn:
                return c

    return None


# -------------------- Smart Excel header detection --------------------
REQUIRED_COL_GROUPS = OrderedDict(
    [
        ("ìƒí’ˆëª…", ["ìƒí’ˆëª…", "ìƒí’ˆ", "ì œí’ˆëª…"]),
        ("ìˆ˜ëŸ‰", ["ìˆ˜ëŸ‰", "ì£¼ë¬¸ìˆ˜ëŸ‰", "êµ¬ë§¤ìˆ˜ëŸ‰", "ê°œìˆ˜"]),
        ("êµ¬ë§¤ìëª…", ["êµ¬ë§¤ìëª…", "êµ¬ë§¤ì"]),
        ("ìˆ˜ì·¨ì¸ëª…", ["ìˆ˜ì·¨ì¸ëª…", "ìˆ˜ë ¹ì¸", "ë°›ëŠ”ì‚¬ëŒ"]),
        ("í†µí•©ë°°ì†¡ì§€", ["í†µí•©ë°°ì†¡ì§€", "ë°°ì†¡ì§€", "ì£¼ì†Œ"]),
        ("ì˜µì…˜ì •ë³´", ["ì˜µì…˜ì •ë³´", "ì˜µì…˜", "ì„ íƒì˜µì…˜"]),
        ("ìˆ˜ì·¨ì¸ì—°ë½ì²˜", ["ìˆ˜ì·¨ì¸ì—°ë½ì²˜", "ìˆ˜ë ¹ì¸ì—°ë½ì²˜", "ìˆ˜ì·¨ì¸ ì—°ë½ì²˜", "ìˆ˜ë ¹ì¸ ì—°ë½ì²˜", "ì „í™”ë²ˆí˜¸", "ì—°ë½ì²˜"]),
        ("ë°°ì†¡ë©”ì„¸ì§€", ["ë°°ì†¡ë©”ì„¸ì§€", "ë°°ì†¡ë©”ì‹œì§€", "ë°°ì†¡ ë©”ì‹œì§€", "ë°°ì†¡ ë©”ì„¸ì§€", "ë°°ì†¡ìš”ì²­ì‚¬í•­", "ìš”ì²­ì‚¬í•­"]),
    ]
)


# -------------------- âœ… Sticker Exclude Settings (persist) --------------------
def load_sticker_exclude() -> List[str]:
    """ìŠ¤í‹°ì»¤ìš©ì§€ PDFì—ì„œ ì œì™¸í•  ìƒí’ˆëª… ëª©ë¡ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
    if not STICKER_SETTINGS_PATH.exists():
        return []
    try:
        data = json.loads(STICKER_SETTINGS_PATH.read_text(encoding="utf-8"))
        # allow either list or {"exclude":[...]}
        if isinstance(data, dict):
            data = data.get("exclude", [])
        if not isinstance(data, list):
            return []
        out: List[str] = []
        seen = set()
        for x in data:
            s = normalize_text(x)
            if not s:
                continue
            if s in seen:
                continue
            out.append(s)
            seen.add(s)
        return out
    except Exception:
        return []


def save_sticker_exclude(exclude: List[str]) -> None:
    exclude = exclude or []
    out: List[str] = []
    seen = set()
    for x in exclude:
        s = normalize_text(x)
        if not s:
            continue
        if s in seen:
            continue
        out.append(s)
        seen.add(s)
    _atomic_write_text(
        STICKER_SETTINGS_PATH,
        json.dumps({"exclude": out}, ensure_ascii=False, indent=2),
        encoding="utf-8",
    )



def _missing_required_cols(df: pd.DataFrame) -> List[str]:
    missing = []
    for k, kws in REQUIRED_COL_GROUPS.items():
        if find_col(df, kws) is None:
            missing.append(k)
    return missing


def _guess_header_row(preview: pd.DataFrame, scan_limit: int = 40) -> Tuple[Optional[int], int]:
    """header=None ë¡œ ì½ì€ previewì—ì„œ 'í—¤ë”ë¡œ ë³´ì´ëŠ” í–‰'ì„ ì¶”ì •"""
    if preview is None or preview.empty:
        return None, 0

    best_i = None
    best_score = -1

    n = min(scan_limit, len(preview))
    for i in range(n):
        row = preview.iloc[i].tolist()
        row_strs = []
        for v in row:
            if v is None or (isinstance(v, float) and pd.isna(v)):
                continue
            s = _norm_col(v)
            if s:
                row_strs.append(s)

        if not row_strs:
            continue

        score = 0
        for _, kws in REQUIRED_COL_GROUPS.items():
            hit = False
            for kw in kws:
                kw_n = _norm_col(kw)
                if not kw_n:
                    continue
                if any(kw_n in cell for cell in row_strs):
                    hit = True
                    break
            if hit:
                score += 1

        if score > best_score:
            best_score = score
            best_i = i

    return best_i, max(best_score, 0)


def smart_read_orders_excel(excel_bytes: bytes, min_score: int = 4) -> Tuple[pd.DataFrame, Dict]:
    """
    ìŠ¤ë§ˆíŠ¸ìŠ¤í† ì–´/ì¼ê´„ë°œì†¡ ì—‘ì…€ì²˜ëŸ¼ ìƒë‹¨ì— ì•ˆë‚´ë¬¸ì´ ìˆëŠ” ê²½ìš°,
    'í—¤ë” í–‰'ì„ ìë™ìœ¼ë¡œ ì°¾ì•„ì„œ DataFrameì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    if not excel_bytes:
        raise ValueError("empty excel bytes")

    def _read(sheet, header, nrows=None):
        bio = io.BytesIO(excel_bytes)
        bio.seek(0)
        return pd.read_excel(bio, sheet_name=sheet, header=header, nrows=nrows, engine="openpyxl")

    bio0 = io.BytesIO(excel_bytes)
    bio0.seek(0)
    xls = pd.ExcelFile(bio0, engine="openpyxl")

    best_fallback = None  # (df, meta)

    for sheet in xls.sheet_names:
        # 1) ì¼ë°˜ header=0 ì‹œë„
        try:
            df0 = _read(sheet, header=0)
            df0 = df0.dropna(how="all")
            missing0 = _missing_required_cols(df0)
            if not missing0:
                return df0, {"sheet": sheet, "header_row": 0, "method": "header=0"}
            if best_fallback is None:
                best_fallback = (df0, {"sheet": sheet, "header_row": 0, "method": "header=0", "missing": missing0})
        except Exception:
            pass

        # 2) header í–‰ ì¶”ì •
        try:
            preview = _read(sheet, header=None, nrows=60)
            header_row, score = _guess_header_row(preview, scan_limit=40)

            if header_row is None or score < min_score:
                continue

            df = _read(sheet, header=int(header_row))
            df = df.dropna(how="all")

            # ë¹ˆ ì»¬ëŸ¼ ì œê±°(Unnamed: n)
            try:
                df = df.loc[:, ~df.columns.astype(str).str.match(r"^Unnamed")]
            except Exception:
                pass

            missing = _missing_required_cols(df)
            if not missing:
                return df, {"sheet": sheet, "header_row": int(header_row), "method": f"guessed(score={score})"}

            if best_fallback is None:
                best_fallback = (
                    df,
                    {"sheet": sheet, "header_row": int(header_row), "method": f"guessed(score={score})", "missing": missing},
                )
        except Exception:
            pass

    if best_fallback is not None:
        return best_fallback

    df_last = _read(0, header=0)
    return df_last, {"sheet": 0, "header_row": 0, "method": "fallback"}


# -------------------- í•©ì‚°ê·œì¹™ ì ìš© (í‘œí˜„ê·œì¹™ì—ì„œ ì¼  ë‹¨ìœ„ì—ë§Œ) --------------------
def parse_bundle_variant(variant: str, bundle_re: re.Pattern) -> Tuple[Optional[int], Optional[str]]:
    m = bundle_re.match((variant or "").strip())
    if not m:
        return None, None
    try:
        return int(m.group(1)), m.group(2)
    except Exception:
        return None, None


def explode_sum_rule_rows(
    df_rows: pd.DataFrame,
    bundle_units: List[str],
    default_unit: str,
) -> pd.DataFrame:
    bundle_units = bundle_units or [default_unit or "ê°œ"]
    default_unit = default_unit or "ê°œ"
    bundle_re = build_bundle_re(bundle_units)
    unit_set = set(bundle_units)

    out = []
    for _, r in df_rows.iterrows():
        product = r["ì œí’ˆëª…"]
        variant = (r.get("êµ¬ë¶„", "") or "").strip()
        qty = r.get("ìˆ˜ëŸ‰", None)
        rule_n = _safe_int(r.get("í•©ì‚°ê·œì¹™", None))

        if rule_n is None or rule_n < 2:
            out.append({"ì œí’ˆëª…": product, "êµ¬ë¶„": variant, "ìˆ˜ëŸ‰": qty})
            continue

        # êµ¬ë¶„ì´ ë¹„ì–´ ìˆìœ¼ë©´ ê¸°ë³¸ ë‹¨ìœ„ 1ê°œë¡œ ê°„ì£¼
        if variant == "":
            unit_size, unit_label = 1, default_unit
            is_bundle = unit_label in unit_set
        else:
            unit_size, unit_label = parse_bundle_variant(variant, bundle_re)
            is_bundle = (unit_size is not None and unit_label in unit_set)

        if not is_bundle:
            out.append({"ì œí’ˆëª…": product, "êµ¬ë¶„": variant, "ìˆ˜ëŸ‰": qty})
            continue

        try:
            total_units = int(round(float(qty))) * int(unit_size)
        except Exception:
            out.append({"ì œí’ˆëª…": product, "êµ¬ë¶„": variant, "ìˆ˜ëŸ‰": qty})
            continue

        if total_units <= 0:
            continue

        full = total_units // rule_n
        rem = total_units % rule_n

        if full > 0:
            out.append({"ì œí’ˆëª…": product, "êµ¬ë¶„": f"{rule_n}{unit_label}", "ìˆ˜ëŸ‰": full})
        if rem > 0:
            out.append({"ì œí’ˆëª…": product, "êµ¬ë¶„": f"{rem}{unit_label}", "ìˆ˜ëŸ‰": 1})

    return pd.DataFrame(out)


# -------------------- ë°°ì†¡ ì˜µì…˜ ë¶„ë¥˜ & ê·¸ë£¹ ê·œì¹™ (ìƒˆë²½ ìš°ì„ ) --------------------
def classify_delivery(opt: str) -> str:
    s = str(opt or "")
    if "ìƒˆë²½ë°°ì†¡" in s:
        return "ìƒˆë²½ë°°ì†¡"
    if "ìµì¼ë°°ì†¡" in s:
        return "ìµì¼ë°°ì†¡"
    return "ê¸°íƒ€"


def decide_group_delivery(deliv_set: set) -> str:
    if "ìƒˆë²½ë°°ì†¡" in deliv_set:
        return "ìƒˆë²½ë°°ì†¡"
    if "ìµì¼ë°°ì†¡" in deliv_set:
        return "ìµì¼ë°°ì†¡"
    return "ê¸°íƒ€"


# -------------------- PDF 1) ì œí’ˆë³„ ê°œìˆ˜ --------------------
def build_summary_pdf(summary_df: pd.DataFrame) -> bytes:
    buf = io.BytesIO()

    font_name = "Helvetica"
    try:
        pdfmetrics.registerFont(UnicodeCIDFont("HYGothic-Medium"))
        font_name = "HYGothic-Medium"
    except Exception:
        pass

    doc = SimpleDocTemplate(
        buf,
        pagesize=A4,
        leftMargin=15 * mm,
        rightMargin=15 * mm,
        topMargin=15 * mm,
        bottomMargin=15 * mm,
    )

    styles = getSampleStyleSheet()
    title_style = ParagraphStyle(
        "title",
        parent=styles["Heading2"],
        fontName=font_name,
        fontSize=14,
        leading=18,
        spaceAfter=8,
    )

    elems = []
    elems.append(Paragraph("â–£ ì œí’ˆë³„ ê°œìˆ˜", title_style))
    elems.append(Spacer(1, 4))

    data = [["ì œí’ˆëª…", "êµ¬ë¶„", "ìˆ˜ëŸ‰"]]
    for _, row in summary_df.iterrows():
        data.append([str(row["ì œí’ˆëª…"]), str(row["êµ¬ë¶„"]), str(row["ìˆ˜ëŸ‰"])])

    table = LongTable(
        data,
        colWidths=[75 * mm, 60 * mm, 25 * mm],
        repeatRows=1,
    )
    table.setStyle(
        TableStyle(
            [
                ("FONTNAME", (0, 0), (-1, -1), font_name),
                ("FONTSIZE", (0, 0), (-1, -1), 10),
                ("BACKGROUND", (0, 0), (-1, 0), colors.whitesmoke),
                ("ALIGN", (0, 0), (-1, 0), "CENTER"),
                ("ALIGN", (2, 1), (2, -1), "RIGHT"),
                ("GRID", (0, 0), (-1, -1), 0.25, colors.grey),
                ("TOPPADDING", (0, 0), (-1, 0), 6),
                ("BOTTOMPADDING", (0, 0), (-1, 0), 6),
                ("TOPPADDING", (0, 1), (-1, -1), 4),
                ("BOTTOMPADDING", (0, 1), (-1, -1), 4),
            ]
        )
    )

    elems.append(table)
    doc.build(elems)
    return buf.getvalue()


# -------------------- PDF 2) ìˆ˜ì·¨ì¸ë³„ ì¶œë ¥ --------------------
def build_recipient_pdf(entries: List[Dict[str, str]], footer_prefix: str = "") -> bytes:
    buf = io.BytesIO()

    font_name = "Helvetica"
    try:
        pdfmetrics.registerFont(UnicodeCIDFont("HYGothic-Medium"))
        font_name = "HYGothic-Medium"
    except Exception:
        pass

    footer_prefix = (footer_prefix or "").strip()
    footer_font_size = 11

    left_margin = 12 * mm
    right_margin = 12 * mm
    top_margin = 12 * mm
    bottom_margin = 12 * mm

    doc = SimpleDocTemplate(
        buf,
        pagesize=A4,
        leftMargin=left_margin,
        rightMargin=right_margin,
        topMargin=top_margin,
        bottomMargin=bottom_margin,
    )

    styles = getSampleStyleSheet()
    base_style = ParagraphStyle(
        "base",
        parent=styles["Normal"],
        fontName=font_name,
        fontSize=RECIPIENT_FONT_SIZE,
        leading=RECIPIENT_LEADING,
        spaceAfter=0,
    )

    usable_width = A4[0] - left_margin - right_margin

    def _draw_footer(c: canvas.Canvas, _doc):
        # í•˜ë‹¨ ì¤‘ì•™ í˜ì´ì§€ í‘œê¸°: "ìƒˆë²½ -1-" / "ìµì¼ -2-" ...
        if not footer_prefix:
            return
        try:
            page_no = int(c.getPageNumber())
        except Exception:
            page_no = 1

        txt = f"{footer_prefix} -{page_no}-"
        y = 6 * mm  # í•˜ë‹¨ ì—¬ë°± ì•ˆìª½ì— ê³ ì •

        c.saveState()
        try:
            c.setFont(font_name, footer_font_size)
        except Exception:
            c.setFont("Helvetica", footer_font_size)

        w = _text_width_pt(txt, font_name, footer_font_size)
        x = (A4[0] - w) / 2.0
        c.drawString(x, y, txt)
        c.restoreState()

    elems = []
    for e in entries:
        recv = (e.get("ìˆ˜ì·¨ì¸ëª…") or "").strip() or " "
        items = (e.get("items_line") or "").strip() or " "

        name_token_plain = f"{recv} - "
        indent = _text_width_pt(name_token_plain, font_name, base_style.fontSize)
        indent_cap = usable_width * 0.55
        indent = min(max(indent, 40), indent_cap)

        line_style = ParagraphStyle(
            f"line_{abs(hash(recv)) % 10_000_000}",
            parent=base_style,
            leftIndent=indent,
            firstLineIndent=-indent,
        )

        text = f"<b>{_xml_escape(recv)}</b> - {_xml_escape(items)}"
        p = Paragraph(text, line_style)

        block = KeepTogether(
            [
                p,
                Spacer(1, RECIPIENT_BLOCK_GAP_MM * mm),
                HRFlowable(
                    width="100%",
                    thickness=0.4,
                    color=colors.lightgrey,
                    spaceBefore=0,
                    spaceAfter=RECIPIENT_LINE_AFTER_MM * mm,
                ),
            ]
        )
        elems.append(block)

    doc.build(elems, onFirstPage=_draw_footer, onLaterPages=_draw_footer)
    return buf.getvalue()


# -------------------- PDF 3) ìŠ¤í‹°ì»¤ ìš©ì§€ --------------------
def _wrap_for_cell(txt: str, font_name: str, font_size: int, max_w_pt: float) -> List[str]:
    txt = (txt or "").strip()
    if not txt:
        return [""]

    def w(s: str) -> float:
        return _text_width_pt(s, font_name, font_size)

    if w(txt) <= max_w_pt:
        return [txt]

    if " " in txt:
        parts = txt.split()
        line1 = ""
        consumed = 0
        for p in parts:
            cand = (line1 + " " + p).strip()
            if w(cand) <= max_w_pt:
                line1 = cand
                consumed += 1
            else:
                break
        rest = " ".join(parts[consumed:]).strip()
        if not rest:
            return [line1]
        if w(rest) <= max_w_pt:
            return [line1, rest]
        trimmed = rest
        while trimmed and w(trimmed + "...") > max_w_pt:
            trimmed = trimmed[:-1]
        return [line1, (trimmed + "...") if trimmed else "..."]

    line1 = ""
    for ch in txt:
        if w(line1 + ch) <= max_w_pt:
            line1 += ch
        else:
            break
    rest = txt[len(line1):].strip()
    if not rest:
        return [line1]
    if w(rest) <= max_w_pt:
        return [line1, rest]
    trimmed = rest
    while trimmed and w(trimmed + "...") > max_w_pt:
        trimmed = trimmed[:-1]
    return [line1, (trimmed + "...") if trimmed else "..."]


def _draw_center_text(c: canvas.Canvas, font_name: str, font_size: int, x_center: float, y: float, txt: str):
    txt = (txt or "").strip()
    if not txt:
        return
    w = _text_width_pt(txt, font_name, font_size)
    x_left = x_center - (w / 2.0)
    t = c.beginText()
    t.setTextOrigin(x_left, y)
    t.setFont(font_name, font_size)
    t.textOut(txt)
    c.drawText(t)


def build_sticker_pdf(label_texts: List[str]) -> bytes:
    """
    ìŠ¤í‹°ì»¤(ë¼ë²¨) PDF ì¶œë ¥
    - ìš©ì§€: 21Ã—29.5cm
    - ì—¬ë°±: L/R 0.4cm, T 1.1cm, B 1.0cm
    - ë¼ë²¨: 3.82Ã—2.11cm, 5Ã—13 = 65ì¹¸
    - ê°„ê²©: ì¢Œ/ìš° 0.3cm, ìƒ/í•˜ 0cm (í­/ì—¬ë°±ì„ ë§ì¶”ê¸° ìœ„í•´ ê°€ë¡œ ê°„ê²©ì€ ìë™ ë³´ì •ë  ìˆ˜ ìˆìŒ)
    - ê° ë¼ë²¨ ì¤‘ì•™ì— ìƒí’ˆëª…(í…ìŠ¤íŠ¸) ì¶œë ¥
    """
    buf = io.BytesIO()

    font_name = "Helvetica"
    try:
        pdfmetrics.registerFont(UnicodeCIDFont("HYGothic-Medium"))
        font_name = "HYGothic-Medium"
    except Exception:
        pass

    pagesize = (STICKER_PAGE_W_MM * mm, STICKER_PAGE_H_MM * mm)
    c = canvas.Canvas(buf, pagesize=pagesize)
    page_w_pt, page_h_pt = pagesize

    left_pt = STICKER_MARGIN_LEFT_MM * mm
    right_pt = STICKER_MARGIN_RIGHT_MM * mm
    top_pt = STICKER_MARGIN_TOP_MM * mm
    bottom_pt = STICKER_MARGIN_BOTTOM_MM * mm

    cell_w_pt = STICKER_CELL_W_MM * mm
    cell_h_pt = STICKER_CELL_H_MM * mm

    # gap (ê°€ë¡œëŠ” "0.3cm" ëª©í‘œì´ì§€ë§Œ, ì‹¤ì œ í­/ì—¬ë°±ì— ë§ì¶”ê¸° ìœ„í•´ ìë™ ë³´ì •)
    gap_x_target_pt = STICKER_GAP_X_MM * mm
    gap_y_target_pt = STICKER_GAP_Y_MM * mm

    usable_w = page_w_pt - left_pt - right_pt
    usable_h = page_h_pt - top_pt - bottom_pt

    # ê°€ë¡œ ê°„ê²© ìë™ ë³´ì •(ê·¸ë¦¬ë“œê°€ ì—¬ë°±ì„ ì¹¨ë²”í•˜ë©´ gapì„ ì¤„ì—¬ì„œ ë§ì¶¤)
    if STICKER_COLS > 1:
        grid_w_target = (STICKER_COLS * cell_w_pt) + ((STICKER_COLS - 1) * gap_x_target_pt)
        if grid_w_target > usable_w + (0.1 * mm):
            gap_x_pt = max(0.0, (usable_w - (STICKER_COLS * cell_w_pt)) / (STICKER_COLS - 1))
        else:
            gap_x_pt = gap_x_target_pt
    else:
        gap_x_pt = 0.0

    # ì„¸ë¡œëŠ” ê¸°ë³¸ "0", í˜¹ì‹œë¼ë„ ì˜¤ì°¨ë¡œ ë„˜ì¹˜ë©´ gapì„ ì¤„ì—¬ì„œ(=0 ìœ ì§€) ë§ì¶¤
    if STICKER_ROWS > 1:
        grid_h_target = (STICKER_ROWS * cell_h_pt) + ((STICKER_ROWS - 1) * gap_y_target_pt)
        if grid_h_target > usable_h + (0.1 * mm):
            gap_y_pt = max(0.0, (usable_h - (STICKER_ROWS * cell_h_pt)) / (STICKER_ROWS - 1))
        else:
            gap_y_pt = gap_y_target_pt
    else:
        gap_y_pt = 0.0

    # ì‹œì‘ì : ì¢Œì¸¡ ì—¬ë°± ê¸°ì¤€, ìƒë‹¨ ì—¬ë°± ê¸°ì¤€(ReportLabì€ ì¢Œí•˜ë‹¨ì´ (0,0))
    x0 = left_pt + (STICKER_OFFSET_X_MM * mm)
    y_top = (page_h_pt - top_pt) + (STICKER_OFFSET_Y_MM * mm)  # ì²« ì¤„ ìŠ¤í‹°ì»¤ì˜ ìœ—ë³€

    total = len(label_texts)
    page_count = (total + STICKER_PER_PAGE - 1) // STICKER_PER_PAGE if total else 1

    pad_x = 2.0 * mm
    max_text_w = cell_w_pt - (pad_x * 2)

    for p in range(page_count):
        c.setFillColor(colors.black)
        c.setFont(font_name, STICKER_FONT_SIZE)

        for r in range(STICKER_ROWS):
            for col in range(STICKER_COLS):
                slot = r * STICKER_COLS + col
                global_i = p * STICKER_PER_PAGE + slot
                if global_i >= total:
                    continue

                text = (label_texts[global_i] or "").strip()
                if not text:
                    continue

                x = x0 + col * (cell_w_pt + gap_x_pt)
                y = y_top - ((r + 1) * cell_h_pt) - (r * gap_y_pt)  # ì…€ì˜ í•˜ë‹¨

                # âœ… í–‰ë³„ í…ìŠ¤íŠ¸ ìœ„ì¹˜ ë¯¸ì„¸ë³´ì • (ìš”ì²­: 1~5í–‰ â†‘ / 12~13í–‰ â†“)
                row_shift_pt = 0.0
                if 0 <= r <= 4:
                    row_shift_pt += STICKER_TEXT_SHIFT_TOP_ROWS_MM * mm
                elif r in (11, 12):
                    row_shift_pt -= STICKER_TEXT_SHIFT_BOTTOM_ROWS_MM * mm

                lines = _wrap_for_cell(text, font_name, STICKER_FONT_SIZE, max_text_w)[:2]
                cx = x + cell_w_pt / 2.0

                if len(lines) == 1:
                    cy = y + (cell_h_pt / 2.0) - (STICKER_FONT_SIZE * 0.35) + row_shift_pt
                    _draw_center_text(c, font_name, STICKER_FONT_SIZE, cx, cy, lines[0])
                else:
                    center = y + (cell_h_pt / 2.0) + row_shift_pt
                    upper_y = center + (STICKER_LEADING * 0.25)
                    lower_y = center - (STICKER_LEADING * 0.95)
                    _draw_center_text(c, font_name, STICKER_FONT_SIZE, cx, upper_y, lines[0])
                    _draw_center_text(c, font_name, STICKER_FONT_SIZE, cx, lower_y, lines[1])

        if p < page_count - 1:
            c.showPage()

    c.save()
    return buf.getvalue()



# -------------------- TC ì£¼ë¬¸_ë“±ë¡ì–‘ì‹ ìë™ ì±„ìš°ê¸° --------------------
def _norm_header(s: str) -> str:
    s = str(s or "")
    s = s.replace("*", "")
    s = re.sub(r"\s+", "", s)
    return s.strip().lower()


def build_tc_excel_bytes(template_bytes: bytes, rows: List[Dict[str, str]]) -> bytes:
    wb = openpyxl.load_workbook(io.BytesIO(template_bytes))
    if "ì–‘ì‹" not in wb.sheetnames:
        raise ValueError("TC í…œí”Œë¦¿ì— 'ì–‘ì‹' ì‹œíŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
    ws = wb["ì–‘ì‹"]

    headers = {}
    for col in range(1, ws.max_column + 1):
        v = ws.cell(1, col).value
        if v is None:
            continue
        headers[_norm_header(v)] = col

    def col_of(label_candidates: List[str]) -> int:
        for cand in label_candidates:
            key = _norm_header(cand)
            if key in headers:
                return headers[key]
        raise KeyError(f"í•„ìˆ˜ í—¤ë”ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤: {label_candidates}")

    c_req = col_of(["ë°°ì†¡ìš”ì²­ì¼", "ë°°ì†¡ìš”ì²­ì¼*"])
    c_orderer = col_of(["ì£¼ë¬¸ì", "ì£¼ë¬¸ì*"])
    c_receiver = col_of(["ìˆ˜ë ¹ì", "ìˆ˜ë ¹ì*"])
    c_addr = col_of(["ìˆ˜ë ¹ìë„ë¡œëª…ì£¼ì†Œ", "ìˆ˜ë ¹ì ë„ë¡œëª… ì£¼ì†Œ", "ìˆ˜ë ¹ì ë„ë¡œëª… ì£¼ì†Œ*"])
    c_phone = col_of(["ìˆ˜ë ¹ìì—°ë½ì²˜", "ìˆ˜ë ¹ì ì—°ë½ì²˜", "ìˆ˜ë ¹ì ì—°ë½ì²˜*"])
    c_in = col_of(["ì¶œì…ë°©ë²•", "ì¶œì… ë°©ë²•"])
    c_prod = col_of(["ìƒí’ˆëª…", "ìƒí’ˆëª…*"])
    c_type = col_of(["ë°°ì†¡ìœ í˜•", "ë°°ì†¡ ìœ í˜•", "ë°°ì†¡ ìœ í˜•*"])

    start_row = 2
    for i, r in enumerate(rows):
        rr = start_row + i
        ws.cell(rr, c_req).value = r.get("ë°°ì†¡ìš”ì²­ì¼", "")
        ws.cell(rr, c_orderer).value = r.get("ì£¼ë¬¸ì", "")
        ws.cell(rr, c_receiver).value = r.get("ìˆ˜ë ¹ì", "")
        ws.cell(rr, c_addr).value = r.get("ìˆ˜ë ¹ìë„ë¡œëª…ì£¼ì†Œ", "")
        ws.cell(rr, c_phone).value = r.get("ìˆ˜ë ¹ìì—°ë½ì²˜", "")
        ws.cell(rr, c_in).value = r.get("ì¶œì…ë°©ë²•", "")
        ws.cell(rr, c_prod).value = r.get("ìƒí’ˆëª…", "")
        ws.cell(rr, c_type).value = r.get("ë°°ì†¡ìœ í˜•", "")
        # ë°°ì†¡ë°›ì„ì¥ì†ŒëŠ” ê±´ë“œë¦¬ì§€ ì•ŠìŒ

    out = io.BytesIO()
    wb.save(out)
    return out.getvalue()


# =====================================================
# (C) 1ë²ˆ ì½”ë“œ: PDF(ìŠ¤í¬ë¦°ìƒ·/í•©ê³„í‘œ) + ì¬ê³ ê´€ë¦¬
# =====================================================
def render_pdf_pages_to_images(file_bytes: bytes, zoom: float = 2.0) -> list[bytes]:
    """
    PDF ê° í˜ì´ì§€ë¥¼ PNG ìŠ¤í¬ë¦°ìƒ·ìœ¼ë¡œ ë Œë”ë§í•˜ì—¬ bytes ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
    zoom: 1.0~3.5 (í´ìˆ˜ë¡ ì„ ëª…/ìš©ëŸ‰ ì¦ê°€)
    """
    if fitz is None:
        raise RuntimeError("ìŠ¤í¬ë¦°ìƒ· ì €ì¥ì€ pymupdfê°€ í•„ìš”í•©ë‹ˆë‹¤. (pip install pymupdf)")

    doc = fitz.open(stream=file_bytes, filetype="pdf")
    out: list[bytes] = []
    mat = fitz.Matrix(zoom, zoom)

    for i in range(doc.page_count):
        page = doc.load_page(i)
        pix = page.get_pixmap(matrix=mat, alpha=False)
        out.append(pix.tobytes("png"))

    doc.close()
    return out


def merge_png_pages_to_one(png_bytes_list: list[bytes]) -> bytes:
    """
    ì—¬ëŸ¬ PNG(í˜ì´ì§€)ë¥¼ ì„¸ë¡œë¡œ ì´ì–´ë¶™ì—¬ 1ì¥ PNGë¡œ ë°˜í™˜
    Pillow(PIL) í•„ìš”
    """
    if not png_bytes_list:
        return b""

    if len(png_bytes_list) == 1:
        return png_bytes_list[0]

    if Image is None:
        return png_bytes_list[0]

    imgs = [Image.open(io.BytesIO(b)).convert("RGBA") for b in png_bytes_list]
    max_w = max(im.width for im in imgs)
    total_h = sum(im.height for im in imgs)

    canvas_img = Image.new("RGBA", (max_w, total_h), (255, 255, 255, 0))
    y = 0
    for im in imgs:
        x = (max_w - im.width) // 2
        canvas_img.paste(im, (x, y))
        y += im.height

    out = io.BytesIO()
    canvas_img.save(out, format="PNG", optimize=True)
    return out.getvalue()


def fmt_num(x: float, max_dec=2) -> str:
    s = f"{x:.{max_dec}f}".rstrip("0").rstrip(".")
    return s if s else "0"


def format_weight(grams: float) -> str | None:
    """kg/gë„ ìˆ«ìë§Œ: kg ì†Œìˆ˜ë¡œ í‘œì‹œ (19kg250g -> 19.25)"""
    if grams <= 0:
        return None
    kg = grams / 1000.0
    return fmt_num(kg, 3)


def parse_spec_components(spec: str):
    if not spec:
        return None

    s = spec.replace(",", "").replace(" ", "")
    s = s.replace("ã", "kg").replace("ï¼«ï¼§", "kg").replace("KG", "kg").lower()

    out = {"grams_per_unit": None, "bunch_per_unit": None, "counts_per_unit": {}}

    # âœ… 19kg250g ê°™ì€ ê²°í•© í‘œê¸° ì§€ì›
    m2 = re.search(r"(\d+(?:\.\d+)?)kg(\d+(?:\.\d+)?)g", s)
    if m2:
        kg = float(m2.group(1))
        g = float(m2.group(2))
        out["grams_per_unit"] = kg * 1000.0 + g
    else:
        mw = re.search(r"(\d+(?:\.\d+)?)(kg|g)", s)
        if mw:
            num = float(mw.group(1))
            unit = mw.group(2)
            out["grams_per_unit"] = num * 1000.0 if unit == "kg" else num

    mb = re.search(r"(\d+)ë‹¨", s)
    if mb:
        out["bunch_per_unit"] = int(mb.group(1))

    for u in COUNT_UNITS:
        mu = re.search(r"(\d+)" + re.escape(u), s)
        if mu:
            out["counts_per_unit"][u] = int(mu.group(1))

    if out["grams_per_unit"] is None and out["bunch_per_unit"] is None and not out["counts_per_unit"]:
        return None
    return out


def aggregate(items: list[tuple[str, str, float]]):
    agg = defaultdict(lambda: {"grams": 0.0, "bunch": 0.0, "counts": defaultdict(float), "unknown": defaultdict(float)})

    for product, spec, qty in items:
        try:
            q = float(qty)
        except Exception:
            q = 0.0

        comp = parse_spec_components(spec)
        if comp is None:
            agg[product]["unknown"][spec] += q
            continue

        if comp["grams_per_unit"] is not None:
            agg[product]["grams"] += float(comp["grams_per_unit"]) * q

        if comp["bunch_per_unit"] is not None:
            agg[product]["bunch"] += float(comp["bunch_per_unit"]) * q

        for unit, n in comp["counts_per_unit"].items():
            agg[product]["counts"][unit] += float(n) * q

    return agg


def _append_count_parts(parts: list[str], counts: dict):
    for u in ["ê°œ", "íŒ©", "í†µ", "ë´‰"]:
        v = counts.get(u, 0)
        if v:
            # ì†Œìˆ˜ëŠ” ê±°ì˜ ì—†ê² ì§€ë§Œ í˜¹ì‹œ ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œ
            if abs(v - round(v)) < 1e-9:
                parts.append(f"{int(round(v))}")
            else:
                parts.append(fmt_num(float(v), 2))


def format_total_custom(product: str, rec, pack_rules, box_rules, ea_rules,
                        allow_decimal_pack: bool, allow_decimal_box: bool) -> str:
    parts: list[str] = []

    # ë‹¨ë„ ìˆ«ìë§Œ
    if rec["bunch"]:
        if abs(rec["bunch"] - round(rec["bunch"])) < 1e-9:
            parts.append(f'{int(round(rec["bunch"]))}')
        else:
            parts.append(fmt_num(float(rec["bunch"]), 2))

    grams = float(rec["grams"])
    counts = dict(rec["counts"])

    # BOX ìš°ì„ : ë°•ìŠ¤ ê¸°ì¤€ìœ¼ë¡œ ë‚˜ëˆˆ ê°’(0.3ì²˜ëŸ¼) í‘œì‹œ (1 ë¯¸ë§Œì´ì–´ë„ í•­ìƒ í‘œì‹œ)
    if product in box_rules and grams > 0:
        box_size_kg = float(box_rules[product]["size_kg"])
        denom_g = box_size_kg * 1000.0
        boxes = grams / denom_g

        if allow_decimal_box:
            parts.append(f"{fmt_num(boxes, 2)}")
        else:
            if abs(boxes - round(boxes)) < 1e-9:
                parts.append(f"{int(round(boxes))}")
            else:
                parts.append(f"{fmt_num(boxes, 2)}")

        _append_count_parts(parts, counts)
        return " ".join(parts).strip() if parts else "0"

    # PACK / EA ì²˜ë¦¬
    pack_shown = False
    ea_shown = False

    # spec ìì²´ì— íŒ©ì´ ìˆìœ¼ë©´ ìš°ì„ 
    if counts.get("íŒ©", 0) > 0:
        v = counts.get("íŒ©", 0)
        parts.append(f"{int(round(v))}" if abs(v - round(v)) < 1e-9 else fmt_num(float(v), 2))
        pack_shown = True
        counts.pop("íŒ©", None)

    # rulesë¡œ g -> íŒ© ë³€í™˜
    elif product in pack_rules and grams > 0:
        size_g = float(pack_rules[product]["size_g"])
        packs = grams / size_g
        if allow_decimal_pack:
            parts.append(f"{fmt_num(packs, 2)}")
            pack_shown = True
        else:
            if abs(packs - round(packs)) < 1e-9:
                parts.append(f"{int(round(packs))}")
                pack_shown = True

    # íŒ©ì´ ì•ˆ ì¡í˜”ìœ¼ë©´ "ê°œ" ì²˜ë¦¬
    if not pack_shown:
        if counts.get("ê°œ", 0) > 0:
            v = counts.get("ê°œ", 0)
            parts.append(f"{int(round(v))}" if abs(v - round(v)) < 1e-9 else fmt_num(float(v), 2))
            ea_shown = True
            counts.pop("ê°œ", None)

        elif product in ea_rules and grams > 0:
            size_g = float(ea_rules[product]["size_g"])
            eas = grams / size_g
            # ì •ìˆ˜ë¡œ ë”± ë–¨ì–´ì§ˆ ë•Œë§Œ í‘œì‹œ(ì•„ë‹ˆë©´ ì¤‘ëŸ‰ kg ì†Œìˆ˜ë¡œ)
            if abs(eas - round(eas)) < 1e-9:
                parts.append(f"{int(round(eas))}")
                ea_shown = True

    # íŒ©ë„ ê°œë„ ì•ˆ ì¡íˆë©´ ì¤‘ëŸ‰(kg ì†Œìˆ˜)
    if not pack_shown and not ea_shown:
        w = format_weight(grams)
        if w:
            parts.append(w)

    _append_count_parts(parts, counts)
    return " ".join(parts).strip() if parts else "0"


def to_3_per_row(df: pd.DataFrame, n: int = 3) -> pd.DataFrame:
    """
    âœ… ì„¸ë¡œ ìš°ì„  ë°°ì¹˜(ìœ„â†’ì•„ë˜), ê·¸ ë‹¤ìŒ ì—´ë¡œ ì´ë™
    n=3ì´ë©´ 1ì—´ì„ ìœ„â†’ì•„ë˜ë¡œ ë‹¤ ì±„ìš´ ë’¤ 2ì—´, 3ì—´ ìˆœì„œ
    """
    if df is None or len(df) == 0:
        row = {}
        for c in range(n):
            row[f"ì œí’ˆëª…{c+1}"] = ""
            row[f"í•©ê³„{c+1}"] = ""
        return pd.DataFrame([row])

    total = len(df)
    rows_count = math.ceil(total / n)

    out = []
    for r in range(rows_count):
        row = {}
        for c in range(n):
            idx = c * rows_count + r  # ì„¸ë¡œ ìš°ì„ 
            if idx < total:
                row[f"ì œí’ˆëª…{c+1}"] = df.iloc[idx]["ì œí’ˆëª…"]
                row[f"í•©ê³„{c+1}"] = df.iloc[idx]["í•©ê³„"]
            else:
                row[f"ì œí’ˆëª…{c+1}"] = ""
                row[f"í•©ê³„{c+1}"] = ""
        out.append(row)

    return pd.DataFrame(out)


def make_pdf_bytes(df: pd.DataFrame, title: str) -> bytes:
    """
    1ë²ˆ ì½”ë“œ ìŠ¤íƒ€ì¼(landscape A4 + NanumGothic í°íŠ¸) ìœ ì§€
    """
    font_path = os.path.join("fonts", "NanumGothic.ttf")
    font_name = "NanumGothic"

    if not os.path.exists(font_path):
        raise RuntimeError(f"í°íŠ¸ íŒŒì¼ì„ ëª» ì°¾ìŒ: {font_path} (fonts í´ë”/íŒŒì¼ëª… í™•ì¸)")

    if font_name not in pdfmetrics.getRegisteredFontNames():
        pdfmetrics.registerFont(TTFont(font_name, font_path))
        pdfmetrics.registerFontFamily(
            font_name, normal=font_name, bold=font_name, italic=font_name, boldItalic=font_name
        )

    buf = io.BytesIO()
    doc = SimpleDocTemplate(
        buf, pagesize=landscape(A4),
        leftMargin=18, rightMargin=18, topMargin=18, bottomMargin=18
    )

    styles = getSampleStyleSheet()
    title_style = styles["Title"].clone("KTitle")
    title_style.fontName = font_name

    cell_style = ParagraphStyle(
        "KCell", fontName=font_name, fontSize=10, leading=12,
        alignment=1, wordWrap="CJK"
    )
    header_style = ParagraphStyle(
        "KHeader", fontName=font_name, fontSize=10, leading=12,
        alignment=1, wordWrap="CJK"
    )

    elements = [Paragraph(title, title_style), Spacer(1, 12)]
    safe_df = df.fillna("").astype(str)

    header = [Paragraph(str(c), header_style) for c in safe_df.columns]
    body = [[Paragraph(str(v), cell_style) for v in row] for row in safe_df.values.tolist()]
    data = [header] + body

    page_w, _ = landscape(A4)
    usable_w = page_w - 36
    col_w = usable_w / max(1, len(safe_df.columns))
    col_widths = [col_w] * len(safe_df.columns)

    table = Table(data, repeatRows=1, colWidths=col_widths)
    table.setStyle(TableStyle([
        ("FONTNAME", (0, 0), (-1, -1), font_name),
        ("BACKGROUND", (0, 0), (-1, 0), colors.lightgrey),
        ("GRID", (0, 0), (-1, -1), 0.5, colors.grey),
        ("VALIGN", (0, 0), (-1, -1), "MIDDLE"),
        ("TOPPADDING", (0, 0), (-1, -1), 6),
        ("BOTTOMPADDING", (0, 0), (-1, -1), 6),
    ]))

    elements.append(table)
    doc.build(elements)
    return buf.getvalue()


# =====================================================
# ì¬ê³ ê´€ë¦¬ (1ë²ˆ ì½”ë“œ)
# =====================================================
INVENTORY_COLUMNS = [
    "ìƒí’ˆëª…",
    "ì¬ê³ ",
    "ì…ê³ ",
    "ë³´ìœ ìˆ˜ëŸ‰",
    "1ì°¨",
    "2ì°¨",
    "3ì°¨",
    "ì£¼ë¬¸ìˆ˜ëŸ‰",
    "ë‚¨ì€ìˆ˜ëŸ‰",
]


def _coerce_num_series(s: pd.Series) -> pd.Series:
    """ìˆ«ì/ì†Œìˆ˜ í—ˆìš© (ë¹ˆê°’/ë¬¸ì -> 0)"""
    return pd.to_numeric(s, errors="coerce").fillna(0.0).astype(float)


def compute_inventory_df(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()

    if "ìƒí’ˆëª…" not in df.columns:
        df.insert(0, "ìƒí’ˆëª…", "")

    for col in ["ì¬ê³ ", "ì…ê³ ", "1ì°¨", "2ì°¨", "3ì°¨"]:
        if col not in df.columns:
            df[col] = 0

    for col in ["ì¬ê³ ", "ì…ê³ ", "1ì°¨", "2ì°¨", "3ì°¨"]:
        df[col] = _coerce_num_series(df[col])

    df["ìƒí’ˆëª…"] = df["ìƒí’ˆëª…"].fillna("").astype(str).str.strip()

    def _to_decimal(v):
        if v is None:
            return Decimal("0")
        try:
            if isinstance(v, float) and math.isnan(v):
                return Decimal("0")
            return Decimal(str(v))
        except Exception:
            return Decimal("0")

    stock_dec = [_to_decimal(v) for v in df["ì¬ê³ "].tolist()]
    in_dec = [_to_decimal(v) for v in df["ì…ê³ "].tolist()]
    one_dec = [_to_decimal(v) for v in df["1ì°¨"].tolist()]
    two_dec = [_to_decimal(v) for v in df["2ì°¨"].tolist()]
    three_dec = [_to_decimal(v) for v in df["3ì°¨"].tolist()]

    have_dec = [a + b for a, b in zip(stock_dec, in_dec)]
    order_dec = [a + b + c for a, b, c in zip(one_dec, two_dec, three_dec)]
    remain_dec = [a - b for a, b in zip(have_dec, order_dec)]

    df["ë³´ìœ ìˆ˜ëŸ‰"] = [float(x) for x in have_dec]
    df["ì£¼ë¬¸ìˆ˜ëŸ‰"] = [float(x) for x in order_dec]
    df["ë‚¨ì€ìˆ˜ëŸ‰"] = [float(x) for x in remain_dec]

    for c in ["ë³´ìœ ìˆ˜ëŸ‰", "ì£¼ë¬¸ìˆ˜ëŸ‰", "ë‚¨ì€ìˆ˜ëŸ‰"]:
        df[c] = pd.to_numeric(df[c], errors="coerce").fillna(0.0)
        df[c] = df[c].mask(df[c].abs() < 1e-12, 0.0)

    return df[INVENTORY_COLUMNS]


def sort_inventory_df(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()
    fixed_index = {name: i for i, name in enumerate(FIXED_PRODUCT_ORDER)}

    def _rank(name: str) -> int:
        return fixed_index.get(name, 10_000)

    df["__rank"] = df["ìƒí’ˆëª…"].apply(lambda x: _rank(str(x).strip()))
    df = df.sort_values(by=["__rank", "ìƒí’ˆëª…"], kind="mergesort").drop(columns=["__rank"])
    return df


def load_inventory_df() -> pd.DataFrame:
    if os.path.exists(INVENTORY_FILE):
        try:
            df = pd.read_csv(INVENTORY_FILE, encoding="utf-8-sig")
        except Exception:
            df = pd.read_csv(INVENTORY_FILE, encoding="utf-8", errors="ignore")
    else:
        df = pd.DataFrame({"ìƒí’ˆëª…": FIXED_PRODUCT_ORDER})

    existing = set(df.get("ìƒí’ˆëª…", pd.Series(dtype=str)).fillna("").astype(str).str.strip())
    missing = [p for p in FIXED_PRODUCT_ORDER if p not in existing]
    if missing:
        df = pd.concat([df, pd.DataFrame({"ìƒí’ˆëª…": missing})], ignore_index=True)

    df = compute_inventory_df(df)
    df = sort_inventory_df(df)
    df = df[df["ìƒí’ˆëª…"].astype(str).str.strip() != ""].reset_index(drop=True)
    return df


def save_inventory_df(df: pd.DataFrame) -> None:
    p = Path(INVENTORY_FILE)
    tmp = p.with_suffix(p.suffix + ".tmp")
    df.to_csv(tmp, index=False, encoding="utf-8-sig")
    tmp.replace(p)


def parse_sum_to_number(total_str: str) -> float:
    """ì œí’ˆë³„í•©ê³„ 'í•©ê³„' ë¬¸ìì—´ì—ì„œ ì²« ë²ˆì§¸ ìˆ«ìë§Œ ë½‘ì•„ ë“±ë¡ìš© ìˆ˜ì¹˜ë¡œ ì‚¬ìš©"""
    s = (total_str or "").strip()
    nums = re.findall(r"[-+]?\d*\.?\d+", s)
    if not nums:
        return 0.0
    try:
        return float(nums[0])
    except Exception:
        return 0.0


def register_sum_to_inventory(sum_df_long: pd.DataFrame, target_col: str, add_mode: bool = False):
    """ì œí’ˆë³„í•©ê³„(df_long)ë¥¼ ì¬ê³ ê´€ë¦¬ì˜ 1ì°¨/2ì°¨/3ì°¨ ì¤‘ í•˜ë‚˜ë¡œ ë“±ë¡(ìƒí’ˆëª…ì´ ìˆëŠ” ê²ƒë§Œ)"""
    if sum_df_long is None or len(sum_df_long) == 0:
        return 0, []

    if "inventory_df" in st.session_state:
        inv = st.session_state["inventory_df"].copy()
    else:
        inv = load_inventory_df()

    inv = compute_inventory_df(inv)
    inv_names = inv["ìƒí’ˆëª…"].fillna("").astype(str).str.strip()
    name_to_idx = {n: i for i, n in enumerate(inv_names)}

    skipped = []
    updated = 0

    for _, r in sum_df_long.iterrows():
        name = str(r.get("ì œí’ˆëª…", "")).strip()
        if not name:
            continue
        if name not in name_to_idx:
            skipped.append(name)
            continue

        qty = parse_sum_to_number(str(r.get("í•©ê³„", "0")))
        i = name_to_idx[name]

        if add_mode:
            inv.at[i, target_col] = float(inv.at[i, target_col]) + float(qty)
        else:
            inv.at[i, target_col] = float(qty)

        updated += 1

    inv = compute_inventory_df(inv)
    inv = sort_inventory_df(inv).reset_index(drop=True)

    st.session_state["inventory_df"] = inv
    save_inventory_df(inv)

    return updated, skipped


def inventory_df_to_xlsx_bytes(df: pd.DataFrame) -> bytes:
    buf = io.BytesIO()
    with pd.ExcelWriter(buf, engine="openpyxl") as writer:
        df.to_excel(writer, index=False, sheet_name="ì¬ê³ í‘œ")
        ws = writer.sheets["ì¬ê³ í‘œ"]
        ws.freeze_panes = "B2"
        widths = {"A": 16, "B": 8, "C": 8, "D": 10, "E": 8, "F": 8, "G": 8, "H": 10, "I": 10}
        for col, w in widths.items():
            ws.column_dimensions[col].width = w
    return buf.getvalue()


# =====================================================
# âœ… (í•µì‹¬ ë³€ê²½) ì—‘ì…€ ìš”ì•½(summary_df) â†’ 1ë²ˆ ì œí’ˆë³„í•©ê³„(í•©ê³„) ê³„ì‚°
# =====================================================
def summary_to_items(summary_df: pd.DataFrame, default_unit: str) -> list[tuple[str, str, float]]:
    """
    2ë²ˆ ì½”ë“œ ê²°ê³¼(ì œí’ˆëª…/êµ¬ë¶„/ìˆ˜ëŸ‰)ë¥¼ 1ë²ˆ ì½”ë“œ aggregate() ì…ë ¥ í˜•íƒœë¡œ ë³€í™˜.
    - êµ¬ë¶„ì´ ë¹„ì–´ìˆìœ¼ë©´ default_unit(ê¸°ë³¸ë‹¨ìœ„)ë¡œ 1ê°œ ì²˜ë¦¬: "1ê°œ" ê°™ì€ spec ìƒì„±
    """
    items: list[tuple[str, str, float]] = []
    if summary_df is None or len(summary_df) == 0:
        return items

    default_unit = normalize_text(default_unit) or "ê°œ"

    for _, r in summary_df.iterrows():
        product = str(r.get("ì œí’ˆëª…", "")).strip()
        if not product:
            continue

        spec = str(r.get("êµ¬ë¶„", "") or "").strip()
        if spec.lower() in ("nan", "none"):
            spec = ""
        if spec in ("", "-"):
            spec = f"1{default_unit}"

        try:
            qty = float(r.get("ìˆ˜ëŸ‰", 0) if r.get("ìˆ˜ëŸ‰", 0) is not None else 0)
        except Exception:
            qty = 0.0

        if qty == 0:
            continue

        items.append((product, spec, qty))

    return items


def compute_product_totals_from_summary(
    summary_df: pd.DataFrame,
    pack_rules,
    box_rules,
    ea_rules,
    allow_decimal_pack: bool,
    allow_decimal_box: bool,
    default_unit: str,
) -> tuple[pd.DataFrame, pd.DataFrame]:
    """
    ë°˜í™˜: (df_long[ì œí’ˆëª…, í•©ê³„], df_wide[3ì—´ ë°°ì¹˜])
    """
    items = summary_to_items(summary_df, default_unit=default_unit)
    agg = aggregate(items)

    rows = []
    fixed_set = set(FIXED_PRODUCT_ORDER)

    for product in FIXED_PRODUCT_ORDER:
        if product in agg:
            total_str = format_total_custom(
                product, agg[product],
                pack_rules, box_rules, ea_rules,
                allow_decimal_pack=allow_decimal_pack,
                allow_decimal_box=allow_decimal_box,
            )
        else:
            total_str = "0"
        rows.append({"ì œí’ˆëª…": product, "í•©ê³„": total_str})

    rest = [p for p in agg.keys() if p not in fixed_set]
    for product in sorted(rest):
        rows.append({
            "ì œí’ˆëª…": product,
            "í•©ê³„": format_total_custom(
                product, agg[product],
                pack_rules, box_rules, ea_rules,
                allow_decimal_pack=allow_decimal_pack,
                allow_decimal_box=allow_decimal_box,
            ),
        })

    df_long = pd.DataFrame(rows)
    df_wide = to_3_per_row(df_long, 3)
    return df_long, df_wide


# =====================================================
# Streamlit UI (1ë²ˆ ì½”ë“œ ë ˆì´ì•„ì›ƒ ìœ ì§€)
# =====================================================
st.set_page_config(
    page_title="ì¬ê³ í”„ë¡œê·¸ë¨",
    page_icon="assets/favicon.png",  # âœ… 1ë²ˆ ì½”ë“œ íŒŒë¹„ì½˜/ë””ìì¸ ìœ ì§€
    layout="wide",
)

# ----- Navigation -----
if "page" not in st.session_state:
    # âœ… ìš”ì²­: ì²˜ìŒ ì—´ë©´ "ì—‘ì…€ ì—…ë¡œë“œ"ê°€ ë¨¼ì €
    st.session_state["page"] = "excel_results"

with st.sidebar:
    st.markdown("## ğŸ“Œ ë©”ë‰´")
    if st.button("ğŸ“¥ ì—‘ì…€ ì—…ë¡œë“œ", use_container_width=True):
        st.session_state["page"] = "excel_results"
        st.rerun()
    if st.button("ğŸ§¾ ì œí’ˆë³„ í•©ê³„", use_container_width=True):
        st.session_state["page"] = "product_totals"
        st.rerun()
    if st.button("ğŸ“¦ ì¬ê³ ê´€ë¦¬", use_container_width=True):
        st.session_state["page"] = "inventory"
        st.rerun()
    if st.button("ğŸ§° ì¬ê³ ì¼ê´„ë³€ê²½", use_container_width=True):
        st.session_state["page"] = "bulk_stock"
        st.rerun()
    if st.button("ğŸ’° ë§¤ì¶œê³„ì‚°", use_container_width=True):
        st.session_state["page"] = "sales_calc"
        st.rerun()
    if st.button("ğŸ§© ìƒí’ˆëª… ë§¤ì¹­ ê·œì¹™", use_container_width=True):
        st.session_state["page"] = "mapping_rules"
        st.rerun()
    st.divider()


# =====================================================
# Pages
# =====================================================
def render_mapping_rules_page():
    # ğŸ”’ ë¹„ë°€ë²ˆí˜¸ ë³´í˜¸ (ìƒí’ˆëª… ë§¤ì¹­ ê·œì¹™)
    if "mapping_authed" not in st.session_state:
        st.session_state["mapping_authed"] = False

    if not st.session_state["mapping_authed"]:
        st.title("ğŸ”’ ìƒí’ˆëª… ë§¤ì¹­ ê·œì¹™")
        st.caption("ì´ í˜ì´ì§€ëŠ” ë¹„ë°€ë²ˆí˜¸ê°€ í•„ìš”í•©ë‹ˆë‹¤.")

        pw = None
        ok = False
        left_col, _ = st.columns([2, 8])
        with left_col:
            with st.form("mapping_pw_form", clear_on_submit=False):
                pw = st.text_input(
                    "ë¹„ë°€ë²ˆí˜¸",
                    type="password",
                    label_visibility="collapsed",
                    placeholder="ë¹„ë°€ë²ˆí˜¸",
                )
                ok = st.form_submit_button("ì…ì¥", use_container_width=False)

        if ok:
            if (pw or "").strip() == "1390":
                st.session_state["mapping_authed"] = True
                st.success("ì¸ì¦ ì™„ë£Œ!")
                st.rerun()
            else:
                st.error("ë¹„ë°€ë²ˆí˜¸ê°€ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return

    st.title("ğŸ§© ìƒí’ˆëª… ë§¤ì¹­ ê·œì¹™")
    if st.button("ğŸ”“ ë¡œê·¸ì•„ì›ƒ", use_container_width=False, key="mapping_logout_btn"):
        st.session_state["mapping_authed"] = False
        st.success("ì ê¸ˆ ìƒíƒœë¡œ ì „í™˜ë˜ì—ˆìŠµë‹ˆë‹¤.")
        st.rerun()

    st.caption("ì—‘ì…€ì˜ ì‹¤ì œ ìƒí’ˆëª… â†’ í‘œì‹œë  ìƒí’ˆëª…ìœ¼ë¡œ ë§¤í•‘í•˜ê³ , í•©ì‚°ê·œì¹™(N)ë„ ì„¤ì •í•©ë‹ˆë‹¤.")

    sidebar_backup_folder()
    sidebar_expression_rules()

    mapping_rules = load_mapping_rules()
    expr = load_expression_rules()

    st.markdown(
        """
**ë§¤ì¹­ë°©ì‹ ì„¤ëª…**
- **contains**: `íŒ¨í„´`ì´ `ì—‘ì…€ ìƒí’ˆëª…` ì•ˆì— í¬í•¨ë˜ë©´ ë§¤ì¹­
- **exact**: `íŒ¨í„´`ê³¼ `ì—‘ì…€ ìƒí’ˆëª…`ì´ ì™„ì „íˆ ë™ì¼í•  ë•Œë§Œ ë§¤ì¹­
- **regex**: `íŒ¨í„´`ì„ ì •ê·œì‹ìœ¼ë¡œ í•´ì„í•´ ë§¤ì¹­

**í•©ì‚°ê·œì¹™(N)**  
- N=5, ë‹¨ìœ„ê°€ í‘œí˜„ê·œì¹™ì— í¬í•¨ëœ ê²½ìš°(ê°œ/ë´‰/í†µ/íŒ© ë“±) â†’ 8ê°œ ì£¼ë¬¸ ì‹œ `5ê°œ 1ê°œ` + `3ê°œ 1ê°œ`ë¡œ í‘œí˜„
"""
    )

    df = mapping_df_from_list(mapping_rules)
    edited = st.data_editor(
        df,
        use_container_width=True,
        num_rows="dynamic",
        hide_index=True,
        column_config={
            "enabled": st.column_config.CheckboxColumn("ì‚¬ìš©", default=True),
            "match_type": st.column_config.SelectboxColumn("ë§¤ì¹­ ë°©ì‹", options=["contains", "exact", "regex"]),
            "pattern": st.column_config.TextColumn("ì‹¤ì œ ìƒí’ˆëª…(íŒ¨í„´)", width="large"),
            "display_name": st.column_config.TextColumn("í‘œì‹œë  ìƒí’ˆëª…", width="medium"),
            "sum_rule": st.column_config.NumberColumn("í•©ì‚°ê·œì¹™(N)", min_value=2, step=1),
            "note": st.column_config.TextColumn("ë©”ëª¨", width="large"),
        },
        key="mapping_editor_main",
    )

    c1, c2 = st.columns([1, 1])
    with c1:
        if st.button("ğŸ’¾ ì €ì¥", use_container_width=True):
            cleaned = mapping_list_from_df(edited)
            save_mapping_rules(cleaned)
            st.success(f"ì €ì¥ ì™„ë£Œ! (ê·œì¹™ {len(cleaned)}ê°œ)")
            st.rerun()

    with c2:
        if st.button("ğŸ“— ì—‘ì…€ë¡œ ì €ì¥í•˜ê¸°(ë°±ì—…)", use_container_width=True):
            cleaned_map = mapping_list_from_df(edited)
            outp = backup_rules_to_excel(cleaned_map, expr)
            st.success(f"ë°±ì—… ì €ì¥ ì™„ë£Œ: {outp.name}")
            st.rerun()


def render_excel_results_page():
    st.title("ğŸ“¥ ì—‘ì…€ ì—…ë¡œë“œ")
    st.caption("ì—‘ì…€ ì—…ë¡œë“œ â†’ ì œí’ˆë³„ ì§‘ê³„ + ìˆ˜ì·¨ì¸ë³„ PDF + ìŠ¤í‹°ì»¤ìš©ì§€ PDF + TCì£¼ë¬¸_ë“±ë¡ì–‘ì‹ ìë™ì‘ì„±")
    st.markdown("---")

    if msoffcrypto is None:
        st.error("msoffcryptoê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. requirements.txtì— 'msoffcrypto-tool'ì„ ì¶”ê°€í•˜ê³  ì¬ë°°í¬í•´ ì£¼ì„¸ìš”.")
        st.stop()

    # âœ… ì´ í˜ì´ì§€ì˜ ì‚¬ì´ë“œë°”ì—ì„œë§Œ TC ë°°ì†¡ìœ í˜• ì„¤ì • + ì €ì¥ (ìš”ì²­ì‚¬í•­ ìœ ì§€)
    tc_saved = load_tc_settings()
    if "tc_type_dawn" not in st.session_state:
        st.session_state.tc_type_dawn = tc_saved["dawn"]
    if "tc_type_next" not in st.session_state:
        st.session_state.tc_type_next = tc_saved["next"]

    with st.sidebar.expander("ğŸ”§ ë°°ì†¡ë°©ë²• ì„¤ì •", expanded=False):
        st.caption("ë³€ê²½ í›„ [ì €ì¥]ì„ ëˆ„ë¥´ë©´ ë‹¤ìŒ ì‹¤í–‰ì—ë„ ê·¸ëŒ€ë¡œ ìœ ì§€ë©ë‹ˆë‹¤.")

        dawn_val = st.text_input(
            "ìƒˆë²½ë°°ì†¡ â†’ ë°°ì†¡ìœ í˜•",
            value=st.session_state.tc_type_dawn,
            key="tc_type_dawn_input",
        )

        next_val = st.text_input(
            "ìµì¼ë°°ì†¡ â†’ ë°°ì†¡ìœ í˜•",
            value=st.session_state.tc_type_next,
            key="tc_type_next_input",
        )

        if st.button("ğŸ’¾ TC ì„¤ì • ì €ì¥", use_container_width=True, key="save_tc_settings_btn"):
            dawn_val = (dawn_val or "").strip() or TC_TYPE_DAWN_DEFAULT
            next_val = (next_val or "").strip() or TC_TYPE_NEXT_DEFAULT

            st.session_state.tc_type_dawn = dawn_val
            st.session_state.tc_type_next = next_val

            save_tc_settings(dawn_val, next_val)
            st.success("TC ì„¤ì • ì €ì¥ ì™„ë£Œ")
            st.rerun()

    uploaded = st.file_uploader("ë¹„ë°€ë²ˆí˜¸(0000) ì—‘ì…€ ì—…ë¡œë“œ (.xlsx)", type=["xlsx"], key="orders_excel_uploader")
    if uploaded is None:
        st.info("ì—‘ì…€ì„ ì—…ë¡œë“œí•˜ë©´ ê²°ê³¼ í‘œì™€ ë‹¤ìš´ë¡œë“œê°€ ë‚˜íƒ€ë‚©ë‹ˆë‹¤.")
        st.stop()

    upload_day = datetime.now(KST_TZ).date()
    req_day = upload_day + timedelta(days=1)
    req_day_str = req_day.strftime("%Y-%m-%d")

    try:
        decrypted_io = decrypt_excel(uploaded.getvalue(), password=EXCEL_PASSWORD)
        excel_bytes = decrypted_io.getvalue()
        raw_df, read_meta = smart_read_orders_excel(excel_bytes)
    except Exception as e:
        st.error('ì—‘ì…€ ì½ê¸°/ë³µí˜¸í™” ì‹¤íŒ¨: ë¹„ë°€ë²ˆí˜¸ "0000" ë˜ëŠ” íŒŒì¼ í˜•ì‹ì„ í™•ì¸í•´ ì£¼ì„¸ìš”.')
        st.exception(e)
        st.stop()

    if isinstance(read_meta, dict) and read_meta.get("method") != "header=0":
        st.caption(
            f"ğŸ“Œ í—¤ë” ìë™íƒì§€: sheet={read_meta.get('sheet')} / header_row={read_meta.get('header_row')} / {read_meta.get('method')}"
        )

    col_name = find_col(raw_df, ["ìƒí’ˆëª…", "ìƒí’ˆ", "ì œí’ˆëª…"])
    col_qty = find_col(raw_df, ["ìˆ˜ëŸ‰", "ì£¼ë¬¸ìˆ˜ëŸ‰", "êµ¬ë§¤ìˆ˜ëŸ‰", "ê°œìˆ˜"])
    col_buyer = find_col(raw_df, ["êµ¬ë§¤ìëª…", "êµ¬ë§¤ì"])
    col_recv = find_col(raw_df, ["ìˆ˜ì·¨ì¸ëª…", "ìˆ˜ë ¹ì¸", "ë°›ëŠ”ì‚¬ëŒ"])
    col_addr = find_col(raw_df, ["í†µí•©ë°°ì†¡ì§€", "ë°°ì†¡ì§€", "ì£¼ì†Œ"])
    col_opt = find_col(raw_df, ["ì˜µì…˜ì •ë³´", "ì˜µì…˜", "ì„ íƒì˜µì…˜"])
    col_recv_phone = find_col(raw_df, ["ìˆ˜ì·¨ì¸ì—°ë½ì²˜", "ìˆ˜ë ¹ì¸ì—°ë½ì²˜", "ìˆ˜ì·¨ì¸ ì—°ë½ì²˜", "ìˆ˜ë ¹ì¸ ì—°ë½ì²˜", "ì „í™”ë²ˆí˜¸", "ì—°ë½ì²˜"])
    col_msg = find_col(raw_df, ["ë°°ì†¡ë©”ì„¸ì§€", "ë°°ì†¡ë©”ì‹œì§€", "ë°°ì†¡ ë©”ì‹œì§€", "ë°°ì†¡ ë©”ì„¸ì§€", "ë°°ì†¡ìš”ì²­ì‚¬í•­", "ìš”ì²­ì‚¬í•­"])

    missing = [k for k, v in {
        "ìƒí’ˆëª…": col_name,
        "ìˆ˜ëŸ‰": col_qty,
        "êµ¬ë§¤ìëª…": col_buyer,
        "ìˆ˜ì·¨ì¸ëª…": col_recv,
        "í†µí•©ë°°ì†¡ì§€": col_addr,
        "ì˜µì…˜ì •ë³´": col_opt,
        "ìˆ˜ì·¨ì¸ì—°ë½ì²˜": col_recv_phone,
        "ë°°ì†¡ë©”ì„¸ì§€": col_msg,
    }.items() if v is None]
    if missing:
        st.error(f"í•„ìˆ˜ ì»¬ëŸ¼ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤: {', '.join(missing)}")
        st.write("í˜„ì¬ ì»¬ëŸ¼:", list(raw_df.columns))
        st.stop()

    mapping_rules = load_mapping_rules()
    expr = load_expression_rules()
    bundle_units = get_bundle_units(expr)
    default_unit = normalize_text(expr.get("default_unit", "ê°œ")) or "ê°œ"

    work = raw_df[[col_buyer, col_recv, col_addr, col_recv_phone, col_msg, col_opt, col_name, col_qty]].copy()
    work.columns = ["êµ¬ë§¤ìëª…", "ìˆ˜ì·¨ì¸ëª…", "í†µí•©ë°°ì†¡ì§€", "ìˆ˜ì·¨ì¸ì—°ë½ì²˜", "ë°°ì†¡ë©”ì„¸ì§€", "ì˜µì…˜ì •ë³´", "ìƒí’ˆëª…", "ìˆ˜ëŸ‰"]

    work["ìƒí’ˆëª…"] = work["ìƒí’ˆëª…"].astype(str)
    work["ìˆ˜ëŸ‰"] = pd.to_numeric(work["ìˆ˜ëŸ‰"], errors="coerce")
    work["êµ¬ë¶„"] = work["ìƒí’ˆëª…"].apply(extract_variant)

    mapped = work["ìƒí’ˆëª…"].apply(lambda x: apply_mapping(x, mapping_rules))
    work["ì œí’ˆëª…"] = mapped.apply(lambda t: t[0])
    work["ë§¤ì¹­ì„±ê³µ"] = mapped.apply(lambda t: t[1])
    work["í•©ì‚°ê·œì¹™"] = mapped.apply(lambda t: t[2])

    base = work[(work["ìˆ˜ëŸ‰"].notna()) & (work["ì œí’ˆëª…"] != "")].copy()

    exploded = explode_sum_rule_rows(
        base[["ì œí’ˆëª…", "êµ¬ë¶„", "ìˆ˜ëŸ‰", "í•©ì‚°ê·œì¹™"]],
        bundle_units=bundle_units,
        default_unit=default_unit,
    )

    summary = (
        exploded.groupby(["ì œí’ˆëª…", "êµ¬ë¶„"], as_index=False)["ìˆ˜ëŸ‰"]
        .sum()
        .sort_values(["ì œí’ˆëª…", "êµ¬ë¶„"], kind="mergesort")
        .reset_index(drop=True)
    )
    summary["ìˆ˜ëŸ‰"] = summary["ìˆ˜ëŸ‰"].apply(fmt_qty)

    # âœ… ë‹¤ë¥¸ í˜ì´ì§€(ì œí’ˆë³„ í•©ê³„)ì—ì„œ ë°”ë¡œ ì“°ë„ë¡ ì €ì¥
    st.session_state["excel_summary_df"] = summary.copy()
    st.session_state["excel_default_unit"] = default_unit

    # -------------------- Results --------------------
    with st.expander("âœ… ê²°ê³¼ (ì œí’ˆëª… / êµ¬ë¶„ / ìˆ˜ëŸ‰)", expanded=False):
        st.dataframe(summary, use_container_width=True, height=520)

    with st.expander("âš ï¸ ë¯¸ë§¤ì¹­/ëˆ„ë½ í–‰ (ê·œì¹™ ì¶”ê°€ìš©)", expanded=False):
        bad = work[(work["ë§¤ì¹­ì„±ê³µ"] == False) | (work["ìˆ˜ëŸ‰"].isna())].copy()
        st.dataframe(bad.head(300), use_container_width=True)

    # ì œí’ˆë³„ ê°œìˆ˜ PDF ë‹¤ìš´ë¡œë“œ
    st.download_button(
        "â¬‡ï¸ ì œí’ˆë³„ ê°œìˆ˜ PDF ë‹¤ìš´ë¡œë“œ",
        data=build_summary_pdf(summary),
        file_name="ì œí’ˆë³„ê°œìˆ˜.pdf",
        mime="application/pdf",
        use_container_width=True,
    )

    # ìŠ¤í‹°ì»¤ PDF
    st.markdown("---")
    st.subheader("ğŸ·ï¸ ìŠ¤í‹°ì»¤ìš©ì§€ PDF")

    # âœ… ìŠ¤í‹°ì»¤ë¡œ ì¶œë ¥í•˜ì§€ ì•Šì„ ìƒí’ˆ ì„¤ì • (í¼ì³ë³´ê¸°)
    # - ì €ì¥í•œ ì œì™¸ëª©ë¡ì€ data/sticker_settings.json ì— ë‚¨ì•„ ì´í›„ì—ë„ ìë™ ì ìš©ë©ë‹ˆë‹¤.
    if "sticker_exclude_products" not in st.session_state:
        st.session_state["sticker_exclude_products"] = load_sticker_exclude()

    product_options = sorted(
        [p for p in summary["ì œí’ˆëª…"].dropna().astype(str).str.strip().unique().tolist() if p]
    )

    saved_all = st.session_state.get("sticker_exclude_products", []) or []
    saved_in_options = [p for p in saved_all if p in product_options]
    saved_outside = [p for p in saved_all if p not in product_options]

    # ê¸°ë³¸ì€ "ì €ì¥ëœ ì œì™¸ëª©ë¡"ë§Œ ì´ë²ˆ ìƒì„±ì— ì ìš© (ì €ì¥ ì „ í¸ì§‘ê°’ì€ ì ìš©ë˜ì§€ ì•ŠìŒ)
    desired_editor = [p for p in saved_in_options if p in product_options]

    if "sticker_exclude_products_editor" not in st.session_state:
        st.session_state["sticker_exclude_products_editor"] = desired_editor
    else:
        # ì—…ë¡œë“œ íŒŒì¼ì´ ë°”ë€Œì–´ ì˜µì…˜ ëª©ë¡ì´ ë‹¬ë¼ì ¸ë„ ì˜¤ë¥˜ê°€ ë‚˜ì§€ ì•Šê²Œ, í˜„ì¬ ì˜µì…˜ì— ì—†ëŠ” ê°’ì€ ì œê±°
        st.session_state["sticker_exclude_products_editor"] = [
            p for p in (st.session_state.get("sticker_exclude_products_editor") or [])
            if p in product_options
        ]

    if "sticker_exclude_products_extra" not in st.session_state:
        st.session_state["sticker_exclude_products_extra"] = ",".join(saved_outside)

    with st.expander("ğŸš« ìŠ¤í‹°ì»¤ë¡œ ì¶œë ¥í•˜ì§€ ì•Šì„ ìƒí’ˆ ì„¤ì •", expanded=False):
        st.caption("ì„ íƒí•œ ìƒí’ˆì€ ìŠ¤í‹°ì»¤ìš©ì§€ PDF ìƒì„±ì—ì„œ ì œì™¸ë©ë‹ˆë‹¤. (ì €ì¥í•˜ë©´ ë‹¤ìŒ ì‹¤í–‰/ë‹¤ë¥¸ íŒŒì¼ì—ë„ ë™ì¼ ì ìš©)")

        st.multiselect(
            "ì œì™¸í•  ìƒí’ˆ (í˜„ì¬ ì—…ë¡œë“œí•œ íŒŒì¼ì— ì¡´ì¬í•˜ëŠ” ìƒí’ˆ)",
            options=product_options,
            key="sticker_exclude_products_editor",
        )

        st.text_input(
            "ì¶”ê°€ ì œì™¸ (ì˜µì…˜ì— ì—†ëŠ” ìƒí’ˆ Â· ì‰¼í‘œë¡œ ì—¬ëŸ¬ê°œ ì…ë ¥ Â· ì •í™•íˆ ì¼ì¹˜)",
            key="sticker_exclude_products_extra",
            placeholder="ì˜ˆ: ê³ ìˆ˜,ë”œ",
        )

        if st.button("ğŸ’¾ ì œì™¸ëª©ë¡ ì €ì¥", use_container_width=True):
            _selected = st.session_state.get("sticker_exclude_products_editor", []) or []
            _extra_text = st.session_state.get("sticker_exclude_products_extra", "") or ""
            _extra = [normalize_text(x) for x in _extra_text.split(",") if normalize_text(x)]

            _merged = []
            _seen = set()
            for _p in (_selected + _extra):
                if _p and _p not in _seen:
                    _merged.append(_p)
                    _seen.add(_p)

            save_sticker_exclude(_merged)
            st.session_state["sticker_exclude_products"] = _merged
            st.success("ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤. ë‹¤ìŒ ì‹¤í–‰ì—ë„ ê·¸ëŒ€ë¡œ ì ìš©ë©ë‹ˆë‹¤.")

        st.write(
            "í˜„ì¬ ì €ì¥ëœ ê°’:",
            (", ".join(st.session_state.get("sticker_exclude_products", []) or []) or "ì—†ìŒ"),
        )

    exclude_set = set(st.session_state.get("sticker_exclude_products", []) or [])

    excluded_stickers = 0

    label_rows = []
    for _, r in summary.iterrows():
        name = str(r["ì œí’ˆëª…"]).strip()
        qty = _as_int_qty(r["ìˆ˜ëŸ‰"])

        # ì œì™¸ ìƒí’ˆì€ ìŠ¤í‹°ì»¤ ìƒì„±ì—ì„œ ì œì™¸
        if name in exclude_set:
            if qty > 0:
                excluded_stickers += qty
            continue

        var = str(r["êµ¬ë¶„"]).strip()
        label = name if var in ("", "-", "nan", "None") else f"{name}{var}"
        if qty > 0:
            label_rows.append((label, qty))
    label_rows.sort(key=lambda x: x[0])

    sticker_texts: List[str] = []
    for label, qty in label_rows:
        sticker_texts.extend([label] * qty)
    st.caption(f"ì´ {len(sticker_texts)}ê°œ Â· í˜ì´ì§€ë‹¹ 65ì¹¸ Â· ê¸€ì {STICKER_FONT_SIZE}pt Â· ìš©ì§€ 21Ã—29.5cm Â· ì—¬ë°± L/R0.4 T1.1 B1.0cm Â· ë¼ë²¨ 3.82Ã—2.11cm Â· ê°€ë¡œê°„ê²© 0.3cm(ìë™ë³´ì •) (ì œì™¸ {excluded_stickers}ê°œ)")
    st.download_button(
        "â¬‡ï¸ ìŠ¤í‹°ì»¤ìš©ì§€ PDF ë‹¤ìš´ë¡œë“œ",
        data=build_sticker_pdf(sticker_texts),
        file_name="ìŠ¤í‹°ì»¤ìš©ì§€.pdf",
        mime="application/pdf",
        use_container_width=True,
    )

    # ìˆ˜ì·¨ì¸ë³„ ì¶œë ¥
    st.markdown("---")
    st.subheader("ğŸ“„ ìˆ˜ì·¨ì¸ë³„ ì¶œë ¥ ( ìƒˆë²½ / ìµì¼ )")

    base2 = base.copy()
    base2["ë°°ì†¡êµ¬ë¶„"] = base2["ì˜µì…˜ì •ë³´"].apply(classify_delivery)
    key_cols = ["êµ¬ë§¤ìëª…", "ìˆ˜ì·¨ì¸ëª…", "í†µí•©ë°°ì†¡ì§€"]

    grp_deliv = (
        base2.groupby(key_cols)["ë°°ì†¡êµ¬ë¶„"]
        .agg(lambda x: set(x))
        .apply(decide_group_delivery)
        .reset_index()
        .rename(columns={"ë°°ì†¡êµ¬ë¶„": "ê·¸ë£¹ë°°ì†¡êµ¬ë¶„"})
    )
    base2 = base2.merge(grp_deliv, on=key_cols, how="left")

    def build_items_for_group(g: pd.DataFrame) -> Tuple[str, str]:
        g = g.sort_index()
        od = OrderedDict()
        for _, r in g.iterrows():
            prod = str(r["ì œí’ˆëª…"]).strip()
            var = str(r["êµ¬ë¶„"] or "").strip()
            qty = r["ìˆ˜ëŸ‰"]
            sr = _safe_int(r.get("í•©ì‚°ê·œì¹™", None))
            if not prod:
                continue
            if var == "":
                var = "-"
            key = (prod, var, sr)
            od[key] = od.get(key, 0.0) + float(qty)

        rows = [{"ì œí’ˆëª…": p, "êµ¬ë¶„": v, "ìˆ˜ëŸ‰": q, "í•©ì‚°ê·œì¹™": sr} for (p, v, sr), q in od.items()]
        rows_df = pd.DataFrame(rows) if rows else pd.DataFrame(columns=["ì œí’ˆëª…", "êµ¬ë¶„", "ìˆ˜ëŸ‰", "í•©ì‚°ê·œì¹™"])

        rows_ex = explode_sum_rule_rows(
            rows_df[["ì œí’ˆëª…", "êµ¬ë¶„", "ìˆ˜ëŸ‰", "í•©ì‚°ê·œì¹™"]],
            bundle_units=bundle_units,
            default_unit=default_unit,
        ) if len(rows_df) else rows_df

        od2 = OrderedDict()
        for _, rr in rows_ex.iterrows():
            k2 = (str(rr["ì œí’ˆëª…"]), str(rr["êµ¬ë¶„"]))
            od2[k2] = od2.get(k2, 0.0) + float(rr["ìˆ˜ëŸ‰"])

        parts = [f"{pname}/{v} {fmt_qty(q2)}" for (pname, v), q2 in od2.items()]
        recv_name = str(g["ìˆ˜ì·¨ì¸ëª…"].iloc[0]).strip()
        return recv_name, ", ".join(parts)

    group_entries = []
    for _, g in base2.groupby(key_cols, sort=False):
        recv_name, items_line = build_items_for_group(g)
        group_entries.append(
            {"ê·¸ë£¹ë°°ì†¡êµ¬ë¶„": str(g["ê·¸ë£¹ë°°ì†¡êµ¬ë¶„"].iloc[0]), "ìˆ˜ì·¨ì¸ëª…": recv_name, "items_line": items_line}
        )

    dawn_entries = [e for e in group_entries if e["ê·¸ë£¹ë°°ì†¡êµ¬ë¶„"] == "ìƒˆë²½ë°°ì†¡"]
    next_entries = [e for e in group_entries if e["ê·¸ë£¹ë°°ì†¡êµ¬ë¶„"] == "ìµì¼ë°°ì†¡"]

    c1, c2 = st.columns(2)
    with c1:
        st.write(f"ìƒˆë²½ë°°ì†¡: {len(dawn_entries)}ëª…")
        st.download_button(
            "â¬‡ï¸ ìƒˆë²½ë°°ì†¡ ìˆ˜ì·¨ì¸ë³„ PDF",
            data=build_recipient_pdf(dawn_entries, footer_prefix="ìƒˆë²½"),
            file_name="ìƒˆë²½ë°°ì†¡.pdf",
            mime="application/pdf",
            use_container_width=True,
        )

    with c2:
        st.write(f"ìµì¼ë°°ì†¡: {len(next_entries)}ëª…")
        st.download_button(
            "â¬‡ï¸ ìµì¼ë°°ì†¡ ìˆ˜ì·¨ì¸ë³„ PDF",
            data=build_recipient_pdf(next_entries, footer_prefix="ìµì¼"),
            file_name="ìµì¼ë°°ì†¡.pdf",
            mime="application/pdf",
            use_container_width=True,
        )

    # TC ì£¼ë¬¸ ë“±ë¡
    st.markdown("---")
    st.subheader("ğŸ§¾ TCì£¼ë¬¸_ë“±ë¡ì–‘ì‹ ( ìƒˆë²½ / ìµì¼ )")

    if not TC_TEMPLATE_DEFAULT_PATH.exists():
        st.error("ì•± í´ë”ì— 'TCì£¼ë¬¸_ë“±ë¡ì–‘ì‹.xlsx' íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. GitHubì— app.pyì™€ ê°™ì´ ì˜¬ë ¤ì£¼ì„¸ìš”.")
    else:
        template_bytes = TC_TEMPLATE_DEFAULT_PATH.read_bytes()

        # ìˆ˜ì·¨ì¸ë³„ ì¶œë ¥ ìˆœì„œ(ì›ë³¸ ë“±ì¥ ìˆœì„œ)
        order_keys_df = base2[key_cols].drop_duplicates(keep="first").copy()

        def _first_nonempty(series: pd.Series) -> str:
            for v in series.tolist():
                if v is None:
                    continue
                s = str(v).strip()
                if s and s.lower() != "nan":
                    return s
            return ""

        grp_info_agg = (
            base2.groupby(key_cols, as_index=False)
            .agg(
                ê·¸ë£¹ë°°ì†¡êµ¬ë¶„=("ê·¸ë£¹ë°°ì†¡êµ¬ë¶„", "first"),
                ìˆ˜ì·¨ì¸ì—°ë½ì²˜=("ìˆ˜ì·¨ì¸ì—°ë½ì²˜", _first_nonempty),
                ë°°ì†¡ë©”ì„¸ì§€=("ë°°ì†¡ë©”ì„¸ì§€", _first_nonempty),
                êµ¬ë§¤ìëª…=("êµ¬ë§¤ìëª…", "first"),
                ìˆ˜ì·¨ì¸ëª…=("ìˆ˜ì·¨ì¸ëª…", "first"),
                í†µí•©ë°°ì†¡ì§€=("í†µí•©ë°°ì†¡ì§€", "first"),
            )
        )
        grp_info = order_keys_df.merge(grp_info_agg, on=key_cols, how="left")

        def make_tc_rows(df: pd.DataFrame, ship: str) -> List[Dict[str, str]]:
            out = []
            ship_type = st.session_state.tc_type_dawn if ship == "ìƒˆë²½ë°°ì†¡" else st.session_state.tc_type_next
            ship_type = (ship_type or "").strip() or (TC_TYPE_DAWN_DEFAULT if ship == "ìƒˆë²½ë°°ì†¡" else TC_TYPE_NEXT_DEFAULT)

            for _, r in df.iterrows():
                out.append(
                    {
                        "ë°°ì†¡ìš”ì²­ì¼": req_day_str,
                        "ì£¼ë¬¸ì": str(r["êµ¬ë§¤ìëª…"] or "").strip(),
                        "ìˆ˜ë ¹ì": str(r["ìˆ˜ì·¨ì¸ëª…"] or "").strip(),
                        "ìˆ˜ë ¹ìë„ë¡œëª…ì£¼ì†Œ": str(r["í†µí•©ë°°ì†¡ì§€"] or "").strip(),
                        "ìˆ˜ë ¹ìì—°ë½ì²˜": str(r.get("ìˆ˜ì·¨ì¸ì—°ë½ì²˜", "") or "").strip(),
                        "ì¶œì…ë°©ë²•": _clean_access_message(r.get("ë°°ì†¡ë©”ì„¸ì§€", "")),
                        "ìƒí’ˆëª…": TC_PRODUCT_NAME_FIXED,
                        "ë°°ì†¡ìœ í˜•": ship_type,
                    }
                )
            return out

        dawn_df = grp_info[grp_info["ê·¸ë£¹ë°°ì†¡êµ¬ë¶„"] == "ìƒˆë²½ë°°ì†¡"].copy()
        next_df = grp_info[grp_info["ê·¸ë£¹ë°°ì†¡êµ¬ë¶„"] == "ìµì¼ë°°ì†¡"].copy()

        cols = st.columns(2)
        with cols[0]:
            st.write(f"ìƒˆë²½ë°°ì†¡ í–‰: {len(dawn_df)} (ë°°ì†¡ìœ í˜•: {st.session_state.tc_type_dawn})")
            if len(dawn_df):
                out_bytes = build_tc_excel_bytes(template_bytes, make_tc_rows(dawn_df, "ìƒˆë²½ë°°ì†¡"))
                st.download_button(
                    "â¬‡ï¸ TCì£¼ë¬¸_ë“±ë¡ì–‘ì‹(ìƒˆë²½ë°°ì†¡) ì—‘ì…€ ë‹¤ìš´ë¡œë“œ",
                    data=out_bytes,
                    file_name="ìƒˆë²½ë°°ì†¡_ì†¡ì¥.xlsx",
                    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                    use_container_width=True,
                )

        with cols[1]:
            st.write(f"ìµì¼ë°°ì†¡ í–‰: {len(next_df)} (ë°°ì†¡ìœ í˜•: {st.session_state.tc_type_next})")
            if len(next_df):
                out_bytes = build_tc_excel_bytes(template_bytes, make_tc_rows(next_df, "ìµì¼ë°°ì†¡"))
                st.download_button(
                    "â¬‡ï¸ TCì£¼ë¬¸_ë“±ë¡ì–‘ì‹(ìµì¼ë°°ì†¡) ì—‘ì…€ ë‹¤ìš´ë¡œë“œ",
                    data=out_bytes,
                    file_name="ìµì¼ë°°ì†¡_ì†¡ì¥.xlsx",
                    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                    use_container_width=True,
                )


def render_product_totals_page():
    st.title("ğŸ§¾ ì œí’ˆë³„ í•©ê³„ (PACK/BOX/EA ê·œì¹™ ì ìš©)")
    st.caption("PDF ì—…ë¡œë“œ ì—†ì´, ì—‘ì…€ ê²°ê³¼(ì œí’ˆë³„ ê°œìˆ˜)ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìë™ ê³„ì‚°í•©ë‹ˆë‹¤.")

    summary_df = st.session_state.get("excel_summary_df")
    default_unit = st.session_state.get("excel_default_unit", "ê°œ")

    if summary_df is None or len(summary_df) == 0:
        st.info("ë¨¼ì € [ğŸ“¥ ì—‘ì…€ ì—…ë¡œë“œ] í˜ì´ì§€ì—ì„œ ì—‘ì…€ì„ ì—…ë¡œë“œí•´ ì£¼ì„¸ìš”.")
        if st.button("ğŸ“¥ ì—‘ì…€ ì—…ë¡œë“œë¡œ ì´ë™", use_container_width=True):
            st.session_state["page"] = "excel_results"
            st.rerun()
        return

    # (1) PACK/BOX/EA ê·œì¹™ ì‚¬ì´ë“œë°” (1ë²ˆ ì½”ë“œ ìœ ì§€)
    if "rules_text" not in st.session_state:
        st.session_state["rules_text"] = load_rules_text()

    allow_decimal_pack = False
    allow_decimal_box = True

    with st.sidebar:
        st.subheader("âš™ï¸ ì œí’ˆë³„ í•©ê³„ í‘œí˜„ ê·œì¹™")

        with st.expander("ğŸ§© PACK/BOX/EA ê·œì¹™", expanded=False):
            up = st.file_uploader("rules.txt ì—…ë¡œë“œ(ì„ íƒ)", type=["txt"], key="rules_uploader")
            if up is not None:
                st.session_state["rules_text"] = up.getvalue().decode("utf-8", errors="ignore")

            st.text_area("ê·œì¹™", key="rules_text", height=260)

            colA, colB = st.columns(2)
            allow_decimal_pack = colA.checkbox("íŒ© ì†Œìˆ˜ í—ˆìš©", value=False, key="allow_decimal_pack")
            allow_decimal_box = colB.checkbox("ë°•ìŠ¤ ì†Œìˆ˜ í—ˆìš©", value=True, key="allow_decimal_box")

            with st.form("add_rule_form", clear_on_submit=False):
                st.markdown("**ê·œì¹™ ì¶”ê°€/ì—…ë°ì´íŠ¸**")
                r_type = st.selectbox("TYPE", ["íŒ©", "ê°œ", "ë°•ìŠ¤"])
                r_name = st.text_input("ìƒí’ˆëª…(ì›ë³¸ ì œí’ˆëª…ê³¼ ë™ì¼)", value="")
                r_val = st.text_input("ê°’(PACK=1íŒ© g, BOX=1ë°•ìŠ¤ kg, EA=1ê°œ g)", value="")
                submitted = st.form_submit_button("ì¶”ê°€/ì—…ë°ì´íŠ¸")
                if submitted:
                    st.session_state["rules_text"] = upsert_rule(
                        st.session_state["rules_text"], r_type, r_name, r_val
                    )
                    st.success("ê·œì¹™ ë°˜ì˜ ì™„ë£Œ!")

            col1, col2 = st.columns(2)
            if col1.button("rules.txtë¡œ ì €ì¥(ë¡œì»¬ìš©)", key="save_rules_txt"):
                try:
                    save_rules_text(st.session_state["rules_text"])
                    st.success("rules.txt ì €ì¥ ì™„ë£Œ!")
                except Exception as e:
                    st.error(f"ì €ì¥ ì‹¤íŒ¨: {e}")

            col2.download_button(
                "rules.txt ë‹¤ìš´ë¡œë“œ",
                data=st.session_state["rules_text"].encode("utf-8"),
                file_name="rules.txt",
                mime="text/plain",
            )

    pack_rules, box_rules, ea_rules = parse_rules(st.session_state["rules_text"])

    # (2) ì—‘ì…€ ìš”ì•½ ê¸°ë°˜ìœ¼ë¡œ ì œí’ˆë³„ í•©ê³„ ê³„ì‚°
    df_long, df_wide = compute_product_totals_from_summary(
        summary_df=summary_df,
        pack_rules=pack_rules,
        box_rules=box_rules,
        ea_rules=ea_rules,
        allow_decimal_pack=allow_decimal_pack,
        allow_decimal_box=allow_decimal_box,
        default_unit=default_unit,
    )

    st.session_state["last_sum_df_long"] = df_long.copy()

    st.subheader("ğŸ§¾ ì œí’ˆë³„ í•©ê³„")
    st.dataframe(df_wide, use_container_width=True, hide_index=True)

    # (3) ë‹¤ìš´ë¡œë“œ + ì¬ê³ ë“±ë¡(1ë²ˆ ì½”ë“œ ê·¸ëŒ€ë¡œ)
    try:
        pdf_bytes = make_pdf_bytes(df_wide, "ì œí’ˆë³„ í•©ê³„")

        sum_imgs = render_pdf_pages_to_images(pdf_bytes, zoom=3.0)
        sum_png_one = merge_png_pages_to_one(sum_imgs)

        c1, c2, c3 = st.columns(3)
        with c1:
            st.download_button(
                "ğŸ“„ PDF ë‹¤ìš´ë¡œë“œ(ì œí’ˆë³„í•©ê³„)",
                data=pdf_bytes,
                file_name="ì œí’ˆë³„í•©ê³„.pdf",
                mime="application/pdf",
                use_container_width=True,
            )
        with c2:
            st.download_button(
                "ğŸ–¼ï¸ ìŠ¤í¬ë¦°ìƒ·(PNG) ë‹¤ìš´ë¡œë“œ",
                data=sum_png_one,
                file_name="ì œí’ˆë³„í•©ê³„(ìŠ¤í¬ë¦°ìƒ·).png",
                mime="image/png",
                use_container_width=True,
            )
        with c3:
            if st.button("ğŸ“¦ ì¬ê³ ë“±ë¡", use_container_width=True):
                st.session_state["show_register_panel"] = True

        if st.session_state.get("show_register_panel"):
            st.markdown("#### ğŸ“ ì¬ê³ ë“±ë¡ (1ì°¨/2ì°¨/3ì°¨)")
            target = st.radio("ë“±ë¡í•  ì°¨ìˆ˜", ["1ì°¨", "2ì°¨", "3ì°¨"], horizontal=True, key="register_target")
            add_mode = st.checkbox("ê¸°ì¡´ ê°’ì— ëˆ„ì (ë”í•˜ê¸°)", value=False, key="register_add_mode")

            colR1, colR2 = st.columns([1, 3])
            with colR1:
                do_reg = st.button("âœ… ë“±ë¡", use_container_width=True, key="do_register_btn")
            with colR2:
                st.caption("â€» ì¬ê³ ê´€ë¦¬ í‘œì— **ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ìƒí’ˆëª…ë§Œ** ë“±ë¡ë©ë‹ˆë‹¤. (ì—†ëŠ” ìƒí’ˆì€ ì œì™¸)")

            if do_reg:
                sum_df = st.session_state.get("last_sum_df_long")
                updated, skipped = register_sum_to_inventory(sum_df, target_col=target, add_mode=add_mode)
                st.session_state["show_register_panel"] = False

                if skipped:
                    st.warning("ë“±ë¡ ì œì™¸(ì¬ê³ ê´€ë¦¬ ìƒí’ˆëª… ì—†ìŒ): " + ", ".join(sorted(set(skipped))))
                st.success(f"{target}ì— ë“±ë¡ ì™„ë£Œ! (ë°˜ì˜ í–‰: {updated})")
                st.info("ğŸ“¦ ì‚¬ì´ë“œë°”ì˜ 'ì¬ê³ ê´€ë¦¬'ë¡œ ì´ë™í•˜ë©´ í™•ì¸í•  ìˆ˜ ìˆì–´ìš”.")

        if Image is None and len(sum_imgs) > 1:
            st.warning("âš ï¸ Pillow(PIL)ê°€ ì—†ì–´ ì œí’ˆë³„í•©ê³„ ìŠ¤í¬ë¦°ìƒ·ì€ 1í˜ì´ì§€ë§Œ PNGë¡œ ì €ì¥ë©ë‹ˆë‹¤. ì „ì²´ë¥¼ 1ì¥ìœ¼ë¡œ í•©ì¹˜ë ¤ë©´ Pillow ì„¤ì¹˜ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
    except Exception as e:
        st.error(f"ì œí’ˆë³„ í•©ê³„ PDF/PNG ìƒì„± ì‹¤íŒ¨: {e} (fonts/NanumGothic.ttf ë˜ëŠ” pymupdf í™•ì¸)")


def render_inventory_page():
    st.title("ì¬ê³ ê´€ë¦¬")

    # ---- ğŸ“ ë‚´ë³´ë‚´ê¸° í´ë”(ì¬ê³ ê´€ë¦¬ì—ì„œë§Œ í‘œì‹œ) ----
    with st.sidebar:
        with st.expander("ğŸ“ ë‚´ë³´ë‚´ê¸° í´ë”", expanded=False):
            dates = list_export_dates()
            if not dates:
                st.caption("ë‚´ë³´ë‚´ê¸° ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤.")
            else:
                last = st.session_state.get("last_export_date")
                if last:
                    st.caption(f"ë§ˆì§€ë§‰ ë‚´ë³´ë‚´ê¸°: {last}")

                st.caption("â€» ì‚­ì œí•˜ë©´ ë³µêµ¬í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                for d in dates:
                    data = read_export_xlsx_bytes(d)
                    row1, row2 = st.columns([3, 1])

                    with row1:
                        if data is None:
                            st.caption(f"ğŸ“ {d} (íŒŒì¼ ì—†ìŒ)")
                        else:
                            st.download_button(
                                label=f"â¬‡ï¸ {d} ì¬ê³ í‘œ(.xlsx)",
                                data=data,
                                file_name=f"ì¬ê³ í‘œ_{d}.xlsx",
                                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                                use_container_width=True,
                                key=f"export_dl_{d}",
                            )

                    with row2:
                        if st.button("ğŸ—‘ï¸", use_container_width=True, key=f"export_del_{d}"):
                            ok = False
                            try:
                                ok = delete_export_date(d)
                            except Exception:
                                ok = False

                            if ok:
                                if st.session_state.get("last_export_date") == d:
                                    st.session_state["last_export_date"] = None
                                st.session_state["inventory_toast"] = f"{d} ë‚´ë³´ë‚´ê¸° ì‚­ì œ ì™„ë£Œ!"
                                st.rerun()
                            else:
                                st.error("ì‚­ì œ ì‹¤íŒ¨: í´ë”/íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")

    msg = st.session_state.pop("inventory_toast", None)
    if msg:
        st.success(msg)

    if "inventory_df" not in st.session_state:
        st.session_state["inventory_df"] = load_inventory_df()
    if "inventory_editor_version" not in st.session_state:
        st.session_state["inventory_editor_version"] = 0

    df_view = compute_inventory_df(st.session_state["inventory_df"])
    df_view = sort_inventory_df(df_view).reset_index(drop=True)
    df_view = df_view[df_view["ìƒí’ˆëª…"].astype(str).str.strip() != ""].reset_index(drop=True)

    # -------------------- ìŠ¤íƒ€ì¼ (1ë²ˆ ì½”ë“œ ìœ ì§€) --------------------
    def _remain_bg(v):
        try:
            x = float(v)
        except Exception:
            return ""
        if x < 0:
            return "background-color: #ffb3b3;"  # ì—°í•œ ë¹¨ê°•
        if 0 <= x <= 10:
            return "background-color: #ffd6e7;"  # ì—°ë¶„í™
        if x >= 30:
            return "background-color: #d6ecff;"  # ì—°íŒŒë‘
        return ""

    st.markdown(
        """
        <style>
        /* í—¤ë” í…ìŠ¤íŠ¸ Bold */
        div[data-testid="stDataEditor"] .ag-header-cell[col-id="ìƒí’ˆëª…"] .ag-header-cell-text,
        div[data-testid="stDataEditor"] .ag-header-cell[col-id="ë³´ìœ ìˆ˜ëŸ‰"] .ag-header-cell-text,
        div[data-testid="stDataEditor"] .ag-header-cell[col-id="ë‚¨ì€ìˆ˜ëŸ‰"] .ag-header-cell-text,
        div[data-testid="stDataFrame"]  .ag-header-cell[col-id="ìƒí’ˆëª…"] .ag-header-cell-text,
        div[data-testid="stDataFrame"]  .ag-header-cell[col-id="ë³´ìœ ìˆ˜ëŸ‰"] .ag-header-cell-text,
        div[data-testid="stDataFrame"]  .ag-header-cell[col-id="ë‚¨ì€ìˆ˜ëŸ‰"] .ag-header-cell-text {
            font-weight: 800 !important;
        }

        /* ì…€ ê°’ Bold(í´ë°±) */
        div[data-testid="stDataEditor"] .ag-cell[col-id="ìƒí’ˆëª…"],
        div[data-testid="stDataEditor"] .ag-cell[col-id="ë³´ìœ ìˆ˜ëŸ‰"],
        div[data-testid="stDataEditor"] .ag-cell[col-id="ë‚¨ì€ìˆ˜ëŸ‰"],
        div[data-testid="stDataEditor"] .ag-cell[col-id="ìƒí’ˆëª…"] .ag-cell-value,
        div[data-testid="stDataEditor"] .ag-cell[col-id="ë³´ìœ ìˆ˜ëŸ‰"] .ag-cell-value,
        div[data-testid="stDataEditor"] .ag-cell[col-id="ë‚¨ì€ìˆ˜ëŸ‰"] .ag-cell-value,
        div[data-testid="stDataFrame"]  .ag-cell[col-id="ìƒí’ˆëª…"],
        div[data-testid="stDataFrame"]  .ag-cell[col-id="ë³´ìœ ìˆ˜ëŸ‰"],
        div[data-testid="stDataFrame"]  .ag-cell[col-id="ë‚¨ì€ìˆ˜ëŸ‰"],
        div[data-testid="stDataFrame"]  .ag-cell[col-id="ìƒí’ˆëª…"] .ag-cell-value,
        div[data-testid="stDataFrame"]  .ag-cell[col-id="ë³´ìœ ìˆ˜ëŸ‰"] .ag-cell-value,
        div[data-testid="stDataFrame"]  .ag-cell[col-id="ë‚¨ì€ìˆ˜ëŸ‰"] .ag-cell-value {
            font-weight: 800 !important;
        }

        /* âœ… ì¬ê³ í‘œ ë°ì´í„°(ì…€) ì „ì²´ ì™¼ìª½ ì •ë ¬ (ìˆ«ì í¬í•¨) */
        div[data-testid="stDataEditor"] .ag-center-cols-container .ag-cell[col-id],
        div[data-testid="stDataFrame"]  .ag-center-cols-container .ag-cell[col-id] {
            text-align: left !important;
            justify-content: flex-start !important;
        }
        div[data-testid="stDataEditor"] .ag-center-cols-container .ag-cell[col-id] .ag-cell-wrapper,
        div[data-testid="stDataFrame"]  .ag-center-cols-container .ag-cell[col-id] .ag-cell-wrapper {
            justify-content: flex-start !important;
            width: 100% !important;
        }
        div[data-testid="stDataEditor"] .ag-center-cols-container .ag-cell[col-id] .ag-cell-value,
        div[data-testid="stDataFrame"]  .ag-center-cols-container .ag-cell[col-id] .ag-cell-value {
            text-align: left !important;
            width: 100% !important;
        }
        div[data-testid="stDataEditor"] .ag-center-cols-container .ag-cell[col-id] input,
        div[data-testid="stDataFrame"]  .ag-center-cols-container .ag-cell[col-id] input {
            text-align: left !important;
        }

        /* ìˆ«ì ê¸°ë³¸ ì˜¤ë¥¸ìª½ ì •ë ¬ í´ë˜ìŠ¤ ê°•ì œ override */
        div[data-testid="stDataEditor"] .ag-cell.ag-right-aligned,
        div[data-testid="stDataFrame"]  .ag-cell.ag-right-aligned,
        div[data-testid="stDataEditor"] .ag-cell.ag-number-cell,
        div[data-testid="stDataFrame"]  .ag-cell.ag-number-cell {
            text-align: left !important;
        }
        div[data-testid="stDataEditor"] .ag-cell.ag-right-aligned .ag-cell-wrapper,
        div[data-testid="stDataFrame"]  .ag-cell.ag-right-aligned .ag-cell-wrapper,
        div[data-testid="stDataEditor"] .ag-cell.ag-number-cell .ag-cell-wrapper,
        div[data-testid="stDataFrame"]  .ag-cell.ag-number-cell .ag-cell-wrapper {
            justify-content: flex-start !important;
            width: 100% !important;
        }

        /* (ì„ íƒ) í—¤ë”ë„ ì™¼ìª½ ì •ë ¬ */
        div[data-testid="stDataEditor"] .ag-header-cell .ag-header-cell-label,
        div[data-testid="stDataFrame"]  .ag-header-cell .ag-header-cell-label {
            justify-content: flex-start !important;
        }
        div[data-testid="stDataEditor"] .ag-header-cell-text,
        div[data-testid="stDataFrame"]  .ag-header-cell-text {
            text-align: left !important;
        }
        </style>
        """,
        unsafe_allow_html=True,
    )

    df_display = df_view.copy()

    def _fmt_num(v):
        if v is None or (isinstance(v, float) and math.isnan(v)):
            return "0"
        try:
            x = float(v)
            if abs(x) < 1e-12:
                x = 0.0
            if float(x).is_integer():
                return str(int(round(x)))
            return format(x, "g")
        except Exception:
            s = str(v).strip()
            return s if s else "0"

    for c in ["ì¬ê³ ", "ì…ê³ ", "ë³´ìœ ìˆ˜ëŸ‰", "1ì°¨", "2ì°¨", "3ì°¨", "ì£¼ë¬¸ìˆ˜ëŸ‰", "ë‚¨ì€ìˆ˜ëŸ‰"]:
        if c in df_display.columns:
            df_display[c] = df_display[c].map(_fmt_num)

    def _remain_bg_any(v):
        try:
            x = float(str(v).replace(",", "").strip())
        except Exception:
            return ""
        return _remain_bg(x)

    df_styler = (
        df_display.style
        .applymap(_remain_bg_any, subset=["ë‚¨ì€ìˆ˜ëŸ‰"])
        .set_properties(subset=["ìƒí’ˆëª…", "ë³´ìœ ìˆ˜ëŸ‰", "ë‚¨ì€ìˆ˜ëŸ‰"], **{"font-weight": "800"})
    )

    st.markdown("### ì¬ê³ í‘œ (ìˆ˜ì •/ì¶”ê°€/ì‚­ì œ ê°€ëŠ¥)")

    ver = int(st.session_state.get("inventory_editor_version", 0))
    editor_key = f"inventory_editor_{ver}"

    edited_raw = st.data_editor(
        df_styler,
        num_rows="dynamic",
        use_container_width=True,
        hide_index=True,
        disabled=["ë³´ìœ ìˆ˜ëŸ‰", "ì£¼ë¬¸ìˆ˜ëŸ‰", "ë‚¨ì€ìˆ˜ëŸ‰"],
        column_config={
            "ìƒí’ˆëª…": st.column_config.TextColumn("ìƒí’ˆëª…", required=True),
            "ì¬ê³ ": st.column_config.TextColumn("ì¬ê³ "),
            "ì…ê³ ": st.column_config.TextColumn("ì…ê³ "),
            "ë³´ìœ ìˆ˜ëŸ‰": st.column_config.TextColumn("ë³´ìœ ìˆ˜ëŸ‰"),
            "1ì°¨": st.column_config.TextColumn("1ì°¨"),
            "2ì°¨": st.column_config.TextColumn("2ì°¨"),
            "3ì°¨": st.column_config.TextColumn("3ì°¨"),
            "ì£¼ë¬¸ìˆ˜ëŸ‰": st.column_config.TextColumn("ì£¼ë¬¸ìˆ˜ëŸ‰"),
            "ë‚¨ì€ìˆ˜ëŸ‰": st.column_config.TextColumn("ë‚¨ì€ìˆ˜ëŸ‰"),
        },
        key=editor_key,
    )
    edited_raw = edited_raw.copy() if isinstance(edited_raw, pd.DataFrame) else pd.DataFrame(edited_raw)

    def _base_view(df: pd.DataFrame) -> pd.DataFrame:
        base_cols = ["ìƒí’ˆëª…", "ì¬ê³ ", "ì…ê³ ", "1ì°¨", "2ì°¨", "3ì°¨"]
        dd = df.copy()
        for c in base_cols:
            if c not in dd.columns:
                dd[c] = "" if c == "ìƒí’ˆëª…" else 0
        dd["ìƒí’ˆëª…"] = dd["ìƒí’ˆëª…"].fillna("").astype(str).str.strip()
        for c in ["ì¬ê³ ", "ì…ê³ ", "1ì°¨", "2ì°¨", "3ì°¨"]:
            dd[c] = pd.to_numeric(dd[c], errors="coerce").fillna(0.0)
        return dd[base_cols].reset_index(drop=True)

    df_base_new = _base_view(edited_raw)
    df_base_new = df_base_new[df_base_new["ìƒí’ˆëª…"].astype(str).str.strip() != ""].reset_index(drop=True)

    dup = df_base_new["ìƒí’ˆëª…"][df_base_new["ìƒí’ˆëª…"].duplicated(keep=False)]
    if len(dup) > 0:
        st.warning(f"âš ï¸ ìƒí’ˆëª…ì´ ì¤‘ë³µëœ í–‰ì´ ìˆìŠµë‹ˆë‹¤: {', '.join(sorted(set(dup.astype(str))))}")

    colA, colB, colC = st.columns([1, 1, 1])

    if colA.button("ğŸ’¾ ì €ì¥", use_container_width=True):
        df_save = compute_inventory_df(df_base_new)
        df_save = sort_inventory_df(df_save).reset_index(drop=True)
        df_save = df_save[df_save["ìƒí’ˆëª…"].astype(str).str.strip() != ""].reset_index(drop=True)

        st.session_state["inventory_df"] = df_save
        save_inventory_df(df_save)

        st.session_state["inventory_editor_version"] = ver + 1
        st.session_state["inventory_toast"] = "ì €ì¥ ì™„ë£Œ!"
        st.rerun()

    if colB.button("â†» ì´ˆê¸°í™”(0ìœ¼ë¡œ)", use_container_width=True):
        base = pd.DataFrame({"ìƒí’ˆëª…": FIXED_PRODUCT_ORDER})
        base = compute_inventory_df(base)
        base = sort_inventory_df(base).reset_index(drop=True)

        st.session_state["inventory_df"] = base
        save_inventory_df(base)

        st.session_state["inventory_editor_version"] = ver + 1
        st.session_state["inventory_toast"] = "ì´ˆê¸°í™” ì™„ë£Œ!"
        st.rerun()

    if colC.button("ğŸ“¤ ë‚´ë³´ë‚´ê¸°", use_container_width=True):
        df_export = compute_inventory_df(df_base_new)
        df_export = sort_inventory_df(df_export).reset_index(drop=True)
        df_export = df_export[df_export["ìƒí’ˆëª…"].astype(str).str.strip() != ""].reset_index(drop=True)

        try:
            date_str, _ = export_inventory_snapshot(df_export)

            df_roll = df_export.copy()
            remain = pd.to_numeric(df_roll["ë‚¨ì€ìˆ˜ëŸ‰"], errors="coerce").fillna(0.0)
            df_roll["ì¬ê³ "] = remain.clip(lower=0.0)  # âœ… ìŒìˆ˜ëŠ” ì¬ê³ ë¡œ ì´ê´€í•˜ì§€ ì•ŠìŒ
            for c in ["ì…ê³ ", "1ì°¨", "2ì°¨", "3ì°¨"]:
                df_roll[c] = 0.0

            df_roll = df_roll[["ìƒí’ˆëª…", "ì¬ê³ ", "ì…ê³ ", "1ì°¨", "2ì°¨", "3ì°¨"]]
            df_roll = compute_inventory_df(df_roll)
            df_roll = sort_inventory_df(df_roll).reset_index(drop=True)
            df_roll = df_roll[df_roll["ìƒí’ˆëª…"].astype(str).str.strip() != ""].reset_index(drop=True)

            st.session_state["inventory_df"] = df_roll
            save_inventory_df(df_roll)

            st.session_state["inventory_editor_version"] = ver + 1
            st.session_state["inventory_toast"] = (
                f"ë‚´ë³´ë‚´ê¸° ì™„ë£Œ! ë‚¨ì€ìˆ˜ëŸ‰ì„ ì¬ê³ ë¡œ ì´ê´€(ìŒìˆ˜ëŠ” 0 ì²˜ë¦¬)í–ˆê³ , ë‚˜ë¨¸ì§€ëŠ” 0ìœ¼ë¡œ ì´ˆê¸°í™”í–ˆìŠµë‹ˆë‹¤. "
                f"(ì‚¬ì´ë“œë°” â–¶ ğŸ“ ë‚´ë³´ë‚´ê¸° í´ë” â–¶ {date_str})"
            )
            st.session_state["last_export_date"] = date_str
            st.rerun()
        except Exception as e:
            st.error(f"ë‚´ë³´ë‚´ê¸° ì‹¤íŒ¨: {e}")



# =====================================================
# Sales Calc Page (ë§¤ì¶œê³„ì‚°)  âœ… 2.py ê¸°ëŠ¥ í†µí•©
# =====================================================
def _sales_is_zip_xlsx(file_bytes: bytes) -> bool:
    # Normal xlsx starts with PK.. (zip)
    return file_bytes[:4] == b"PK\x03\x04"


def _sales_decrypt_excel_bytes(file_bytes: bytes, password: str = EXCEL_PASSWORD) -> io.BytesIO:
    """
    Returns a BytesIO that can be read by pandas/openpyxl.
    - If file is normal xlsx(zip), returns as-is.
    - If file is encrypted (OLE), decrypts using msoffcrypto.
    """
    if _sales_is_zip_xlsx(file_bytes):
        return io.BytesIO(file_bytes)

    if msoffcrypto is None:
        raise RuntimeError(
            "ì´ ì—‘ì…€ì€ ë¹„ë°€ë²ˆí˜¸ë¡œ ì•”í˜¸í™”ë˜ì–´ ìˆì–´ìš”. requirements.txtì— 'msoffcrypto-tool'ì„ ì¶”ê°€í•´ ì„¤ì¹˜í•´ ì£¼ì„¸ìš”."
        )

    decrypted = io.BytesIO()
    office = msoffcrypto.OfficeFile(io.BytesIO(file_bytes))
    office.load_key(password=password)
    office.decrypt(decrypted)
    decrypted.seek(0)
    return decrypted


def _sales_to_number(series: pd.Series) -> pd.Series:
    # ìˆ«ì/ë¬¸ì ì„ì—¬ ìˆì–´ë„ ì•ˆì „í•˜ê²Œ ìˆ«ìë¡œ ë³€í™˜ (ì½¤ë§ˆ, ì›, ê³µë°± ë“± ì œê±°)
    return pd.to_numeric(
        series.astype(str).str.replace(r"[^\d\.-]", "", regex=True),
        errors="coerce",
    )


def _sales_normalize_text_series(series: pd.Series) -> pd.Series:
    return (
        series.astype(str)
        .replace({"nan": "", "None": ""})
        .str.replace(r"\s+", " ", regex=True)
        .str.strip()
    )


def _sales_norm_no_space(x: str) -> str:
    return re.sub(r"\s+", "", str(x or "")).strip()


def _sales_find_col(cols: List[str], candidates: List[str]) -> Optional[str]:
    # 1) exact match
    for c in candidates:
        if c in cols:
            return c

    # 2) normalized match (remove spaces/newlines)
    cols_norm = {_sales_norm_no_space(c): c for c in cols}
    for cand in candidates:
        n = _sales_norm_no_space(cand)
        if n in cols_norm:
            return cols_norm[n]

    # 3) substring match
    for cand in candidates:
        for col in cols:
            if str(cand) and str(cand) in str(col):
                return col

    return None


def _sales_detect_header_row(df: pd.DataFrame, max_scan: int = 30) -> int:
    """
    ì—‘ì…€ ìƒë‹¨ì— ì•ˆë‚´ë¬¸/ìš”ì•½ ë“±ì´ ì„ì—¬ ìˆì„ ìˆ˜ ìˆì–´
    ì•ìª½ ëª‡ ì¤„ ìŠ¤ìº” í›„ 'êµ¬ë§¤ìëª…/ìˆ˜ì·¨ì¸ëª…'ì´ í•¨ê»˜ ì¡´ì¬í•˜ëŠ” ì¤„ì„ í—¤ë”ë¡œ íŒë‹¨.
    """
    must_have = {_sales_norm_no_space("êµ¬ë§¤ìëª…"), _sales_norm_no_space("ìˆ˜ì·¨ì¸ëª…")}

    scan_n = min(max_scan, len(df))
    for r in range(scan_n):
        row_vals = df.iloc[r].astype(str).tolist()
        row_norm_set = set(_sales_norm_no_space(v) for v in row_vals if str(v).strip() != "")
        if must_have.issubset(row_norm_set):
            return r

    return 0


def _sales_read_excel_sheets(file_bytes: bytes) -> Dict[str, pd.DataFrame]:
    bio = _sales_decrypt_excel_bytes(file_bytes, EXCEL_PASSWORD)
    raw = pd.read_excel(bio, sheet_name=None, header=None, engine="openpyxl")

    sheets: Dict[str, pd.DataFrame] = {}
    for name, df in raw.items():
        if df is None or df.empty:
            continue

        header_row = _sales_detect_header_row(df, max_scan=30)
        header = df.iloc[header_row].astype(str).str.strip().tolist()

        # make header unique (avoid duplicate col names)
        seen = {}
        new_cols = []
        for h in header:
            h2 = (h or "").strip()
            if h2.lower() == "nan" or h2 == "":
                h2 = "col"
            cnt = seen.get(h2, 0)
            new_cols.append(h2 if cnt == 0 else f"{h2}_{cnt}")
            seen[h2] = cnt + 1

        data = df.iloc[header_row + 1 :].copy()
        data.columns = new_cols
        data = data.dropna(how="all").reset_index(drop=True)
        sheets[name] = data

    return sheets


def _sales_compute_from_sheets(sheets: Dict[str, pd.DataFrame]) -> Tuple[float, set]:
    """
    Returns:
      (sum_of_final_order_amount, set_of_unique_keys_with_nonzero_shipping)
    """
    AMOUNT_CANDS = ["ìµœì¢… ìƒí’ˆë³„ ì´ ì£¼ë¬¸ê¸ˆì•¡"]
    SHIP_CANDS = ["ë°°ì†¡ë¹„ í•©ê³„"]
    BUYER_CANDS = ["êµ¬ë§¤ìëª…"]
    RECIP_CANDS = ["ìˆ˜ì·¨ì¸ëª…"]
    ADDR_CANDS = ["í†µí•©ë°°ì†¡ì§€", "ì£¼ì†Œ", "ë°°ì†¡ì§€", "ìˆ˜ì·¨ì¸ì£¼ì†Œ", "ìˆ˜ë ¹ì¸ì£¼ì†Œ", "ìˆ˜ì·¨ì¸ ì£¼ì†Œ", "ìˆ˜ë ¹ì¸ ì£¼ì†Œ"]

    total_amount = 0.0
    nonzero_people_keys: set = set()

    for _, df in sheets.items():
        cols = [str(c).strip() for c in df.columns]

        amount_col = _sales_find_col(cols, AMOUNT_CANDS)
        ship_col = _sales_find_col(cols, SHIP_CANDS)
        buyer_col = _sales_find_col(cols, BUYER_CANDS)
        recip_col = _sales_find_col(cols, RECIP_CANDS)
        addr_col = _sales_find_col(cols, ADDR_CANDS)

        if amount_col is not None:
            amt = _sales_to_number(df[amount_col])
            total_amount += float(amt.sum(skipna=True) or 0.0)

        if ship_col is not None:
            ship = _sales_to_number(df[ship_col]).fillna(0)
            nonzero_mask = ship != 0

            buyer = _sales_normalize_text_series(df[buyer_col]) if buyer_col else pd.Series([""] * len(df))
            recip = _sales_normalize_text_series(df[recip_col]) if recip_col else pd.Series([""] * len(df))
            addr = _sales_normalize_text_series(df[addr_col]) if addr_col else pd.Series([""] * len(df))

            keys = (buyer + "||" + recip + "||" + addr)
            keys = keys[nonzero_mask].dropna()

            # ë¹ˆ í‚¤ ì œê±°
            keys = keys[keys.str.replace("||", "", regex=False).str.strip() != ""]
            nonzero_people_keys.update(keys.tolist())

    return total_amount, nonzero_people_keys


def _sales_fmt_commas(x) -> str:
    if x is None:
        return ""
    try:
        if pd.isna(x):
            return ""
    except Exception:
        pass

    try:
        v = float(x)
    except Exception:
        return str(x)

    # integer-like
    if abs(v - round(v)) < 1e-9:
        return f"{int(round(v)):,}"

    # keep decimals (trim trailing zeros)
    s = f"{v:,.10f}"
    s = s.rstrip("0").rstrip(".")
    return s


def _sales_fmt_won(x) -> str:
    s = _sales_fmt_commas(x)
    return f"{s} ì›" if s != "" else ""


def _sales_fmt_person(x) -> str:
    s = _sales_fmt_commas(x)
    return f"{s} ëª…" if s != "" else ""


def render_sales_calc_page():
    st.title("ğŸ’° ë§¤ì¶œê³„ì‚°")

    # ğŸ”’ ë¹„ë°€ë²ˆí˜¸ ë³´í˜¸ (ë§¤ì¶œê³„ì‚°)    # ğŸ”’ ë¹„ë°€ë²ˆí˜¸ ë³´í˜¸ (ë§¤ì¶œê³„ì‚°)
    # âš ï¸ Streamlit ì œì•½: ìœ„ì ¯ì´ ìƒì„±ëœ ë’¤ì—ëŠ” ë™ì¼ keyì˜ session_state ê°’ì„ ê°™ì€ runì—ì„œ ì§ì ‘ ë³€ê²½í•˜ë©´ ì˜¤ë¥˜ê°€ ë‚©ë‹ˆë‹¤.
    # ê·¸ë˜ì„œ ë¹„ë°€ë²ˆí˜¸ ì…ë ¥ì¹¸(value)ì€ ê±´ë“œë¦¬ì§€ ì•Šê³ , ì¸ì¦ ì„±ê³µ ì‹œì—ëŠ” rerun í›„(ìœ„ì ¯ ë¯¸ìƒì„± ìƒíƒœ)ì—ì„œë§Œ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
    if st.session_state.get("sales_authed", False):
        # ì¸ì¦ëœ ìƒíƒœì—ì„œëŠ” ë¹„ë°€ë²ˆí˜¸ ì…ë ¥ê°’ì„ ì§€ì›Œë‘ (ì´ runì—ì„œëŠ” ì…ë ¥ ìœ„ì ¯ì´ ì—†ì–´ì„œ ì•ˆì „)
        if "sales_password_input" in st.session_state:
            try:
                del st.session_state["sales_password_input"]
            except Exception:
                pass

        if st.button("ğŸ”“ ë¡œê·¸ì•„ì›ƒ", use_container_width=False, key="sales_logout_btn"):
            st.session_state["sales_authed"] = False
            st.rerun()

    else:
        st.caption("ì´ í˜ì´ì§€ëŠ” ë¹„ë°€ë²ˆí˜¸ê°€ í•„ìš”í•©ë‹ˆë‹¤.")

        pw = None
        ok = False
        left_col, _ = st.columns([2, 8])
        with left_col:
            with st.form("sales_pw_form", clear_on_submit=False):
                pw = st.text_input(
                    "ë¹„ë°€ë²ˆí˜¸",
                    type="password",
                    key="sales_password_input",
                    label_visibility="collapsed",
                    placeholder="ë¹„ë°€ë²ˆí˜¸",
                )
                ok = st.form_submit_button("ì…ì¥", use_container_width=False)

            if ok:
                if (pw or "").strip() == "1390":
                    st.session_state["sales_authed"] = True
                    st.rerun()
                else:
                    st.error("ë¹„ë°€ë²ˆí˜¸ê°€ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤.")

        st.stop()

    st.subheader("ğŸ“Š ë„¤ì´ë²„ ë§¤ì¶œ ì—‘ì…€ í•©ê³„ ê³„ì‚°ê¸°")

    uploaded_files = st.file_uploader(
        "ì—‘ì…€ íŒŒì¼ ì—…ë¡œë“œ (ë¹„ë°€ë²ˆí˜¸ 0000 ê³ ì •) â€” ì—¬ëŸ¬ ê°œ ì—…ë¡œë“œ ê°€ëŠ¥",
        type=["xlsx"],
        accept_multiple_files=True,
        key="sales_uploaded_files",
    )

    left, _ = st.columns([1, 2])
    with left:
        calc_btn = st.button("âœ… ê³„ì‚°", use_container_width=True, key="sales_calc_btn")

    if calc_btn:
        if not uploaded_files:
            st.warning("ë¨¼ì € ì—‘ì…€ íŒŒì¼ì„ ì—…ë¡œë“œí•´ ì£¼ì„¸ìš”.")
        else:
            per_file_rows = []
            grand_amount = 0.0

            # âœ… ì „ì²´ ê²°ê³¼ì˜ ì¸ì›ìˆ˜ = "íŒŒì¼ë³„(ê° íŒŒì¼ ë‚´ë¶€ ì¤‘ë³µ ì œê±°) ì¸ì›ìˆ˜"ë¥¼ í•©ì‚°
            grand_unique_count_sum = 0

            progress = st.progress(0)

            for i, f in enumerate(uploaded_files, start=1):
                try:
                    sheets = _sales_read_excel_sheets(f.getvalue())
                    amount_sum, keyset = _sales_compute_from_sheets(sheets)

                    unique_count = len(keyset)  # íŒŒì¼ ë‚´ë¶€(ì‹œíŠ¸ í¬í•¨) ì¤‘ë³µ ì œê±°
                    shipping_calc = unique_count * 3500

                    per_file_rows.append(
                        {
                            "íŒŒì¼ëª…": f.name,
                            "ìµœì¢… ìƒí’ˆë³„ ì´ ì£¼ë¬¸ê¸ˆì•¡ í•©ê³„": amount_sum,
                            "ë°°ì†¡ë¹„â‰ 0 (ì¤‘ë³µì œê±° ì¸ì›ìˆ˜)": unique_count,
                            "ì¸ì›Ã—3,500 í•©ê³„": shipping_calc,
                        }
                    )

                    grand_amount += amount_sum
                    grand_unique_count_sum += unique_count  # âœ… íŒŒì¼ë³„ í•©ì‚°

                except Exception as e:
                    per_file_rows.append(
                        {
                            "íŒŒì¼ëª…": f.name,
                            "ìµœì¢… ìƒí’ˆë³„ ì´ ì£¼ë¬¸ê¸ˆì•¡ í•©ê³„": None,
                            "ë°°ì†¡ë¹„â‰ 0 (ì¤‘ë³µì œê±° ì¸ì›ìˆ˜)": None,
                            "ì¸ì›Ã—3,500 í•©ê³„": None,
                            "ì˜¤ë¥˜": str(e),
                        }
                    )

                progress.progress(i / len(uploaded_files))

            grand_shipping_calc = grand_unique_count_sum * 3500
            summary_df = pd.DataFrame(per_file_rows)

            st.session_state["sales_result"] = {
                "summary_df": summary_df,
                "grand_amount": grand_amount,
                "grand_unique_count_sum": grand_unique_count_sum,
                "grand_shipping_calc": grand_shipping_calc,
            }

    if "sales_result" in st.session_state:
        res = st.session_state["sales_result"]
        summary_df = res["summary_df"]
        grand_amount = res["grand_amount"]
        grand_unique_count_sum = res["grand_unique_count_sum"]
        grand_shipping_calc = res["grand_shipping_calc"]

        st.subheader("âœ… ì „ì²´ ê²°ê³¼")

        amount_view = _sales_fmt_commas(grand_amount)
        shipping_view = _sales_fmt_commas(grand_shipping_calc)

        # âœ… ê²½ê³  ë°©ì§€ + ê°’ ë¶ˆì¼ì¹˜ ë°©ì§€:
        # text_inputì— value=ë¥¼ ì£¼ì§€ ì•Šê³ , session_stateë¡œë§Œ ê°’ì„ ì„¸íŒ…
        st.session_state["sales_copy_total_amount_fmt_only"] = amount_view
        st.session_state["sales_copy_shipping_fmt_only"] = shipping_view

        # âœ… â€œğŸ“‹ ì—‘ì…€ ë³µì‚¬ìš©â€ì„ ë§¨ ì™¼ìª½ìœ¼ë¡œ ë°°ì¹˜
        c_copy, c1, c2, c3 = st.columns([1.3, 1, 1, 1])

        with c_copy:
            st.caption("ğŸ“‹ ì—‘ì…€ ë³µì‚¬ìš© (í´ë¦­ â†’ Ctrl+C)")
            st.text_input(
                "ìµœì¢… ìƒí’ˆë³„ ì´ ì£¼ë¬¸ê¸ˆì•¡ ì´í•© (í‘œì‹œìš© / ì½¤ë§ˆ)",
                key="sales_copy_total_amount_fmt_only",
            )
            st.text_input(
                "ì¸ì›Ã—3,500ì› í•©ê³„ (í‘œì‹œìš© / ì½¤ë§ˆ)",
                key="sales_copy_shipping_fmt_only",
            )

        c1.metric("ìµœì¢… ìƒí’ˆë³„ ì´ ì£¼ë¬¸ê¸ˆì•¡ ì´í•©", f"{amount_view} ì›")
        c2.metric("ë°°ì†¡ë¹„â‰ 0 ì¸ì›ìˆ˜(íŒŒì¼ë³„ í•©ì‚°)", f"{_sales_fmt_commas(grand_unique_count_sum)} ëª…")
        c3.metric("ì¸ì›Ã—3,500 í•©ê³„", f"{shipping_view} ì›")

        st.subheader("íŒŒì¼ë³„ ìƒì„¸")

        # âœ… íŒŒì¼ë³„ ìƒì„¸ì—ì„œ ìˆ«ìë¥¼ í†µí™”ë¡œ í‘œì‹œ
        display_df = summary_df.copy()

        if "ìµœì¢… ìƒí’ˆë³„ ì´ ì£¼ë¬¸ê¸ˆì•¡ í•©ê³„" in display_df.columns:
            display_df["ìµœì¢… ìƒí’ˆë³„ ì´ ì£¼ë¬¸ê¸ˆì•¡ í•©ê³„"] = display_df["ìµœì¢… ìƒí’ˆë³„ ì´ ì£¼ë¬¸ê¸ˆì•¡ í•©ê³„"].apply(_sales_fmt_won)

        if "ì¸ì›Ã—3,500 í•©ê³„" in display_df.columns:
            display_df["ì¸ì›Ã—3,500 í•©ê³„"] = display_df["ì¸ì›Ã—3,500 í•©ê³„"].apply(_sales_fmt_won)

        if "ë°°ì†¡ë¹„â‰ 0 (ì¤‘ë³µì œê±° ì¸ì›ìˆ˜)" in display_df.columns:
            display_df["ë°°ì†¡ë¹„â‰ 0 (ì¤‘ë³µì œê±° ì¸ì›ìˆ˜)"] = display_df["ë°°ì†¡ë¹„â‰ 0 (ì¤‘ë³µì œê±° ì¸ì›ìˆ˜)"].apply(_sales_fmt_person)

        st.dataframe(display_df, use_container_width=True)



# =====================================================
# Page: ğŸ§° ì¬ê³ ì¼ê´„ë³€ê²½ (2.py ê¸°ëŠ¥ ì´ì‹)
# =====================================================
def render_bulk_stock_page():
    import io
    import json
    import os
    import hashlib
    from dataclasses import dataclass
    from datetime import datetime
    import time
    from decimal import Decimal, InvalidOperation, getcontext
    import re
    from pathlib import Path
    from typing import Dict, Any, Tuple, List

    import pandas as pd
    import streamlit as st
    from openpyxl import load_workbook

    # ============================
    # Persistent config (best effort)
    # ============================
    APP_DIR = Path(__file__).parent

    # Prefer a writable data dir (avoid writing into repo root directly)
    DATA_DIR = Path(os.environ.get("STOCKAPP_DATA_DIR") or str(Path(globals().get("APP_DATA_DIR", APP_DIR)) / "stock_bulk"))
    DATA_DIR.mkdir(parents=True, exist_ok=True)

    CONFIG_FILE = DATA_DIR / "stock_config.json"
    CONFIG_BAK = DATA_DIR / "stock_config.bak.json"

    EXCEL_CONFIG_SHEET = "_STOCKAPP_CONFIG"
    EXCEL_CONFIG_CELL = "A1"

    DEFAULT_PRODUCTS = [
        {"name": "ì—”ë‹¤ì´ë¸Œ", "keyword": "ì—”ë‹¤ì´ë¸Œ"},
        {"name": "ìƒ¬ë¡¯", "keyword": "ìƒ¬ë¡¯"},
        {"name": "ì•„ìŠ¤íŒŒë¼", "keyword": "ìƒ ì•„ìŠ¤íŒŒë¼"},
        {"name": "í™”ì´íŠ¸ì•„ìŠ¤íŒŒë¼", "keyword": "í™”ì´íŠ¸ ì•„ìŠ¤íŒŒë¼"},
        {"name": "ë¯¸ë‹ˆì–‘ë°°ì¶”", "keyword": "ë¯¸ë‹ˆì–‘ë°°ì¶”"},
        {"name": "ì–‘ì†¡ì´", "keyword": "ì–‘ì†¡ì´"},
        {"name": "ìƒˆì†¡ì´", "keyword": "ìƒˆì†¡ì´"},
        {"name": "ëŠíƒ€ë¦¬", "keyword": "ëŠíƒ€ë¦¬"},
        {"name": "íŒ½ì´", "keyword": "íŒ½ì´"},
        {"name": "í™ë‹¹ê·¼", "keyword": "í™ë‹¹ê·¼"},
        {"name": "ë¸Œë¡œì½œë¦¬", "keyword": "ë¸Œë¡œì½œë¦¬"},
        {"name": "ì»¬ë¦¬í”Œë¼ì›Œ", "keyword": "ì»¬ë¦¬í”Œë¼ì›Œ"},
        {"name": "ì¤„ê¸°ìƒëŸ¬ë¦¬", "keyword": "ì¤„ê¸°ìƒëŸ¬ë¦¬"},
        {"name": "ì˜¤ë Œì§€", "keyword": "ì˜¤ë Œì§€"},
        {"name": "ìëª½", "keyword": "ìëª½"},
        {"name": "ë ˆëª¬", "keyword": "ë ˆëª¬"},
        {"name": "ë¼ì„", "keyword": "ë¼ì„"},
        {"name": "ì–‘ìƒì¶”", "keyword": "ì–‘ìƒì¶”"},
        {"name": "ì•Œë°°ê¸°", "keyword": "ì•Œë°°ê¸°"},
        {"name": "ë°©ìš¸í† ë§ˆí† ", "keyword": "ë°©ìš¸í† ë§ˆí† "},
        {"name": "ì™„ìˆ™í† ë§ˆí† ", "keyword": "ì™„ìˆ™í† ë§ˆí† "},
        {"name": "ì•„ë³´ì¹´ë„", "keyword": "ì•„ë³´ì¹´ë„"},
        {"name": "ì‹ìš©ê½ƒ", "keyword": "ì‹ìš©ê½ƒ"},
        {"name": "ì²­í”¼ë§", "keyword": "ì²­í”¼ë§"},
        {"name": "ë¯¸ë‹ˆíŒŒí”„ë¦¬ì¹´", "keyword": "ë¯¸ë‹ˆ íŒŒí”„ë¦¬ì¹´"},
        {"name": "ì‚¼ìƒ‰íŒŒí”„ë¦¬ì¹´", "keyword": "ì‚¼ìƒ‰ íŒŒí”„ë¦¬ì¹´"},
        {"name": "ë¹„íŠ¸", "keyword": "ë¹„íŠ¸"},
        {"name": "ì½œë¼ë¹„", "keyword": "ì½œë¼ë¹„"},
        {"name": "íŒŒì„¸ë¦¬", "keyword": "íŒŒì„¸ë¦¬"},
        {"name": "ê¹ë§ˆëŠ˜", "keyword": "ê¹ë§ˆëŠ˜"},
        {"name": "ë‹¨í˜¸ë°•", "keyword": "ë‹¨í˜¸ë°•"},
        {"name": "ì¥¬í‚¤ë‹ˆ", "keyword": "ì¥¬í‚¤ë‹ˆ"},
        {"name": "ê°€ì§€", "keyword": "ê°€ì§€"},
        {"name": "ë°±ì˜¤ì´", "keyword": "ë°±ì˜¤ì´"},
    ]


    def _default_config() -> Dict[str, Any]:
        return {
            "version": 9,
            "inventory_column": "ì¬ê³ ìˆ˜ëŸ‰",
            "name_column": "ìƒí’ˆëª…",
            "products": [],
            "rules": {},  # {base_product: [ {keyword, mode, value, table}, ... ]}
            "ref_qty": {},  # {product_name: "ì°¸ê³ ìˆ˜ëŸ‰"}
            "recognition_logic": DEFAULT_RECOGNITION_LOGIC.copy(),
        }


    def _atomic_write(path: Path, text: str, encoding: str = "utf-8") -> None:
        tmp = path.with_suffix(path.suffix + ".tmp")
        tmp.write_text(text, encoding=encoding)
        tmp.replace(path)


    def load_config() -> Dict[str, Any]:
        for p in [CONFIG_FILE, CONFIG_BAK]:
            if p.exists():
                try:
                    cfg = json.loads(p.read_text(encoding="utf-8"))
                    if not isinstance(cfg, dict):
                        continue
                    cfg.setdefault("version", 9)
                    cfg.setdefault("inventory_column", "ì¬ê³ ìˆ˜ëŸ‰")
                    cfg.setdefault("name_column", "ìƒí’ˆëª…")
                    cfg.setdefault("products", [])
                    cfg.setdefault("rules", {})
                    cfg.setdefault("ref_qty", {})
                    cfg.setdefault("recognition_logic", DEFAULT_RECOGNITION_LOGIC.copy())
                    return cfg
                except Exception:
                    continue
        return _default_config()


    def save_config(cfg: Dict[str, Any]) -> None:
        cfg["version"] = 9
        txt = json.dumps(cfg, ensure_ascii=False, indent=2)
        if CONFIG_FILE.exists():
            try:
                _atomic_write(CONFIG_BAK, CONFIG_FILE.read_text(encoding="utf-8"))
            except Exception:
                pass
        _atomic_write(CONFIG_FILE, txt)


    # ----------------------------
    # Excel helpers
    # ----------------------------
    def find_header_row_and_columns(ws, name_col: str, inv_col: str, max_scan_rows: int = 15) -> Tuple[int, int, int]:
        for r in range(1, max_scan_rows + 1):
            row_vals = [ws.cell(r, c).value for c in range(1, ws.max_column + 1)]
            row_str = [str(v).strip() if v is not None else "" for v in row_vals]
            if name_col in row_str and inv_col in row_str:
                name_idx = row_str.index(name_col) + 1
                inv_idx = row_str.index(inv_col) + 1
                return r, name_idx, inv_idx
        raise ValueError(f"í—¤ë”ì—ì„œ '{name_col}' ë˜ëŠ” '{inv_col}' ì»¬ëŸ¼ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")


    def to_number(x) -> float:
        if x is None:
            return 0.0
        if isinstance(x, (int, float)):
            return float(x)
        s = str(x).strip()
        if s == "":
            return 0.0
        s = s.replace(",", "")
        try:
            return float(s)
        except Exception:
            return 0.0


    def parse_input_number(x) -> float:
        """Parse user input from data_editor (string or number)."""
        if x is None:
            return 0.0
        if isinstance(x, (int, float)):
            return float(x)
        s = str(x).strip()
        if s == "":
            return 0.0
        s = s.replace(",", "")
        try:
            return float(s)
        except Exception:
            return 0.0


    def qty_key(q: float) -> str:
        """Normalize input quantity to mapping key string."""
        try:
            qf = float(q)
        except Exception:
            return str(q)
        if abs(qf - int(qf)) < 1e-9:
            return str(int(qf))
        return str(qf)


    def parse_map_string(s: str) -> Dict[str, int]:
        """
        Accept formats:
          - "2=5, 3=7"
          - "2:5\n3:7"
          - "2 -> 5"
        Return {"2":5, "3":7}
        Values are stored as INT to avoid 5.0 display.
        """
        if not s:
            return {}
        txt = str(s).strip()
        if not txt:
            return {}
        txt = txt.replace("\n", ",")
        txt = txt.replace("â†’", "=").replace("->", "=").replace(":", "=")
        out: Dict[str, int] = {}
        for chunk in [x.strip() for x in txt.split(",") if x.strip()]:
            if "=" not in chunk:
                continue
            k, v = chunk.split("=", 1)
            k = k.strip()
            v = v.strip()
            if not k or not v:
                continue

            # allow wildcard key "*" (ì…ë ¥ê°’ ë¬´ê´€ ê³ ì • ì ìš©)
            if k == "*":
                try:
                    out["*"] = int(round(float(v)))
                except Exception:
                    pass
                continue
            try:
                kf = float(k)
                kk = str(int(kf)) if abs(kf - int(kf)) < 1e-9 else str(kf)
                out[kk] = int(round(float(v)))
            except Exception:
                continue
        return out


    def fmt_int(v) -> str:
        try:
            return str(int(round(float(v))))
        except Exception:
            return str(v)


    def fmt_qty_for_memo(v: float) -> str:
        """Pretty formatting for memo txt (avoid trailing .0)."""
        try:
            vf = float(v)
        except Exception:
            return str(v)
        if abs(vf - int(vf)) < 1e-9:
            return str(int(vf))
        s = f"{vf:.12f}".rstrip("0").rstrip(".")
        return s if s else "0"


    # ----------------------------
    # Inventory total (display-only)
    # ----------------------------
    getcontext().prec = 28

    _RE_KG = re.compile(r"(\d+(?:\.\d+)?)\s*kg", re.IGNORECASE)
    _RE_G = re.compile(r"(\d+(?:\.\d+)?)\s*g", re.IGNORECASE)
    _RE_PACK = re.compile(r"(\d+(?:\.\d+)?)\s*íŒ©")

    _RE_PACK_EN = re.compile(r"(\d+(?:\.\d+)?)\s*pack", re.IGNORECASE)
    _RE_BONG = re.compile(r"(\d+(?:\.\d+)?)\s*ë´‰")
    _RE_TONG = re.compile(r"(\d+(?:\.\d+)?)\s*í†µ")
    _RE_EA = re.compile(r"(\d+(?:\.\d+)?)\s*ê°œ")
    _RE_BOX = re.compile(r"(\d+(?:\.\d+)?)\s*(ë°•ìŠ¤|box)", re.IGNORECASE)


    # ----------------------------
    # Recognition logic (unit parsing)
    # ----------------------------
    # ê° í•­ëª©: priority(ë‚®ì„ìˆ˜ë¡ ë¨¼ì €), output_unit(í‘œì‹œ ë‹¨ìœ„), multiplier(ìˆ«ìì— ê³±), aliases(ì¸ì‹í•  ë¬¸ìì—´ë“¤)
    DEFAULT_RECOGNITION_LOGIC: List[Dict[str, Any]] = [
        {"priority": 10, "output_unit": "ë‹¨",   "multiplier": "1",     "aliases": ["ë‹¨"]},
        {"priority": 20, "output_unit": "íŒ©",   "multiplier": "1",     "aliases": ["íŒ©", "pack"]},
        {"priority": 30, "output_unit": "ë´‰",   "multiplier": "1",     "aliases": ["ë´‰"]},
        {"priority": 40, "output_unit": "í†µ",   "multiplier": "1",     "aliases": ["í†µ"]},
        {"priority": 50, "output_unit": "ê°œ",   "multiplier": "1",     "aliases": ["ê°œ", "ea"]},
        {"priority": 60, "output_unit": "ë°•ìŠ¤", "multiplier": "1",     "aliases": ["ë°•ìŠ¤", "box"]},
        # gëŠ” kgë¡œ í™˜ì‚°(ì˜ˆ: 500g -> 0.5kg)
        {"priority": 90, "output_unit": "kg",   "multiplier": "0.001", "aliases": ["g", "ê·¸ë¨"]},
        {"priority": 100,"output_unit": "kg",   "multiplier": "1",     "aliases": ["kg", "í‚¬ë¡œ", "í‚¤ë¡œ"]},
    ]

    def _normalize_recognition_logic(logic: Any) -> List[Dict[str, Any]]:
        """configì— ì €ì¥ëœ recognition_logicë¥¼ ì•ˆì „í•˜ê²Œ ì •ê·œí™”í•©ë‹ˆë‹¤."""
        if not isinstance(logic, list):
            logic = []
        cleaned: List[Dict[str, Any]] = []
        for it in logic:
            if not isinstance(it, dict):
                continue
            unit = str(it.get("output_unit") or it.get("unit") or "").strip()
            if not unit:
                continue

            # priority
            try:
                pr = int(it.get("priority", 999))
            except Exception:
                pr = 999

            # multiplier
            mult_raw = it.get("multiplier", "1")
            try:
                mult = str(mult_raw).strip()
                Decimal(mult)  # validate
            except Exception:
                mult = "1"

            aliases = it.get("aliases") or it.get("patterns") or []
            if isinstance(aliases, str):
                aliases = [a.strip() for a in aliases.split(",") if a.strip()]
            if not isinstance(aliases, list):
                aliases = []
            aliases = [str(a).strip() for a in aliases if str(a).strip()]
            if not aliases:
                continue

            cleaned.append({"priority": pr, "output_unit": unit, "multiplier": mult, "aliases": aliases})

        cleaned.sort(key=lambda x: (x.get("priority", 999), len(str(x.get("output_unit","")))))
        return cleaned

    def _get_recognition_logic(cfg: Dict[str, Any]) -> List[Dict[str, Any]]:
        return _normalize_recognition_logic((cfg or {}).get("recognition_logic")) or DEFAULT_RECOGNITION_LOGIC.copy()



    def _cell_to_decimal(v) -> Decimal:
        if v is None:
            return Decimal("0")
        if isinstance(v, (int, float)):
            # Use str() to preserve "0.1" rather than binary float
            return Decimal(str(v))
        s = str(v).strip().replace(",", "")
        if not s:
            return Decimal("0")
        try:
            return Decimal(s)
        except Exception:
            return Decimal("0")


    def _parse_factor_and_unit(name_str: str, recognition_logic: List[Dict[str, Any]] = None) -> Tuple[Decimal, str]:
        """
        ìƒí’ˆëª…(ë˜ëŠ” ê·œì¹™ í‚¤ì›Œë“œ) ë¬¸ìì—´ì—ì„œ "ìˆ«ì+ë‹¨ìœ„"ë¥¼ ì°¾ì•„ (factor, unit)ì„ ë°˜í™˜í•©ë‹ˆë‹¤.

        - recognition_logic ê¸°ë°˜ìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
          ê° í•­ëª©: priority, output_unit, multiplier, aliases
            ì˜ˆ) {"output_unit":"ë‹¨","multiplier":"1","aliases":["ë‹¨"]}

        - ìˆ«ì+ë‹¨ìœ„ê°€ ìˆìœ¼ë©´ factor = ìˆ«ì * multiplier
        - ë‹¨ìœ„ë§Œ ìˆìœ¼ë©´ factor = 1 * multiplier

        ë°˜í™˜ unitì€ output_unit ì…ë‹ˆë‹¤. (ì˜ˆ: gëŠ” kgë¡œ í™˜ì‚°ë˜ì–´ unit='kg')
        """
        s = name_str or ""
        logic = _normalize_recognition_logic(recognition_logic) if recognition_logic is not None else DEFAULT_RECOGNITION_LOGIC.copy()

        for rule in logic:
            unit = str(rule.get("output_unit", "")).strip()
            if not unit:
                continue
            try:
                mult = Decimal(str(rule.get("multiplier", "1")).strip() or "1")
            except Exception:
                mult = Decimal("1")

            aliases = rule.get("aliases") or []
            if not isinstance(aliases, list):
                aliases = [str(aliases)]

            for alias in aliases:
                alias = str(alias).strip()
                if not alias:
                    continue

                # 1) ìˆ«ì+ë‹¨ìœ„
                try:
                    if alias.isascii():
                        m = re.search(rf"(\d+(?:\.\d+)?)\s*{re.escape(alias)}\b", s, flags=re.IGNORECASE)
                    else:
                        m = re.search(rf"(\d+(?:\.\d+)?)\s*{re.escape(alias)}", s)
                except Exception:
                    m = None

                if m:
                    try:
                        n = Decimal(m.group(1))
                    except Exception:
                        n = Decimal("1")
                    return (n * mult), unit

                # 2) ë‹¨ìœ„ë§Œ ìˆëŠ” ê²½ìš°
                try:
                    if alias.isascii():
                        if re.search(rf"\b{re.escape(alias)}\b", s, flags=re.IGNORECASE):
                            return (Decimal("1") * mult), unit
                    else:
                        if alias in s:
                            return (Decimal("1") * mult), unit
                except Exception:
                    pass

        return Decimal("1"), ""


    def _fmt_decimal(d: Decimal, max_decimals: int = 3) -> str:
        try:
            q = Decimal("1") if max_decimals <= 0 else (Decimal("1") / (Decimal(10) ** max_decimals))
            d2 = d.quantize(q)  # rounding
        except Exception:
            d2 = d

        s = format(d2, "f")
        s = s.rstrip("0").rstrip(".")
        return s if s else "0"


    def compute_stock_display_map(xlsx_bytes: bytes, cfg: Dict[str, Any]) -> Dict[str, str]:
        """
        Build display stock totals per base product using **ê·œì¹™ê´€ë¦¬ í‚¤ì›Œë“œ**ë¥¼ ì°¸ê³ í•˜ì—¬ ê³„ì‚°í•©ë‹ˆë‹¤.

        - ê° ê¸°ì¤€ìƒí’ˆ(base)ì— ëŒ€í•´ ê·œì¹™(í‚¤ì›Œë“œ)ì„ ì´ìš©í•´ ì˜µì…˜ ë‹¨ìœ„ë¥¼ íŒŒì•…í•©ë‹ˆë‹¤.
          ì˜ˆ) '1kg', '500g', '100g' -> kg(í•©ì‚°),  '5íŒ©' -> íŒ©,  '6í†µ' -> í†µ
        - ì—‘ì…€ì˜ ê° í–‰(ìƒí’ˆëª…)ì„ ê·œì¹™ í‚¤ì›Œë“œì™€ ë§¤ì¹­í•˜ì—¬, ì¬ê³ ìˆ˜ëŸ‰ * (ì˜µì…˜ ë‹¨ìœ„ ìˆ˜ëŸ‰) ë¥¼ í•©ì‚°í•©ë‹ˆë‹¤.
          ì˜ˆ) ì–‘ìƒì¶”6í†µ:3ê°œ + ì–‘ìƒì¶”1í†µ:3ê°œ => 3*6 + 3*1 = 21í†µ

        Stock ì—†ëŠ” ê²½ìš°ëŠ” '0'ìœ¼ë¡œ í‘œì‹œí•©ë‹ˆë‹¤.
        """
        inv_col = cfg.get("inventory_column", "ì¬ê³ ìˆ˜ëŸ‰")
        name_col = cfg.get("name_column", "ìƒí’ˆëª…")
        recog_logic = _get_recognition_logic(cfg)

        wb = load_workbook(io.BytesIO(xlsx_bytes), data_only=True)
        ws = wb.active
        header_row, name_idx, inv_idx = find_header_row_and_columns(ws, name_col=name_col, inv_col=inv_col)

        products = cfg.get("products", []) or []
        base_kw: Dict[str, str] = {}
        for p in products:
            bn = str(p.get("name", "")).strip()
            kw = str(p.get("keyword") or p.get("name") or "").strip()
            if bn:
                base_kw[bn] = kw or bn

        bases_sorted = sorted([(bn, base_kw.get(bn, bn)) for bn in base_kw.keys()], key=lambda x: len(str(x[1] or "")), reverse=True)

        rules_map = cfg.get("rules", {}) or {}

        def _scoped_tokens(base_keyword: str, rule_keyword: str) -> List[str]:
            base_keyword = str(base_keyword or "").strip()
            rule_keyword = str(rule_keyword or "").strip()
            if not rule_keyword:
                return []
            if not base_keyword:
                return [rule_keyword]
            # If user already typed full keyword incl base keyword, treat as absolute substring match
            if base_keyword in rule_keyword:
                return [rule_keyword]
            # Otherwise require BOTH base keyword and rule keyword
            return [base_keyword, rule_keyword]

        # Build matchers from rules (per base)
        matchers_by_base: Dict[str, List[Dict[str, Any]]] = {}
        unit_pref: Dict[str, str] = {}
        unit_seen: Dict[str, str] = {}

        for bn, bkw in bases_sorted:
            rs = rules_map.get(bn, []) or []
            ms: List[Dict[str, Any]] = []

            unit_counts: Dict[str, int] = {}
            for i, r in enumerate(rs):
                rr = Rule.from_dict(r)
                if not rr.keyword:
                    continue

                tokens = _scoped_tokens(bkw, rr.keyword)
                if not tokens:
                    continue

                factor, unit = _parse_factor_and_unit(rr.keyword, recog_logic)

                ms.append(
                    {
                        "tokens": tokens,
                        "keyword": rr.keyword,
                        "factor": factor,
                        "unit": unit,
                        "order": i,
                    }
                )

                if unit:
                    unit_counts[unit] = unit_counts.get(unit, 0) + 1

            # Sort by specificity (longer/ more tokens first)
            ms.sort(
                key=lambda m: (sum(len(t) for t in m["tokens"]), len(m["tokens"]), len(m["keyword"])),
                reverse=True,
            )
            matchers_by_base[bn] = ms

            # Preferred unit: kg wins if any kg/g exists; else most frequent unit in rules
            if unit_counts.get("kg", 0) > 0:
                unit_pref[bn] = "kg"
            elif unit_counts:
                unit_pref[bn] = sorted(unit_counts.items(), key=lambda kv: (kv[1], len(kv[0])), reverse=True)[0][0]
            else:
                unit_pref[bn] = ""

        totals: Dict[str, Decimal] = {}

        def _match_tokens(tokens: List[str], name_str: str) -> bool:
            return bool(tokens) and all((t in name_str) for t in tokens)

        for r in range(header_row + 1, ws.max_row + 1):
            name_val = ws.cell(r, name_idx).value
            if name_val is None or str(name_val).strip() == "":
                continue

            name_str = str(name_val)

            # determine base product by base keyword
            base_name = None
            for bn, kw in bases_sorted:
                if kw and kw in name_str:
                    base_name = bn
                    break
            if base_name is None:
                continue

            inv_qty = _cell_to_decimal(ws.cell(r, inv_idx).value)
            if inv_qty == 0:
                continue

            # 1) Try rules-based matching first
            chosen = None
            for m in matchers_by_base.get(base_name, []):
                if _match_tokens(m["tokens"], name_str):
                    chosen = m
                    break

            has_rules = bool(matchers_by_base.get(base_name))
            if chosen and chosen.get("unit"):
                factor = chosen["factor"]
                unit = chosen["unit"]
            else:
                # âœ… ê·œì¹™ì´ ìˆëŠ” ê¸°ì¤€ìƒí’ˆì€ "ê·œì¹™ì— ë§¤ì¹­ë˜ëŠ” ì˜µì…˜"ë§Œ í•©ì‚° (ë‹¨ìœ„ í˜¼í•© ë°©ì§€)
                # ê·œì¹™ì´ ìˆëŠ”ë° ì–´ë–¤ í‚¤ì›Œë“œì—ë„ ì•ˆ ê±¸ë¦¬ë©´, í•´ë‹¹ í–‰ì€ ì¬ê³  í•©ì‚°ì—ì„œ ì œì™¸í•©ë‹ˆë‹¤.
                if has_rules and chosen is None:
                    continue
                # 2) Fallback: parse from actual name (kg/g/íŒ©/ë´‰/í†µ/ê°œ/ë°•ìŠ¤)
                factor, unit = _parse_factor_and_unit(name_str, recog_logic)

            # Track unit seen (if rules did not define a preferred unit)
            pref = unit_pref.get(base_name, "") or ""
            if pref:
                unit_seen[base_name] = pref
            elif unit:
                unit_seen[base_name] = unit_seen.get(base_name) or unit

            # Sum: inv_qty * factor (factor already normalized: g -> kg)
            totals[base_name] = totals.get(base_name, Decimal("0")) + (inv_qty * factor)

        out: Dict[str, str] = {}
        for bn, _kw in bases_sorted:
            total = totals.get(bn, Decimal("0"))
            if total == 0:
                out[bn] = "0"
                continue

            unit = unit_pref.get(bn, "") or unit_seen.get(bn, "") or ""
            if unit == "kg":
                out[bn] = f"{_fmt_decimal(total, max_decimals=3)}kg"
            else:
                s = _fmt_decimal(total, max_decimals=3)
                out[bn] = f"{s}{unit}" if unit else s

        return out


    # ----------------------------
    # Rule model
    # ----------------------------
    @dataclass
    class Rule:
        keyword: str
        mode: str   # "mul" or "map"
        value: float
        table: Dict[str, int]

        @staticmethod
        def from_dict(d: Dict[str, Any]) -> "Rule":
            keyword = str(d.get("keyword", "")).strip()
            mode_raw = str(d.get("mode", "mul")).strip()

            # legacy fixed -> map wildcard (ì…ë ¥ê°’ ë¬´ê´€í•˜ê²Œ ì ìš©)
            value_raw = float(d.get("value", 1.0) or 0.0)

            table = d.get("table") or {}
            if isinstance(table, str):
                table = parse_map_string(table)
            if not isinstance(table, dict):
                table = {}

            t2: Dict[str, int] = {}
            for k, v in table.items():
                try:
                    t2[str(k)] = int(round(float(v)))
                except Exception:
                    continue

            # migrate fixed -> map wildcard
            if mode_raw == "fixed":
                mode_raw = "map"
                t2.setdefault("*", int(round(value_raw)))

            # ignore legacy round key silently
            if mode_raw not in ("mul", "map"):
                mode_raw = "mul"

            return Rule(keyword=keyword, mode=mode_raw, value=value_raw, table=t2)


    def build_actions(cfg: Dict[str, Any], inputs: Dict[str, float]) -> Tuple[List[Dict[str, Any]], pd.DataFrame]:
        """
        Build merged actions like:
          [{"match_tokens":["ì—”ë‹¤ì´ë¸Œ","1kg"],"display":"ì—”ë‹¤ì´ë¸Œ & 1kg","delta":5.0,"bases":[...]}...]

        âœ… ê·œì¹™ í‚¤ì›Œë“œ ìë™ 'ê¸°ì¤€ìƒí’ˆ ìŠ¤ì½”í”„' ì²˜ë¦¬
        - ê·œì¹™ í‚¤ì›Œë“œì— **ê¸°ì¤€ìƒí’ˆ í‚¤ì›Œë“œ(ì˜ˆ: ì—”ë‹¤ì´ë¸Œ)**ê°€ ì´ë¯¸ í¬í•¨ë˜ì–´ ìˆìœ¼ë©´: ê·¸ëŒ€ë¡œ(ë‹¨ì¼ í¬í•¨ë¬¸ìì—´) ë§¤ì¹­
          ì˜ˆ) "ì—”ë‹¤ì´ë¸Œ1kg" -> 'ì—”ë‹¤ì´ë¸Œ1kg'ê°€ í¬í•¨ëœ ìƒí’ˆëª…ë§Œ ë§¤ì¹­
        - ê·œì¹™ í‚¤ì›Œë“œì— ê¸°ì¤€ìƒí’ˆ í‚¤ì›Œë“œê°€ **ì—†ìœ¼ë©´**: (ê¸°ì¤€ìƒí’ˆ í‚¤ì›Œë“œ AND ê·œì¹™ í‚¤ì›Œë“œ) ë‘˜ ë‹¤ í¬í•¨ëœ ìƒí’ˆëª…ë§Œ ë§¤ì¹­
          ì˜ˆ) ê¸°ì¤€=ì—”ë‹¤ì´ë¸Œ, ê·œì¹™í‚¤ì›Œë“œ="1kg" -> 'ì—”ë‹¤ì´ë¸Œ'ì™€ '1kg'ê°€ ëª¨ë‘ ë“¤ì–´ê°„ ìƒí’ˆëª…ë§Œ ë§¤ì¹­
             (ê·¸ë˜ì„œ ë‹¤ë¥¸ ìƒí’ˆì˜ "1kg"ëŠ” ê±´ë“œë¦¬ì§€ ì•ŠìŠµë‹ˆë‹¤)

        For mode=map: delta = table[input_qty_key]. If not found and "*" exists, use wildcard.
        If not found and no wildcard -> record missing.
        """
        prod_kw = {p.get("name"): (p.get("keyword") or p.get("name")) for p in cfg.get("products", [])}
        rules_map = cfg.get("rules", {}) or {}

        def _scoped_tokens(base_keyword: str, rule_keyword: str) -> List[str]:
            base_keyword = str(base_keyword or "").strip()
            rule_keyword = str(rule_keyword or "").strip()
            if not rule_keyword:
                return []
            if not base_keyword:
                return [rule_keyword]
            # If user already typed full keyword incl base keyword, treat as absolute substring match
            if base_keyword in rule_keyword:
                return [rule_keyword]
            # Otherwise require BOTH base keyword and rule keyword to exist in product name
            return [base_keyword, rule_keyword]

        actions_raw: List[Dict[str, Any]] = []
        missing: List[Dict[str, Any]] = []

        for base, qty in inputs.items():
            qty = float(qty or 0.0)
            if qty == 0:
                continue

            base_kw = prod_kw.get(base, base)

            rule_list = rules_map.get(base, [])
            if rule_list:
                for r in rule_list:
                    rr = Rule.from_dict(r)
                    if not rr.keyword:
                        continue

                    if rr.mode == "map":
                        k = qty_key(qty)
                        if k in rr.table:
                            delta = rr.table[k]
                        elif "*" in rr.table:
                            delta = rr.table["*"]
                        else:
                            missing.append({"ê¸°ì¤€ìƒí’ˆ": base, "í‚¤ì›Œë“œ": rr.keyword, "ì…ë ¥ê°’": qty, "ì‚¬ìœ ": f"ë§¤í•‘ì— '{k}' ì—†ìŒ"})
                            continue
                    else:  # mul
                        delta = qty * rr.value

                    tokens = _scoped_tokens(base_kw, rr.keyword)
                    if not tokens:
                        continue

                    display = " & ".join(tokens) if len(tokens) > 1 else tokens[0]

                    actions_raw.append({
                        "base": base,
                        "match_tokens": tokens,
                        "display": display,
                        "delta": float(delta),
                    })
            else:
                # No rule: base keyword alone is used (legacy behavior)
                tokens = [str(base_kw)]
                actions_raw.append({"base": base, "match_tokens": tokens, "display": tokens[0], "delta": float(qty)})

        # merge by match_tokens key (so different base products with same suffix like "1kg" won't collide)
        merged: Dict[str, float] = {}
        bases: Dict[str, set] = {}
        meta: Dict[str, Dict[str, Any]] = {}

        for a in actions_raw:
            key = "\u0001".join(a["match_tokens"])
            merged[key] = merged.get(key, 0.0) + float(a["delta"])
            bases.setdefault(key, set()).add(a["base"])
            if key not in meta:
                meta[key] = {"match_tokens": a["match_tokens"], "display": a.get("display")}

        out = []
        for k, v in merged.items():
            out.append({
                "match_tokens": meta[k]["match_tokens"],
                "display": meta[k].get("display") or " & ".join(meta[k]["match_tokens"]),
                "delta": v,
                "bases": sorted(list(bases.get(k, []))),
            })

        out.sort(key=lambda x: len(str(x.get("display", ""))), reverse=True)
        return out, pd.DataFrame(missing)


    def embed_config_into_workbook(wb, cfg: Dict[str, Any]) -> None:
        """Save cfg JSON into a hidden sheet so settings can travel with the Excel file."""
        if EXCEL_CONFIG_SHEET in wb.sheetnames:
            ws_cfg = wb[EXCEL_CONFIG_SHEET]
        else:
            ws_cfg = wb.create_sheet(EXCEL_CONFIG_SHEET)
        ws_cfg[EXCEL_CONFIG_CELL].value = json.dumps(cfg, ensure_ascii=False)
        try:
            ws_cfg.sheet_state = "hidden"
        except Exception:
            pass


    def extract_config_from_workbook_bytes(xlsx_bytes: bytes) -> Dict[str, Any]:
        wb = load_workbook(io.BytesIO(xlsx_bytes))
        if EXCEL_CONFIG_SHEET not in wb.sheetnames:
            raise ValueError("ì—‘ì…€ì— ì €ì¥ëœ ì„¤ì • ì‹œíŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
        ws_cfg = wb[EXCEL_CONFIG_SHEET]
        raw = ws_cfg[EXCEL_CONFIG_CELL].value
        if not raw:
            raise ValueError("ì—‘ì…€ ì„¤ì • ì…€(A1)ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
        cfg = json.loads(str(raw))
        if not isinstance(cfg, dict):
            raise ValueError("ì—‘ì…€ ì„¤ì • í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        cfg.setdefault("version", 9)
        cfg.setdefault("inventory_column", "ì¬ê³ ìˆ˜ëŸ‰")
        cfg.setdefault("name_column", "ìƒí’ˆëª…")
        cfg.setdefault("products", [])
        cfg.setdefault("rules", {})
        return cfg




    def update_workbook_bytes(
        xlsx_bytes: bytes,
        cfg: Dict[str, Any],
        inputs: Dict[str, float],
    ) -> Tuple[bytes, pd.DataFrame, pd.DataFrame, bytes]:
        """
        Returns:
          - updated workbook bytes
          - df_changes (summary)
          - df_missing (map missing table)
          - changed_rows_only workbook bytes (header + changed rows)
        """
        inv_col = cfg.get("inventory_column", "ì¬ê³ ìˆ˜ëŸ‰")
        name_col = cfg.get("name_column", "ìƒí’ˆëª…")

        wb = load_workbook(io.BytesIO(xlsx_bytes))
        ws = wb.active

        header_row, name_idx, inv_idx = find_header_row_and_columns(ws, name_col=name_col, inv_col=inv_col)

        actions, df_missing = build_actions(cfg, inputs)

        def match_action(a: Dict[str, Any], name_str: str) -> bool:
            toks = a.get("match_tokens") or []
            return bool(toks) and all((t in name_str) for t in toks)

        changes = []
        changed_row_indices: List[int] = []

        for r in range(header_row + 1, ws.max_row + 1):
            name_val = ws.cell(r, name_idx).value
            if name_val is None or str(name_val).strip() == "":
                continue

            # skip guideline rows ("í•„ìˆ˜")
            first_cell = ws.cell(r, 1).value
            if isinstance(first_cell, str) and first_cell.strip() in ("í•„ìˆ˜",):
                continue
            if isinstance(name_val, str) and name_val.strip() in ("í•„ìˆ˜",):
                continue

            name_str = str(name_val)
            matched = [a for a in actions if match_action(a, name_str)]
            if not matched:
                continue

            delta = sum(float(m["delta"]) for m in matched)
            if abs(delta) < 1e-12:
                continue

            old = to_number(ws.cell(r, inv_idx).value)
            new = old + delta
            ws.cell(r, inv_idx).value = new

            # record changed row index (to keep same format, we will delete other rows later)
            changed_row_indices.append(r)

            changes.append({
                "í–‰ë²ˆí˜¸": r,
                "ìƒí’ˆëª…": name_str,
                "ê¸°ì¡´ì¬ê³ ": old,
                "ì¦ê°": delta,
                "ìµœì¢…ì¬ê³ ": new,
                "ë§¤ì¹­í‚¤ì›Œë“œ": ", ".join([str(m.get("display") or " & ".join(m.get("match_tokens", []))) for m in matched]),
                "ì›ì²œìƒí’ˆ": ", ".join(sorted({b for m in matched for b in m.get("bases", [])})),
            })

        # Embed current config into output Excel (portable persistence)
        embed_config_into_workbook(wb, cfg)

        out = io.BytesIO()
        wb.save(out)
        out.seek(0)
        updated_bytes = out.getvalue()
        # Build "changed rows only" workbook while keeping the same format/spec as the uploaded Excel:
        # 1) reload the UPDATED workbook bytes (so styles/column widths/etc. are preserved)
        # 2) delete all non-changed data rows (below header_row) from bottom to top
        wb2 = load_workbook(io.BytesIO(updated_bytes))
        ws2 = wb2.active

        keep = set(changed_row_indices)
        # delete from bottom to avoid shifting
        for rr in range(ws2.max_row, header_row, -1):
            if rr not in keep:
                ws2.delete_rows(rr, 1)

        out2 = io.BytesIO()
        wb2.save(out2)
        out2.seek(0)
        changed_rows_bytes = out2.getvalue()

        return updated_bytes, pd.DataFrame(changes), df_missing, changed_rows_bytes


    # ============================
    # UI
    # ============================
    cfg = load_config()

    # ìë™ë³µì›: ìƒí’ˆëª©ë¡ì´ ë¹„ì–´ìˆìœ¼ë©´ ê¸°ë³¸ 34ê°œë¥¼ ì±„ì›Œë„£ìŠµë‹ˆë‹¤.
    if not cfg.get("products"):
        cfg["products"] = DEFAULT_PRODUCTS.copy()
        save_config(cfg)

    st.title("ğŸ§° ì¬ê³ ì¼ê´„ë³€ê²½")

    # ============================
    # Sidebar navigation
    # ============================
    st.sidebar.title("ë©”ë‰´")
    # âœ… ê¸°ë³¸ì€ ì ‘íŒ ìƒíƒœ(ë””í´íŠ¸), í•„ìš”í•  ë•Œë§Œ í¼ì³ì„œ ë©”ë‰´ ì´ë™
    with st.sidebar.expander("ğŸ“‚ í¼ì³ë³´ê¸°", expanded=False):
        page = st.radio(
            "ì´ë™",
            options=["â‘  ì¬ê³  ì…ë ¥", "â‘¡ ìƒí’ˆëª©ë¡ ê´€ë¦¬", "â‘¢ ê·œì¹™ ê´€ë¦¬", "â‘£ ë°±ì—…/ë³µì›"],
            index=0,  # âœ… ì‹œì‘ í˜ì´ì§€: ì¬ê³  ì…ë ¥
            key="bulk_stock_sidebar_page",
        )

    # ============================
    # Pages
    # ============================
    if page.startswith("â‘ "):
        st.subheader("ì—‘ì…€ íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°")
        uploaded = st.file_uploader("ìŠ¤ë§ˆíŠ¸ìŠ¤í† ì–´ ìˆ˜ì •ì–‘ì‹ ì—‘ì…€(.xlsx)ì„ ì—…ë¡œë“œí•˜ì„¸ìš”", type=["xlsx"], key="bulk_xlsx_uploader")

        # ì—…ë¡œë“œ ë°”ì´íŠ¸(ì¤‘ë³µ getvalue() ë°©ì§€)
        uploaded_bytes = uploaded.getvalue() if uploaded is not None else None

        # âœ… 'ë©”ëª¨ì¥ìœ¼ë¡œ ì €ì¥' ë“±ìœ¼ë¡œ rerun ë˜ì–´ë„, ë§ˆì§€ë§‰ ì ìš© ê²°ê³¼(ë‹¤ìš´ë¡œë“œ/ë³€ê²½í‘œ)ê°€ ì‚¬ë¼ì§€ì§€ ì•Šê²Œ ìœ ì§€
        if "last_apply_result" not in st.session_state:
            st.session_state.last_apply_result = None

        def _fingerprint_upload(name: str, b: bytes) -> str:
            md5 = hashlib.md5(b).hexdigest()
            return f"{name}:{len(b)}:{md5}"

        current_fp = (
            _fingerprint_upload(getattr(uploaded, "name", "") or "", uploaded_bytes)
            if uploaded_bytes is not None
            else None
        )

        # ë‹¤ë¥¸ ì—‘ì…€ì„ ìƒˆë¡œ ì—…ë¡œë“œí•˜ë©´, ì´ì „ ê²°ê³¼ëŠ” ìë™ìœ¼ë¡œ ìˆ¨ê¹€ ì²˜ë¦¬
        _last = st.session_state.get("last_apply_result")
        if _last is not None:
            if (current_fp is None) or (_last.get("fingerprint") != current_fp):
                st.session_state.last_apply_result = None


        st.subheader("ì…ë ¥í•  ìˆ˜ëŸ‰")
        prod_list = cfg.get("products", [])
        if not prod_list:
            st.info("ìƒí’ˆëª©ë¡ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. ì‚¬ì´ë“œë°”ì˜ 'ìƒí’ˆëª©ë¡ ê´€ë¦¬'ì—ì„œ ìƒí’ˆì„ ë¨¼ì € ì¶”ê°€í•˜ì„¸ìš”.")
        else:
            # ì—…ë¡œë“œëœ ì—‘ì…€ë¡œë¶€í„° 'ì¬ê³ ìˆ˜ëŸ‰(í‘œì‹œìš©)'ì„ ê³„ì‚° (ì…ë ¥/ì €ì¥ì—ëŠ” ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ)
            stock_map: Dict[str, str] = {}
            if uploaded is not None:
                try:
                    stock_map = compute_stock_display_map(uploaded_bytes, cfg)
                except Exception:
                    stock_map = {}

            current_names = [str(p.get("name", "")).strip() for p in prod_list if str(p.get("name", "")).strip()]
            if not current_names:
                st.info("ìƒí’ˆëª©ë¡ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. ì‚¬ì´ë“œë°”ì˜ 'ìƒí’ˆëª©ë¡ ê´€ë¦¬'ì—ì„œ ìƒí’ˆì„ ë¨¼ì € ì¶”ê°€í•˜ì„¸ìš”.")
            else:
                            # ì…ë ¥ê°’ì€ 'âœ… ì—‘ì…€ì— ì ìš©í•˜ê¸°'ë¥¼ ëˆŒë €ì„ ë•Œë§Œ í™•ì •(ì €ì¥)ë©ë‹ˆë‹¤.
                def _align_qty_df(_df: pd.DataFrame, _names: List[str]) -> pd.DataFrame:
                    try:
                        if _df is not None and not _df.empty and "ìƒí’ˆ" in _df.columns and "ì…ë ¥ìˆ˜ëŸ‰" in _df.columns:
                            _m = _df.set_index("ìƒí’ˆ")["ì…ë ¥ìˆ˜ëŸ‰"].to_dict()
                        else:
                            _m = {}
                    except Exception:
                        _m = {}

                    return pd.DataFrame(
                        {
                            "ìƒí’ˆ": _names,
                            "ì…ë ¥ìˆ˜ëŸ‰": ["" if (_m.get(n) is None) else _m.get(n, "") for n in _names],
                        }
                    )

                # ë§ˆì§€ë§‰ìœ¼ë¡œ 'ì ìš©(í™•ì •)'ëœ ê°’(=ì—‘ì…€ ì ìš©ì— ì‚¬ìš©ë˜ëŠ” ê°’)
                if "qty_committed_df" not in st.session_state:
                    st.session_state.qty_committed_df = pd.DataFrame(
                        {"ìƒí’ˆ": current_names, "ì…ë ¥ìˆ˜ëŸ‰": [""] * len(current_names)}
                    )
                st.session_state.qty_committed_df = _align_qty_df(st.session_state.qty_committed_df, current_names)
                # --- ì°¸ê³ ìˆ˜ëŸ‰: ì €ì¥ëœ ê°’(cfg["ref_qty"])ë§Œ í‘œì‹œí•©ë‹ˆë‹¤. (ğŸ’¾ ì°¸ê³ ìˆ˜ëŸ‰ ì €ì¥ì„ ëˆŒëŸ¬ì•¼ë§Œ ì˜êµ¬ ì €ì¥ë¨) ---
                ref_saved = (cfg.get("ref_qty") or {})

                def _ref_saved_value(_name: str) -> str:
                    v = ref_saved.get(_name)
                    return "" if v is None else str(v)

                # í‘œ í‘œì‹œìš©(ì¬ê³ ìˆ˜ëŸ‰/ì°¸ê³ ìˆ˜ëŸ‰ì€ í‘œì‹œ/ì…ë ¥ë§Œ)
                df_view = st.session_state.qty_committed_df.copy()
                df_view["ì¬ê³ ìˆ˜ëŸ‰"] = [stock_map.get(n, "") for n in df_view["ìƒí’ˆ"]]
                df_view["ì°¸ê³ ìˆ˜ëŸ‰"] = [_ref_saved_value(n) for n in df_view["ìƒí’ˆ"]]
                df_view = df_view[["ìƒí’ˆ", "ì…ë ¥ìˆ˜ëŸ‰", "ì¬ê³ ìˆ˜ëŸ‰", "ì°¸ê³ ìˆ˜ëŸ‰"]]

                st.caption("â€» í‘œì— ê°’ì„ ì…ë ¥í•´ë„ ì¦‰ì‹œ ì €ì¥/ì ìš©ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤. **âœ… ì—‘ì…€ì— ì ìš©í•˜ê¸°**ë¥¼ ëˆŒëŸ¬ì•¼ ì—‘ì…€ì— ë°˜ì˜ë©ë‹ˆë‹¤.")
                st.caption("â€» 'ì…ë ¥ìˆ˜ëŸ‰'ì„ ë¹„ì›Œë‘ë©´ 0ìœ¼ë¡œ ì²˜ë¦¬ë©ë‹ˆë‹¤. (ì˜ˆ: ì—”ë‹¤ì´ë¸Œ - 0)")
                st.caption("â€» 'ì°¸ê³ ìˆ˜ëŸ‰'ì€ **ğŸ’¾ ì°¸ê³ ìˆ˜ëŸ‰ ì €ì¥**ì„ ëˆŒëŸ¬ì•¼ ì €ì¥ë˜ë©°, ì €ì¥ í›„ì—ëŠ” ìˆ˜ì • ì „ê¹Œì§€ ìœ ì§€ë©ë‹ˆë‹¤.")

                # ìƒí’ˆëª©ë¡ì´ ë³€ê²½ë˜ë©´ í¸ì§‘ ìƒíƒœë¥¼ ì´ˆê¸°í™”í•˜ê¸° ìœ„í•´ keyë¥¼ ë³€ê²½í•©ë‹ˆë‹¤.
                _sig = hashlib.md5(("|".join(current_names)).encode("utf-8")).hexdigest()[:10]
                _editor_key = f"qty_editor_{_sig}"


                # data_editorê°€ rerun ë•Œ ê°’ì´ ì‚¬ë¼ì§€ëŠ” ê²ƒì„ ë°©ì§€:
                # - ê°™ì€ keyì˜ widget stateê°€ ìˆìœ¼ë©´ ê·¸ ê°’ì„ ìš°ì„  ì‚¬ìš©
                # - ì¬ê³ ìˆ˜ëŸ‰(í‘œì‹œìš©)ë§Œ ë§¤ë²ˆ ìƒˆë¡œ ê°±ì‹ 
                _df_for_editor = df_view
                if _editor_key in st.session_state and isinstance(st.session_state.get(_editor_key), pd.DataFrame):
                    _prev = st.session_state.get(_editor_key).copy()
                    try:
                        if "ìƒí’ˆ" in _prev.columns:
                            _prev["ìƒí’ˆ"] = _prev["ìƒí’ˆ"].astype(str)
                            _prev = _prev.set_index("ìƒí’ˆ").reindex(current_names).reset_index()
                    except Exception:
                        _prev = df_view.copy()

                    for _c in ["ì…ë ¥ìˆ˜ëŸ‰", "ì¬ê³ ìˆ˜ëŸ‰", "ì°¸ê³ ìˆ˜ëŸ‰"]:
                        if _c not in _prev.columns:
                            _prev[_c] = ""

                    _prev["ì¬ê³ ìˆ˜ëŸ‰"] = [stock_map.get(n, "") for n in _prev["ìƒí’ˆ"]]
                    _df_for_editor = _prev[["ìƒí’ˆ", "ì…ë ¥ìˆ˜ëŸ‰", "ì¬ê³ ìˆ˜ëŸ‰", "ì°¸ê³ ìˆ˜ëŸ‰"]]

                df_edit = st.data_editor(
                    _df_for_editor,
                    key=_editor_key,
                    use_container_width=True,
                    num_rows="fixed",
                    disabled=["ìƒí’ˆ", "ì¬ê³ ìˆ˜ëŸ‰"],
                    column_config={
                        "ì…ë ¥ìˆ˜ëŸ‰": st.column_config.TextColumn("ì…ë ¥ìˆ˜ëŸ‰", help="ìˆ«ì ì…ë ¥(ìŒìˆ˜/ì†Œìˆ˜ ê°€ëŠ¥). ì˜ˆ: 3, -2, 1.5"),
                        "ì¬ê³ ìˆ˜ëŸ‰": st.column_config.TextColumn("ì¬ê³ ìˆ˜ëŸ‰", help="ì—…ë¡œë“œí•œ ì—‘ì…€ ê¸°ì¤€, ëª¨ë“  ì˜µì…˜ ì¬ê³  í•©(í‘œì‹œìš©)"),
                        "ì°¸ê³ ìˆ˜ëŸ‰": st.column_config.TextColumn("ì°¸ê³ ìˆ˜ëŸ‰", help="ë©”ëª¨/ì°¸ê³ ìš© ìˆ˜ëŸ‰(ì €ì¥ ì‹œ ìœ ì§€). ì˜ˆ: 10"),
                    },
                )


                # ì°¸ê³ ìˆ˜ëŸ‰ì€ í‘œì—ì„œ í¸ì§‘í•  ìˆ˜ ìˆì§€ë§Œ, **ğŸ’¾ ì°¸ê³ ìˆ˜ëŸ‰ ì €ì¥**ì„ ëˆŒëŸ¬ì•¼ë§Œ ì„¤ì •ì— ì €ì¥ë©ë‹ˆë‹¤.

                # âœ… í˜„ì¬ í‘œ(í¸ì§‘ì¤‘ ê°’) ê¸°ì¤€ìœ¼ë¡œ ì…ë ¥ê°’/ë©”ëª¨ ìƒì„± (ë¹ˆì¹¸ì€ 0)
                df_inputs = df_edit.drop(columns=["ì¬ê³ ìˆ˜ëŸ‰", "ì°¸ê³ ìˆ˜ëŸ‰"], errors="ignore").copy()

                inputs: Dict[str, float] = {}
                memo_lines: List[str] = []
                for _, r in df_inputs.iterrows():
                    name = str(r.get("ìƒí’ˆ", "")).strip()
                    if not name:
                        continue
                    qty = parse_input_number(r.get("ì…ë ¥ìˆ˜ëŸ‰", ""))
                    inputs[name] = qty
                    memo_lines.append(f"{name} - {fmt_qty_for_memo(qty)}")
                memo_text = "\n".join(memo_lines)

                col_a, col_b, col_c = st.columns(3, gap="small")
                with col_a:
                    apply_clicked = st.button(
                        "âœ… ì—‘ì…€ì— ì ìš©í•˜ê¸°",
                        disabled=(uploaded is None),
                        use_container_width=True,
                    )
                with col_b:
                    memo_filename = "ì£¼ë¬¸ì–‘ì‹.txt"
                    st.download_button(
                        "ğŸ“ ë©”ëª¨ì¥ìœ¼ë¡œ ì €ì¥",
                        data=memo_text.encode("utf-8"),
                        file_name=memo_filename,
                        mime="text/plain",
                        disabled=(len(memo_lines) == 0),
                        help="í˜„ì¬ í‘œ(í¸ì§‘ì¤‘ ê°’) ê¸°ì¤€ìœ¼ë¡œ í…ìŠ¤íŠ¸(.txt)ë¥¼ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤. (ë¹ˆì¹¸=0)",
                        use_container_width=True,
                    )
                with col_c:
                    ref_save_clicked = st.button(
                        "ğŸ’¾ ì°¸ê³ ìˆ˜ëŸ‰ ì €ì¥",
                        use_container_width=True,
                        key=f"bulk_save_ref_qty_{_editor_key}",
                    )

                # ğŸ’¾ ì°¸ê³ ìˆ˜ëŸ‰ ì €ì¥ ì²˜ë¦¬(ì €ì¥ í›„ ìˆ˜ì • ì „ê¹Œì§€ ìœ ì§€)
                if ref_save_clicked:
                    ref_map_new: Dict[str, Any] = {}
                    for _, rr in df_edit.iterrows():
                        pname = str(rr.get("ìƒí’ˆ", "")).strip()
                        if not pname:
                            continue

                        raw = rr.get("ì°¸ê³ ìˆ˜ëŸ‰", "")
                        if raw is None:
                            sval = ""
                        elif isinstance(raw, (int, float)):
                            sval = fmt_qty_for_memo(float(raw))
                        else:
                            sval = str(raw).strip()
                            if sval != "":
                                s_clean = sval.replace(",", "")
                                if re.fullmatch(r"[+-]?\d+(?:\.\d+)?", s_clean):
                                    sval = fmt_qty_for_memo(parse_input_number(sval))

                        if sval == "":
                            continue
                        ref_map_new[pname] = sval

                    cfg["ref_qty"] = ref_map_new
                    save_config(cfg)
                    st.success("ì°¸ê³ ìˆ˜ëŸ‰ ì €ì¥ ì™„ë£Œ!")
                    st.rerun()

                if apply_clicked:
                    # âœ… ì ìš© ë²„íŠ¼ì„ ëˆŒë €ì„ ë•Œë§Œ 'í™•ì •(ì €ì¥)' + ì—‘ì…€ ë°˜ì˜
                    st.session_state.qty_committed_df = df_inputs.copy()

                    if uploaded_bytes is None:
                        st.warning("ì—‘ì…€ íŒŒì¼ì„ ì—…ë¡œë“œí•´ ì£¼ì„¸ìš”.")
                        st.session_state.last_apply_result = None
                    else:
                        try:
                            updated_bytes, df_changes, df_missing, changed_rows_bytes = update_workbook_bytes(
                                uploaded_bytes, cfg, inputs
                            )
                        except Exception as e:
                            st.error(f"ì ìš© ì¤‘ ì˜¤ë¥˜: {e}")
                            st.session_state.last_apply_result = None
                        else:
                            out_name_changed = "ì¬ê³ ìˆ˜ëŸ‰ì¼ê´„ë³€ê²½.xlsx"

                            # âœ… ê²°ê³¼ë¥¼ session_stateì— ì €ì¥ (ë©”ëª¨ì¥ ë‹¤ìš´ë¡œë“œ ë“± rerunì—ë„ ìœ ì§€)
                            st.session_state.last_apply_result = {
                                "fingerprint": current_fp,
                                "out_name": out_name_changed,
                                "changed_rows_bytes": changed_rows_bytes,
                                "df_changes": df_changes,
                                "df_missing": df_missing,
                                "applied_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                            }

                # âœ… ë§ˆì§€ë§‰ ì ìš© ê²°ê³¼ í‘œì‹œ(ë²„íŠ¼/í‘œê°€ rerunìœ¼ë¡œ ì‚¬ë¼ì§€ì§€ ì•ŠìŒ)
                _last = st.session_state.get("last_apply_result")
                if _last is not None:
                    st.success(f"ì™„ë£Œ! ì•„ë˜ì—ì„œ ë‹¤ìš´ë¡œë“œí•˜ì„¸ìš”. (ë§ˆì§€ë§‰ ì ìš©: {_last.get('applied_at', '')})")

                    st.download_button(
                        "â¬‡ï¸ ë‹¤ìš´ë¡œë“œ",
                        data=_last["changed_rows_bytes"],
                        file_name=_last["out_name"],
                        mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                        help="ì¬ê³ ìˆ˜ëŸ‰ì´ ë³€ê²½ëœ í–‰(í—¤ë” í¬í•¨)ë§Œ ë‚¨ê¸´ íŒŒì¼ì…ë‹ˆë‹¤.",
                        use_container_width=True,
                        key=f"bulk_excel_dl_{_last.get('fingerprint', '')}",
                    )

                    df_changes = _last.get("df_changes")
                    df_missing = _last.get("df_missing")

                    if isinstance(df_changes, pd.DataFrame):
                        if df_changes.empty:
                            st.info("ë³€ê²½ëœ í–‰ì´ ì—†ìŠµë‹ˆë‹¤. (í‚¤ì›Œë“œ ë§¤ì¹­ì´ ì•ˆ ëê±°ë‚˜ ì…ë ¥ì´ 0ì´ê±°ë‚˜, map ëª¨ë“œì—ì„œ ì…ë ¥ê°’ì´ ë§¤í•‘ì— ì—†ì„ ìˆ˜ ìˆì–´ìš”)")
                        else:
                            st.dataframe(df_changes.drop(columns=["í–‰ë²ˆí˜¸"], errors="ignore"), use_container_width=True)

                    if isinstance(df_missing, pd.DataFrame) and (not df_missing.empty):
                        st.warning("âš ï¸ map(ë§¤í•‘) ê·œì¹™ì—ì„œ 'ì…ë ¥ê°’ â†’ ì ìš©ê°’'ì´ ì •ì˜ë˜ì§€ ì•Šì•„ ì ìš©ë˜ì§€ ì•Šì€ í•­ëª©ì´ ìˆìŠµë‹ˆë‹¤.")
                        st.dataframe(df_missing, use_container_width=True)

    elif page.startswith("â‘¡"):
        st.subheader("ìƒí’ˆëª©ë¡ ì¶”ê°€/ì‚­ì œ/ìˆ˜ì •")

        prod_list = cfg.get("products", [])
        if prod_list:
            df_prod_raw = pd.DataFrame(prod_list)
        else:
            df_prod_raw = pd.DataFrame(columns=["keyword", "name"])

        # ensure required columns exist
        for _c in ["keyword", "name"]:
            if _c not in df_prod_raw.columns:
                df_prod_raw[_c] = ""

        # âœ… ì»¬ëŸ¼ ìˆœì„œ: í‚¤ì›Œë“œ(íŒ¨í„´) -> í‘œì‹œë  ìƒí’ˆëª…
        df_prod = df_prod_raw[["keyword", "name"]].rename(
            columns={
                "keyword": "ì‹¤ì œ ìƒí’ˆëª…(íŒ¨í„´)",
                "name": "í‘œì‹œë  ìƒí’ˆëª…",
            }
        )

        st.write("â€¢ **ì‹¤ì œ ìƒí’ˆëª…(íŒ¨í„´)**ì€ ì—‘ì…€ 'ìƒí’ˆëª…'ì—ì„œ ë§¤ì¹­í•  ë¬¸ìì—´ì…ë‹ˆë‹¤. (ë¹„ìš°ë©´ 'í‘œì‹œë  ìƒí’ˆëª…'ê³¼ ë™ì¼í•˜ê²Œ ì²˜ë¦¬)")
        st.caption("â€» í‘œì—ì„œ ì…ë ¥/ìˆ˜ì • í›„ **ì €ì¥ ë²„íŠ¼ì„ ëˆŒëŸ¬ì•¼** ì„¤ì •ì´ ì €ì¥ë©ë‹ˆë‹¤.")

        with st.form("prod_form", clear_on_submit=False):
            df_prod_edit = st.data_editor(
                df_prod,
                key="bulk_prod_editor",
                use_container_width=True,
                num_rows="dynamic",
                column_config={
                    "ì‹¤ì œ ìƒí’ˆëª…(íŒ¨í„´)": st.column_config.TextColumn("ì‹¤ì œ ìƒí’ˆëª…(íŒ¨í„´)"),
                    "í‘œì‹œë  ìƒí’ˆëª…": st.column_config.TextColumn("í‘œì‹œë  ìƒí’ˆëª…"),
                },
            )
            save_prod = st.form_submit_button("ğŸ’¾ ìƒí’ˆëª©ë¡ ì €ì¥", type="primary", use_container_width=True)

        if save_prod:
            cleaned = []
            for _, r in df_prod_edit.iterrows():
                name = str(r.get("í‘œì‹œë  ìƒí’ˆëª…", "")).strip()
                if not name:
                    continue
                kw = str(r.get("ì‹¤ì œ ìƒí’ˆëª…(íŒ¨í„´)", "")).strip() or name
                cleaned.append({"name": name, "keyword": kw})

            cfg["products"] = cleaned
            # drop rules of deleted products
            rules = cfg.get("rules", {}) or {}
            rules = {k: v for k, v in rules.items() if k in {p["name"] for p in cleaned}}
            cfg["rules"] = rules

            save_config(cfg)
            st.success("ìƒí’ˆëª©ë¡ ì €ì¥ ì™„ë£Œ!")
            st.rerun()

    elif page.startswith("â‘¢"):
            st.subheader("ê·œì¹™ ì¶”ê°€/ì‚­ì œ/ìˆ˜ì •")



            # ğŸ” ì¸ì‹ë¡œì§ ê´€ë¦¬ëŠ” ì•„ë˜ìª½(ê·œì¹™ ì €ì¥ ë²„íŠ¼ ì•„ë˜)ì— ìˆìŠµë‹ˆë‹¤.

            prod_names = [p.get("name") for p in cfg.get("products", []) if p.get("name")]
            if not prod_names:
                st.info("ë¨¼ì € 'â‘¡ ìƒí’ˆëª©ë¡ ê´€ë¦¬'ì—ì„œ ìƒí’ˆì„ ì¶”ê°€í•˜ì„¸ìš”.")
            else:
                base = st.selectbox("ê·œì¹™ì„ í¸ì§‘í•  ê¸°ì¤€ ìƒí’ˆ", options=prod_names)

                st.markdown(
                    """
        - **mul(ë°°ìˆ˜)**: `ì…ë ¥ìˆ˜ëŸ‰ Ã— value` ë§Œí¼ ë”í•¨  
        - **map(ë§¤í•‘)**: ì…ë ¥ê°’ë³„ë¡œ ë”± ì •í•œ ê°’ë§Œ ë”í•¨ (ì˜ˆ: `1=2, 2=3`)  
          - mapì€ **ë§¤í•‘(table)** ì¹¸ì— `ì…ë ¥ê°’=ì ìš©ê°’`ì„ `,`ë¡œ êµ¬ë¶„í•´ì„œ ì ì–´ìš”.
        - **í‚¤ì›Œë“œ ìë™ ìŠ¤ì½”í”„(ì¤‘ìš”)**: ê¸°ì¤€ìƒí’ˆì„ ì˜ˆ) **ì—”ë‹¤ì´ë¸Œ**ë¡œ ì„ íƒí•œ ìƒíƒœì—ì„œ í‚¤ì›Œë“œë¥¼ `1kg`ì²˜ëŸ¼ **ì˜µì…˜ë§Œ** ì“°ë©´,  
          ì—‘ì…€ ìƒí’ˆëª…ì— `ì—”ë‹¤ì´ë¸Œ`ì™€ `1kg`ê°€ **ë‘˜ ë‹¤ í¬í•¨ëœ í–‰ë§Œ** ì ìš©ë©ë‹ˆë‹¤. (ë‹¤ë¥¸ ìƒí’ˆì˜ `1kg`ëŠ” ì˜í–¥ ì—†ìŒ)  
          ì´ë¯¸ `ì—”ë‹¤ì´ë¸Œ1kg`ì²˜ëŸ¼ **ì „ì²´ ë¬¸ìì—´**ì„ ì“°ë©´ ê·¸ëŒ€ë¡œ ê·¸ ë¬¸ìì—´ë¡œ ë§¤ì¹­í•©ë‹ˆë‹¤.
                    """
                )

                rule_list = (cfg.get("rules", {}) or {}).get(base, [])
                ui_rows = []
                for rr in rule_list:
                    rr2 = Rule.from_dict(rr)
                    table = rr2.table or {}

                    def _sort_key(x):
                        try:
                            return float(x) if x != "*" else 1e19
                        except Exception:
                            return 1e19

                    map_str = ", ".join([f"{k}={fmt_int(table[k])}" for k in sorted(table.keys(), key=_sort_key)]) if table else ""

                    ui_rows.append({
                        "í‚¤ì›Œë“œ(ì—‘ì…€ ìƒí’ˆëª… í¬í•¨ ë¬¸ìì—´)": rr2.keyword,
                        "ëª¨ë“œ": rr2.mode,
                        "value": fmt_int(rr2.value),
                        "ë§¤í•‘(table) - map ëª¨ë“œì—ì„œë§Œ": map_str,
                    })

                df_rule = pd.DataFrame(ui_rows) if ui_rows else pd.DataFrame(columns=[
                    "í‚¤ì›Œë“œ(ì—‘ì…€ ìƒí’ˆëª… í¬í•¨ ë¬¸ìì—´)", "ëª¨ë“œ", "value", "ë§¤í•‘(table) - map ëª¨ë“œì—ì„œë§Œ"
                ])

                df_rule_edit = st.data_editor(
                    df_rule,
                    key="bulk_rule_editor",
                    use_container_width=True,
                    num_rows="dynamic",
                    column_config={
                        "í‚¤ì›Œë“œ(ì—‘ì…€ ìƒí’ˆëª… í¬í•¨ ë¬¸ìì—´)": st.column_config.TextColumn("í‚¤ì›Œë“œ"),
                        "ëª¨ë“œ": st.column_config.SelectboxColumn("ëª¨ë“œ", options=["mul", "map"]),
                        "value": st.column_config.TextColumn("value", help="mul ëª¨ë“œì—ì„œë§Œ ì‚¬ìš© (ì •ìˆ˜)"),
                        "ë§¤í•‘(table) - map ëª¨ë“œì—ì„œë§Œ": st.column_config.TextColumn(
                            "ë§¤í•‘(table)",
                            help="ì˜ˆ: 1=2, 2=3\n(ì¤„ë°”ê¿ˆë„ ê°€ëŠ¥)",
                        ),
                    },
                )

                if st.button("ğŸ’¾ ê·œì¹™ ì €ì¥"):
                    cleaned = []
                    for _, r in df_rule_edit.iterrows():
                        kw = str(r.get("í‚¤ì›Œë“œ(ì—‘ì…€ ìƒí’ˆëª… í¬í•¨ ë¬¸ìì—´)", "")).strip()
                        if not kw:
                            continue
                        mode = str(r.get("ëª¨ë“œ", "mul")).strip()
                        val_raw = r.get("value", 1)
                        if str(val_raw).strip() == "":
                            val = 1
                        else:
                            try:
                                val = int(round(parse_input_number(val_raw)))
                            except Exception:
                                val = 1

                        table_str = str(r.get("ë§¤í•‘(table) - map ëª¨ë“œì—ì„œë§Œ", "") or "")
                        table = parse_map_string(table_str) if mode == "map" else {}

                        cleaned.append({
                            "keyword": kw,
                            "mode": mode,
                            "value": val,
                            "table": table,
                        })

                    rules = cfg.get("rules", {}) or {}
                    rules[base] = cleaned
                    cfg["rules"] = rules
                    save_config(cfg)
                    st.success("ê·œì¹™ ì €ì¥ ì™„ë£Œ!")
                    st.rerun()


            st.write("")

            # ----------------------------
            # Recognition logic editor (moved from sidebar)
            # ----------------------------
            with st.expander("ğŸ” ì¸ì‹ë¡œì§ ê´€ë¦¬(ë‹¨ìœ„ ì¸ì‹)", expanded=False):
                st.caption("ìƒí’ˆëª…/ê·œì¹™ í‚¤ì›Œë“œì—ì„œ ë‹¨ìœ„ë¥¼ ì¸ì‹í•˜ëŠ” ë¡œì§ì…ë‹ˆë‹¤. (priorityëŠ” ë‚´ë¶€ì ìœ¼ë¡œ í–‰ ìˆœì„œë¡œ ìë™ ë¶€ì—¬ë©ë‹ˆë‹¤)")

                logic_rows = _get_recognition_logic(cfg)
                df_logic = pd.DataFrame(
                    [
                        {
                            "priority": int(r.get("priority", 999)),
                            "output_unit": str(r.get("output_unit", "")),
                            "multiplier": str(r.get("multiplier", "1")),
                            "aliases": ", ".join([str(a) for a in (r.get("aliases") or [])]),
                        }
                        for r in logic_rows
                    ]
                )
                if df_logic.empty:
                    df_logic = pd.DataFrame(columns=["priority", "output_unit", "multiplier", "aliases"])

                # í™”ë©´ì—ì„œëŠ” priorityë¥¼ ìˆ¨ê¸°ê³ , ì €ì¥ ì‹œ í–‰ ìˆœì„œëŒ€ë¡œ ìë™ ë¶€ì—¬í•©ë‹ˆë‹¤.
                df_ui = df_logic.sort_values("priority").reset_index(drop=True).drop(columns=["priority"], errors="ignore")

                edited = st.data_editor(
                    df_ui,
                    use_container_width=True,
                    num_rows="dynamic",
                    hide_index=True,
                    column_config={
                        "output_unit": st.column_config.TextColumn("output_unit", help="ìµœì¢… í‘œì‹œ ë‹¨ìœ„ (ì˜ˆ: ë‹¨, kg, íŒ©)"),
                        "multiplier": st.column_config.TextColumn("multiplier", help="ìˆ«ìì— ê³±í•´ì§€ëŠ” ê°’ (ì˜ˆ: gâ†’kg í™˜ì‚°ì€ 0.001)"),
                        "aliases": st.column_config.TextColumn("aliases (ì‰¼í‘œë¡œ êµ¬ë¶„)", help="ì¸ì‹í•  ë¬¸ìì—´ë“¤. ì˜ˆ: kg,í‚¬ë¡œ,í‚¤ë¡œ"),
                    },
                    key="bulk_recognition_logic_editor",
                )

                c1, c2 = st.columns(2)
                if c1.button("ğŸ’¾ ì¸ì‹ë¡œì§ ì €ì¥", use_container_width=True, key="bulk_save_recognition_logic"):
                    raw_rows: List[Dict[str, Any]] = []
                    try:
                        for idx, row in edited.iterrows():
                            unit = str(row.get("output_unit", "")).strip()
                            aliases_raw = str(row.get("aliases", "")).strip()
                            if not unit or not aliases_raw:
                                continue

                            mult = str(row.get("multiplier", "1")).strip() or "1"
                            aliases = [a.strip() for a in aliases_raw.split(",") if a.strip()]

                            # âœ… priorityëŠ” í™”ë©´ì— ìˆ¨ê¸°ê³ , í˜„ì¬ í–‰ ìˆœì„œëŒ€ë¡œ ìë™ ë¶€ì—¬
                            pr = int((idx + 1) * 10)

                            raw_rows.append({"priority": pr, "output_unit": unit, "multiplier": mult, "aliases": aliases})

                        cfg["recognition_logic"] = _normalize_recognition_logic(raw_rows)
                        save_config(cfg)
                        st.success("ì¸ì‹ë¡œì§ ì €ì¥ ì™„ë£Œ!")
                        st.rerun()
                    except Exception as e:
                        st.error(f"ì €ì¥ ì‹¤íŒ¨: {e}")

                # â¬‡ï¸ ë‚´ë³´ë‚´ê¸°: í˜„ì¬ ì¸ì‹ë¡œì§ì„ JSONìœ¼ë¡œ ë‹¤ìš´ë¡œë“œ
                logic_now = _normalize_recognition_logic(cfg.get("recognition_logic")) or DEFAULT_RECOGNITION_LOGIC.copy()
                export_bytes = json.dumps(logic_now, ensure_ascii=False, indent=2).encode("utf-8")
                export_name = f"recognition_logic_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
                c2.download_button("â¬‡ï¸ ë‚´ë³´ë‚´ê¸°(JSON)", data=export_bytes, file_name=export_name, mime="application/json", use_container_width=True)

                # ê°€ë“œ: 'ë‹¨' í•­ëª©ì´ ì—†ìœ¼ë©´ '1ë‹¨' í‚¤ì›Œë“œë„ kgë¡œ ë–¨ì–´ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
                units_now = [str(r.get("output_unit", "")).strip() for r in logic_now]
                if "ë‹¨" not in units_now:
                    st.warning("âš ï¸ í˜„ì¬ ì¸ì‹ë¡œì§ì— 'ë‹¨' í•­ëª©ì´ ì—†ìŠµë‹ˆë‹¤. '1ë‹¨' í‚¤ì›Œë“œë¥¼ ì¨ë„ kgë¡œ ì¸ì‹ë  ìˆ˜ ìˆì–´ìš”.")

    else:
        st.subheader("ì„¤ì • ë°±ì—…/ë³µì› (ìƒí’ˆëª©ë¡ + ê·œì¹™)")
        cfg_json = json.dumps(cfg, ensure_ascii=False, indent=2).encode("utf-8")
        st.download_button("â¬‡ï¸ ì„¤ì •(JSON) ë‹¤ìš´ë¡œë“œ", data=cfg_json, file_name="stock_config.json", mime="application/json")

        up_cfg = st.file_uploader("ì„¤ì •(JSON) ì—…ë¡œë“œí•˜ì—¬ ë³µì›", type=["json"], key="bulk_cfg_uploader")

        # âœ… ë³µì› ì™„ë£Œ ì•ŒëŒ: 3ì´ˆë§Œ í‘œì‹œ í›„ ìë™ìœ¼ë¡œ ì‚¬ë¼ì§
        if "restore_notice" not in st.session_state:
            st.session_state.restore_notice = None  # ("success"|"error", message)

        restore_clicked = st.button("â™»ï¸ ì„¤ì • ë³µì›", disabled=(up_cfg is None))
        notice_ph = st.empty()

        if restore_clicked:
            try:
                new_cfg = json.loads(up_cfg.getvalue().decode("utf-8"))
                if "products" not in new_cfg or "rules" not in new_cfg:
                    raise ValueError("í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                cfg = new_cfg
                save_config(cfg)
                st.session_state.restore_notice = ("success", "ì„¤ì • ë³µì› ì™„ë£Œ! ë°”ë¡œ ë°˜ì˜ë©ë‹ˆë‹¤.")
            except Exception as e:
                st.session_state.restore_notice = ("error", f"ë³µì› ì‹¤íŒ¨: {e}")

        # ë²„íŠ¼ ë°”ë¡œ ì•„ë˜ì— ë³µì› ê²°ê³¼ í‘œì‹œ (3ì´ˆ í›„ ìë™ ì‚­ì œ)
        if st.session_state.restore_notice:
            kind, msg = st.session_state.restore_notice
            if kind == "success":
                notice_ph.success(msg)
            else:
                notice_ph.error(msg)

            time.sleep(3)
            notice_ph.empty()
            st.session_state.restore_notice = None



# =====================================================
# Router
# =====================================================
page = st.session_state.get("page", "excel_results")
if page == "mapping_rules":
    render_mapping_rules_page()
elif page == "product_totals":
    render_product_totals_page()
elif page == "inventory":
    render_inventory_page()
elif page == "bulk_stock":
    render_bulk_stock_page()
elif page == "sales_calc":
    render_sales_calc_page()
else:
    render_excel_results_page()
